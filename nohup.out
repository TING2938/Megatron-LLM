
Settings:
RANK=0
ADDR=localhost
N_NODES=1
DATA_ARGS=--data_path /root/datasets/booksum_megatron/booksum_megatron_text_document
CHECKPOINT_PATH=/root/models/llama-2-7b-chat-hf-megatron_shard_tp_2_pp_1
TRAINED_PATH=/root/models/llama-2-7b-chat-hf-megatron_shard_tp_2_pp_1-pretrained
MODEL=llama2
TP=2
PP=1
MICRO_BATCH=1
GLOBAL_BATCH=2
INSTRUCT=0
COMMON_ARGS=--use_flash_attn --no_bias_gelu_fusion --seq_length 4096 --max_position_embeddings 4096 --log_interval 1 --save_interval 800 --eval_interval 200 --eval_iters 10 --hidden_dropout 0.0 --position_embedding_type rotary --no_bias_dropout_fusion --use_checkpoint_args --attention_dropout 0.0 --adam_beta1 0.9 --adam_beta2 0.95 --adam_eps 1e-5 --lr_decay_style cosine --lr_warmup_fraction 0.1 --lr 3e-4 --min_lr 3e-4 --weight_decay 0.1 --sequence_parallel --recompute_granularity selective --log_timers_to_tensorboard --scalar_loss_mask=0.0 --rope_scaling_factor 1.0 --metrics perplexity accuracy count_loss_mask --train_iters 1000
EXTRA_ARGS=--vocab_file=/root/models/llama-2-7b-chat-hf-megatron/tokenizer.model --use_rms_norm --glu_activation swiglu --no_tie_embed_logits --vocab_extra_ids_list [bib_ref],[/bib_ref],[fig_ref],[/fig_ref],[bib],[/bib],[fig],[/fig],[table],[/table],[formula],[/formula] --vocab_file=/root/models/llama-2-7b-chat-hf-megatron/tokenizer.model --layernorm_epsilon 1e-5

Setting num_layers to 32 from checkpoint
Setting hidden_size to 4096 from checkpoint
Setting ffn_hidden_size to 11008 from checkpoint
Setting num_attention_heads to 32 from checkpoint
Setting kv_channels to 128 from checkpoint
Setting padded_vocab_size to 32000 from checkpoint
Setting position_embedding_type to PositionEmbeddingType.rotary from checkpoint
Setting num_attention_heads_kv to 32 from checkpoint
Setting parallel_attn to False from checkpoint
Setting parallel_layernorm to False from checkpoint
Setting use_rms_norm to True from checkpoint
Setting tie_embed_logits to False from checkpoint
Setting make_vocab_size_divisible_by to 128 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 1 from checkpoint
Setting num_layers to 32 from checkpoint
Setting hidden_size to 4096 from checkpoint
Setting ffn_hidden_size to 11008 from checkpoint
Setting num_attention_heads to 32 from checkpoint
Setting kv_channels to 128 from checkpoint
Setting padded_vocab_size to 32000 from checkpoint
Setting position_embedding_type to PositionEmbeddingType.rotary from checkpoint
Setting num_attention_heads_kv to 32 from checkpoint
Setting parallel_attn to False from checkpoint
Setting parallel_layernorm to False from checkpoint
Setting use_rms_norm to True from checkpoint
Setting tie_embed_logits to False from checkpoint
Setting make_vocab_size_divisible_by to 128 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 1 from checkpoint
using world size: 2, data-parallel-size: 1, tensor-model-parallel size: 2, pipeline-model-parallel size: 1 
WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:SentencePieceTokenizer
accumulate and all-reduce gradients in fp32 for bfloat16 data type.
using torch.bfloat16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. True
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-05
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  async_tensor_model_parallel_allreduce ........... False
  attention_dropout ............................... 0.0
  attention_softmax_in_fp32 ....................... False
  barrier_with_L1_time ............................ True
  bert_load ....................................... None
  bf16 ............................................ True
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 1
  data_path ....................................... ['/root/datasets/booksum_megatron/booksum_megatron_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  data_type ....................................... gpt
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  encoder_num_layers .............................. 32
  encoder_seq_length .............................. 4096
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 200
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_signal_handler ............................. False
  ffn_hidden_size ................................. 11008
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_e4m3 ........................................ False
  fp8_hybrid ...................................... False
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 2
  glu_activation .................................. swiglu
  gradient_accumulation_fusion .................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.0
  hidden_size ..................................... 4096
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  iteration ....................................... release
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lima_dropout .................................... False
  load ............................................ /root/models/llama-2-7b-chat-hf-megatron_shard_tp_2_pp_1
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0003
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.1
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 4096
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... None
  metrics ......................................... ['perplexity', 'accuracy', 'count_loss_mask']
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 0.0003
  mmap_warmup ..................................... False
  model_name ...................................... llama2
  model_type ...................................... encoder_or_decoder
  new_tokens ...................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_kv .......................... 32
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_layers ...................................... 32
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  optimizer ....................................... adam
  override_opt_param_scheduler .................... False
  padded_vocab_size ............................... 32000
  parallel_attn ................................... False
  parallel_layernorm .............................. False
  params_dtype .................................... torch.bfloat16
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... PositionEmbeddingType.rotary
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... selective
  recompute_method ................................ None
  recompute_num_layers ............................ 1
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  rope_scaling_factor ............................. 1.0
  rope_theta ...................................... 10000.0
  sample_rate ..................................... 1.0
  save ............................................ /root/models/llama-2-7b-chat-hf-megatron_shard_tp_2_pp_1-pretrained
  save_interval ................................... 800
  scalar_loss_mask ................................ 0.0
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 4096
  sequence_parallel ............................... True
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_iters ...................................... []
  split ........................................... 969, 30, 1
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  tensor_model_parallel_size ...................... 2
  tensorboard_dir ................................. /root/models/llama-2-7b-chat-hf-megatron_shard_tp_2_pp_1-pretrained/logging
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  tie_embed_logits ................................ False
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. SentencePieceTokenizer
  train_data_path ................................. None
  train_iters ..................................... 1000
  train_samples ................................... None
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 1
  use_bias ........................................ False
  use_checkpoint_args ............................. True
  use_checkpoint_opt_param_scheduler .............. False
  use_contiguous_buffers_in_local_ddp ............. True
  use_cpu_initialization .......................... None
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. True
  use_one_sent_docs ............................... False
  use_post_ln ..................................... False
  use_ring_exchange_p2p ........................... False
  use_rms_norm .................................... True
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_extra_ids_list ............................ [bib_ref],[/bib_ref],[fig_ref],[/fig_ref],[bib],[/bib],[fig],[/fig],[table],[/table],[formula],[/formula]
  vocab_file ...................................... /root/models/llama-2-7b-chat-hf-megatron/tokenizer.model
  wandb_api_key ................................... None
  wandb_entity .................................... meditron
  wandb_id ........................................ None
  wandb_logger .................................... False
  wandb_project ................................... None
  wandb_resume .................................... False
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  world_size ...................................... 2
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 2
> building SentencePieceTokenizer tokenizer ...
Special tokens: {'<s>': 1, '</s>': 2}
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> initializing torch distributed ...
> initialized tensor model parallel with size 2
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
Special tokens: {'<s>': 1, '</s>': 2}
> setting tensorboard ...
time to initialize megatron (seconds): 9.228
[after megatron is initialized] datetime: 2023-10-25 07:08:08 
Building model ...
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 3369340928
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 3369340928
/usr/local/lib/python3.10/dist-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/usr/local/lib/python3.10/dist-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
> learning rate decay style: cosine
 loading release checkpoint from /root/models/llama-2-7b-chat-hf-megatron_shard_tp_2_pp_1
 checkpoint version 3.0
  successfully loaded checkpoint from /root/models/llama-2-7b-chat-hf-megatron_shard_tp_2_pp_1 at iteration 0
/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:2885: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:2885: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
(min, max) time across ranks (ms):
    load-checkpoint ................................: (3364.20, 3364.29)
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-10-25 07:08:12 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      2000
    validation: 120
    test:       20
> building train, validation, and test datasets ...
Single data path provided for train, valid & test
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.000733 seconds
    number of documents: 9600
    number of tokens: 67114666
 > dataset split:
    train:
     document indices in [0, 9302) total of 9302 documents
    validation:
     document indices in [9302, 9590) total of 288 documents
    test:
     document indices in [9590, 9600) total of 10 documents
 > loading doc-idx mapping from /root/datasets/booksum_megatron/booksum_megatron_text_document_train_indexmap_2000ns_4096sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /root/datasets/booksum_megatron/booksum_megatron_text_document_train_indexmap_2000ns_4096sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /root/datasets/booksum_megatron/booksum_megatron_text_document_train_indexmap_2000ns_4096sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of tokens: 64609348
    total number of samples: 15774
    total number of epochs: 1
 > loading doc-idx mapping from /root/datasets/booksum_megatron/booksum_megatron_text_document_valid_indexmap_120ns_4096sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /root/datasets/booksum_megatron/booksum_megatron_text_document_valid_indexmap_120ns_4096sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /root/datasets/booksum_megatron/booksum_megatron_text_document_valid_indexmap_120ns_4096sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.001 seconds
    total number of tokens: 2362927
    total number of samples: 577
    total number of epochs: 1
 > loading doc-idx mapping from /root/datasets/booksum_megatron/booksum_megatron_text_document_test_indexmap_20ns_4096sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /root/datasets/booksum_megatron/booksum_megatron_text_document_test_indexmap_20ns_4096sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /root/datasets/booksum_megatron/booksum_megatron_text_document_test_indexmap_20ns_4096sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.001 seconds
    total number of tokens: 142391
    total number of samples: 35
    total number of epochs: 1
> finished creating datasets ...
[after dataloaders are built] datetime: 2023-10-25 07:08:12 
done with setup ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (3636.10, 3636.28)
    train/valid/test-data-iterators-setup ..........: (274.78, 603.84)
training ...
[before the start of training step] datetime: 2023-10-25 07:08:12 
 iteration        1/    1000 | consumed samples:            2 | elapsed time per iteration (ms): 3958.7 | rate (tokens/sec): 2069.36 | learning rate: 3.000E-06 | global batch size:     2 | lm loss: 2.397206E+00 | loss scale: 1.0 | grad norm: 12.353 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 0] (after 1 iterations) memory (MB) | allocated: 57921.8076171875 | max allocated: 57921.810546875 | reserved: 67268.0 | max reserved: 67268.0
[Rank 1] (after 1 iterations) memory (MB) | allocated: 57921.8076171875 | max allocated: 57921.810546875 | reserved: 67268.0 | max reserved: 67268.0
 iteration        2/    1000 | consumed samples:            4 | elapsed time per iteration (ms): 1328.4 | rate (tokens/sec): 6166.85 | learning rate: 6.000E-06 | global batch size:     2 | lm loss: 2.312991E+00 | loss scale: 1.0 | grad norm: 9.050 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/    1000 | consumed samples:            6 | elapsed time per iteration (ms): 1313.3 | rate (tokens/sec): 6237.59 | learning rate: 9.000E-06 | global batch size:     2 | lm loss: 2.555927E+00 | loss scale: 1.0 | grad norm: 11.610 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/    1000 | consumed samples:            8 | elapsed time per iteration (ms): 1321.5 | rate (tokens/sec): 6198.91 | learning rate: 1.200E-05 | global batch size:     2 | lm loss: 1.938117E+00 | loss scale: 1.0 | grad norm: 8.160 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/    1000 | consumed samples:           10 | elapsed time per iteration (ms): 1311.6 | rate (tokens/sec): 6245.58 | learning rate: 1.500E-05 | global batch size:     2 | lm loss: 2.367347E+00 | loss scale: 1.0 | grad norm: 6.834 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/    1000 | consumed samples:           12 | elapsed time per iteration (ms): 1313.9 | rate (tokens/sec): 6234.91 | learning rate: 1.800E-05 | global batch size:     2 | lm loss: 1.940790E+00 | loss scale: 1.0 | grad norm: 8.220 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/    1000 | consumed samples:           14 | elapsed time per iteration (ms): 1318.0 | rate (tokens/sec): 6215.36 | learning rate: 2.100E-05 | global batch size:     2 | lm loss: 2.281217E+00 | loss scale: 1.0 | grad norm: 4.886 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/    1000 | consumed samples:           16 | elapsed time per iteration (ms): 1318.0 | rate (tokens/sec): 6215.43 | learning rate: 2.400E-05 | global batch size:     2 | lm loss: 2.364722E+00 | loss scale: 1.0 | grad norm: 7.762 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/    1000 | consumed samples:           18 | elapsed time per iteration (ms): 1317.0 | rate (tokens/sec): 6220.35 | learning rate: 2.700E-05 | global batch size:     2 | lm loss: 2.220894E+00 | loss scale: 1.0 | grad norm: 5.381 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/    1000 | consumed samples:           20 | elapsed time per iteration (ms): 1317.3 | rate (tokens/sec): 6218.90 | learning rate: 3.000E-05 | global batch size:     2 | lm loss: 2.291145E+00 | loss scale: 1.0 | grad norm: 4.232 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/    1000 | consumed samples:           22 | elapsed time per iteration (ms): 1317.0 | rate (tokens/sec): 6220.23 | learning rate: 3.300E-05 | global batch size:     2 | lm loss: 1.868511E+00 | loss scale: 1.0 | grad norm: 6.704 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/    1000 | consumed samples:           24 | elapsed time per iteration (ms): 1318.0 | rate (tokens/sec): 6215.24 | learning rate: 3.600E-05 | global batch size:     2 | lm loss: 2.143490E+00 | loss scale: 1.0 | grad norm: 3.931 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/    1000 | consumed samples:           26 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.60 | learning rate: 3.900E-05 | global batch size:     2 | lm loss: 1.980940E+00 | loss scale: 1.0 | grad norm: 6.853 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/    1000 | consumed samples:           28 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.82 | learning rate: 4.200E-05 | global batch size:     2 | lm loss: 2.430876E+00 | loss scale: 1.0 | grad norm: 5.267 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/    1000 | consumed samples:           30 | elapsed time per iteration (ms): 1321.9 | rate (tokens/sec): 6197.24 | learning rate: 4.500E-05 | global batch size:     2 | lm loss: 2.308771E+00 | loss scale: 1.0 | grad norm: 4.512 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/    1000 | consumed samples:           32 | elapsed time per iteration (ms): 1321.7 | rate (tokens/sec): 6197.94 | learning rate: 4.800E-05 | global batch size:     2 | lm loss: 2.100826E+00 | loss scale: 1.0 | grad norm: 4.901 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/    1000 | consumed samples:           34 | elapsed time per iteration (ms): 1320.6 | rate (tokens/sec): 6203.46 | learning rate: 5.100E-05 | global batch size:     2 | lm loss: 2.372378E+00 | loss scale: 1.0 | grad norm: 3.981 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/    1000 | consumed samples:           36 | elapsed time per iteration (ms): 1321.2 | rate (tokens/sec): 6200.37 | learning rate: 5.400E-05 | global batch size:     2 | lm loss: 2.170772E+00 | loss scale: 1.0 | grad norm: 3.388 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/    1000 | consumed samples:           38 | elapsed time per iteration (ms): 1322.3 | rate (tokens/sec): 6195.48 | learning rate: 5.700E-05 | global batch size:     2 | lm loss: 2.028994E+00 | loss scale: 1.0 | grad norm: 3.730 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/    1000 | consumed samples:           40 | elapsed time per iteration (ms): 1322.2 | rate (tokens/sec): 6195.92 | learning rate: 6.000E-05 | global batch size:     2 | lm loss: 1.485305E+00 | loss scale: 1.0 | grad norm: 3.604 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/    1000 | consumed samples:           42 | elapsed time per iteration (ms): 1322.9 | rate (tokens/sec): 6192.34 | learning rate: 6.300E-05 | global batch size:     2 | lm loss: 2.193683E+00 | loss scale: 1.0 | grad norm: 4.897 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/    1000 | consumed samples:           44 | elapsed time per iteration (ms): 1324.0 | rate (tokens/sec): 6187.41 | learning rate: 6.600E-05 | global batch size:     2 | lm loss: 2.200164E+00 | loss scale: 1.0 | grad norm: 3.304 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/    1000 | consumed samples:           46 | elapsed time per iteration (ms): 1322.9 | rate (tokens/sec): 6192.55 | learning rate: 6.900E-05 | global batch size:     2 | lm loss: 2.128103E+00 | loss scale: 1.0 | grad norm: 4.973 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/    1000 | consumed samples:           48 | elapsed time per iteration (ms): 1324.1 | rate (tokens/sec): 6186.90 | learning rate: 7.200E-05 | global batch size:     2 | lm loss: 2.253317E+00 | loss scale: 1.0 | grad norm: 3.671 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/    1000 | consumed samples:           50 | elapsed time per iteration (ms): 1325.2 | rate (tokens/sec): 6181.70 | learning rate: 7.500E-05 | global batch size:     2 | lm loss: 2.333894E+00 | loss scale: 1.0 | grad norm: 5.086 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/    1000 | consumed samples:           52 | elapsed time per iteration (ms): 1324.3 | rate (tokens/sec): 6185.92 | learning rate: 7.800E-05 | global batch size:     2 | lm loss: 2.421779E+00 | loss scale: 1.0 | grad norm: 3.976 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/    1000 | consumed samples:           54 | elapsed time per iteration (ms): 1326.4 | rate (tokens/sec): 6175.93 | learning rate: 8.100E-05 | global batch size:     2 | lm loss: 2.298848E+00 | loss scale: 1.0 | grad norm: 3.668 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/    1000 | consumed samples:           56 | elapsed time per iteration (ms): 1325.6 | rate (tokens/sec): 6179.90 | learning rate: 8.400E-05 | global batch size:     2 | lm loss: 2.215703E+00 | loss scale: 1.0 | grad norm: 3.912 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/    1000 | consumed samples:           58 | elapsed time per iteration (ms): 1334.5 | rate (tokens/sec): 6138.79 | learning rate: 8.700E-05 | global batch size:     2 | lm loss: 2.280051E+00 | loss scale: 1.0 | grad norm: 4.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/    1000 | consumed samples:           60 | elapsed time per iteration (ms): 1325.8 | rate (tokens/sec): 6178.84 | learning rate: 9.000E-05 | global batch size:     2 | lm loss: 2.377273E+00 | loss scale: 1.0 | grad norm: 7.107 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/    1000 | consumed samples:           62 | elapsed time per iteration (ms): 1325.1 | rate (tokens/sec): 6182.26 | learning rate: 9.300E-05 | global batch size:     2 | lm loss: 1.989434E+00 | loss scale: 1.0 | grad norm: 4.070 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/    1000 | consumed samples:           64 | elapsed time per iteration (ms): 1327.9 | rate (tokens/sec): 6169.34 | learning rate: 9.600E-05 | global batch size:     2 | lm loss: 2.555868E+00 | loss scale: 1.0 | grad norm: 6.437 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/    1000 | consumed samples:           66 | elapsed time per iteration (ms): 1329.0 | rate (tokens/sec): 6163.87 | learning rate: 9.900E-05 | global batch size:     2 | lm loss: 2.188085E+00 | loss scale: 1.0 | grad norm: 4.115 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/    1000 | consumed samples:           68 | elapsed time per iteration (ms): 1327.9 | rate (tokens/sec): 6169.28 | learning rate: 1.020E-04 | global batch size:     2 | lm loss: 2.451468E+00 | loss scale: 1.0 | grad norm: 3.665 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/    1000 | consumed samples:           70 | elapsed time per iteration (ms): 1328.2 | rate (tokens/sec): 6167.73 | learning rate: 1.050E-04 | global batch size:     2 | lm loss: 2.176831E+00 | loss scale: 1.0 | grad norm: 3.656 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/    1000 | consumed samples:           72 | elapsed time per iteration (ms): 1326.1 | rate (tokens/sec): 6177.64 | learning rate: 1.080E-04 | global batch size:     2 | lm loss: 2.134131E+00 | loss scale: 1.0 | grad norm: 2.882 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/    1000 | consumed samples:           74 | elapsed time per iteration (ms): 1326.1 | rate (tokens/sec): 6177.62 | learning rate: 1.110E-04 | global batch size:     2 | lm loss: 2.401008E+00 | loss scale: 1.0 | grad norm: 3.053 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/    1000 | consumed samples:           76 | elapsed time per iteration (ms): 1327.3 | rate (tokens/sec): 6171.86 | learning rate: 1.140E-04 | global batch size:     2 | lm loss: 2.444999E+00 | loss scale: 1.0 | grad norm: 3.379 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/    1000 | consumed samples:           78 | elapsed time per iteration (ms): 1326.8 | rate (tokens/sec): 6174.37 | learning rate: 1.170E-04 | global batch size:     2 | lm loss: 2.287505E+00 | loss scale: 1.0 | grad norm: 3.315 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/    1000 | consumed samples:           80 | elapsed time per iteration (ms): 1328.5 | rate (tokens/sec): 6166.26 | learning rate: 1.200E-04 | global batch size:     2 | lm loss: 2.296917E+00 | loss scale: 1.0 | grad norm: 2.804 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/    1000 | consumed samples:           82 | elapsed time per iteration (ms): 1328.4 | rate (tokens/sec): 6166.94 | learning rate: 1.230E-04 | global batch size:     2 | lm loss: 2.322480E+00 | loss scale: 1.0 | grad norm: 3.186 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/    1000 | consumed samples:           84 | elapsed time per iteration (ms): 1328.2 | rate (tokens/sec): 6167.55 | learning rate: 1.260E-04 | global batch size:     2 | lm loss: 2.237446E+00 | loss scale: 1.0 | grad norm: 4.765 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/    1000 | consumed samples:           86 | elapsed time per iteration (ms): 1327.4 | rate (tokens/sec): 6171.60 | learning rate: 1.290E-04 | global batch size:     2 | lm loss: 2.519530E+00 | loss scale: 1.0 | grad norm: 3.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/    1000 | consumed samples:           88 | elapsed time per iteration (ms): 1328.0 | rate (tokens/sec): 6168.45 | learning rate: 1.320E-04 | global batch size:     2 | lm loss: 2.277389E+00 | loss scale: 1.0 | grad norm: 4.782 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/    1000 | consumed samples:           90 | elapsed time per iteration (ms): 1327.3 | rate (tokens/sec): 6171.96 | learning rate: 1.350E-04 | global batch size:     2 | lm loss: 2.313303E+00 | loss scale: 1.0 | grad norm: 3.940 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/    1000 | consumed samples:           92 | elapsed time per iteration (ms): 1327.9 | rate (tokens/sec): 6169.26 | learning rate: 1.380E-04 | global batch size:     2 | lm loss: 2.159349E+00 | loss scale: 1.0 | grad norm: 3.277 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/    1000 | consumed samples:           94 | elapsed time per iteration (ms): 1328.1 | rate (tokens/sec): 6168.35 | learning rate: 1.410E-04 | global batch size:     2 | lm loss: 2.296839E+00 | loss scale: 1.0 | grad norm: 3.866 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/    1000 | consumed samples:           96 | elapsed time per iteration (ms): 1326.6 | rate (tokens/sec): 6175.35 | learning rate: 1.440E-04 | global batch size:     2 | lm loss: 2.386708E+00 | loss scale: 1.0 | grad norm: 4.411 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/    1000 | consumed samples:           98 | elapsed time per iteration (ms): 1326.7 | rate (tokens/sec): 6174.65 | learning rate: 1.470E-04 | global batch size:     2 | lm loss: 2.451496E+00 | loss scale: 1.0 | grad norm: 3.677 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/    1000 | consumed samples:          100 | elapsed time per iteration (ms): 1326.1 | rate (tokens/sec): 6177.42 | learning rate: 1.500E-04 | global batch size:     2 | lm loss: 2.913015E+00 | loss scale: 1.0 | grad norm: 7.845 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/    1000 | consumed samples:          102 | elapsed time per iteration (ms): 1328.1 | rate (tokens/sec): 6168.19 | learning rate: 1.530E-04 | global batch size:     2 | lm loss: 1.975007E+00 | loss scale: 1.0 | grad norm: 3.495 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/    1000 | consumed samples:          104 | elapsed time per iteration (ms): 1327.7 | rate (tokens/sec): 6170.21 | learning rate: 1.560E-04 | global batch size:     2 | lm loss: 2.442436E+00 | loss scale: 1.0 | grad norm: 3.491 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/    1000 | consumed samples:          106 | elapsed time per iteration (ms): 1327.0 | rate (tokens/sec): 6173.25 | learning rate: 1.590E-04 | global batch size:     2 | lm loss: 2.314548E+00 | loss scale: 1.0 | grad norm: 4.018 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/    1000 | consumed samples:          108 | elapsed time per iteration (ms): 1326.5 | rate (tokens/sec): 6175.63 | learning rate: 1.620E-04 | global batch size:     2 | lm loss: 2.115440E+00 | loss scale: 1.0 | grad norm: 3.953 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/    1000 | consumed samples:          110 | elapsed time per iteration (ms): 1325.9 | rate (tokens/sec): 6178.41 | learning rate: 1.650E-04 | global batch size:     2 | lm loss: 2.357689E+00 | loss scale: 1.0 | grad norm: 4.066 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/    1000 | consumed samples:          112 | elapsed time per iteration (ms): 1336.4 | rate (tokens/sec): 6129.88 | learning rate: 1.680E-04 | global batch size:     2 | lm loss: 2.293126E+00 | loss scale: 1.0 | grad norm: 4.066 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/    1000 | consumed samples:          114 | elapsed time per iteration (ms): 1325.2 | rate (tokens/sec): 6181.62 | learning rate: 1.710E-04 | global batch size:     2 | lm loss: 2.429311E+00 | loss scale: 1.0 | grad norm: 3.086 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/    1000 | consumed samples:          116 | elapsed time per iteration (ms): 1327.1 | rate (tokens/sec): 6172.69 | learning rate: 1.740E-04 | global batch size:     2 | lm loss: 2.284292E+00 | loss scale: 1.0 | grad norm: 3.157 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/    1000 | consumed samples:          118 | elapsed time per iteration (ms): 1326.2 | rate (tokens/sec): 6177.12 | learning rate: 1.770E-04 | global batch size:     2 | lm loss: 2.339057E+00 | loss scale: 1.0 | grad norm: 3.206 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/    1000 | consumed samples:          120 | elapsed time per iteration (ms): 1325.5 | rate (tokens/sec): 6180.37 | learning rate: 1.800E-04 | global batch size:     2 | lm loss: 2.203284E+00 | loss scale: 1.0 | grad norm: 3.051 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/    1000 | consumed samples:          122 | elapsed time per iteration (ms): 1325.6 | rate (tokens/sec): 6180.04 | learning rate: 1.830E-04 | global batch size:     2 | lm loss: 2.344268E+00 | loss scale: 1.0 | grad norm: 2.677 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/    1000 | consumed samples:          124 | elapsed time per iteration (ms): 1327.6 | rate (tokens/sec): 6170.60 | learning rate: 1.860E-04 | global batch size:     2 | lm loss: 2.298485E+00 | loss scale: 1.0 | grad norm: 3.381 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/    1000 | consumed samples:          126 | elapsed time per iteration (ms): 1327.5 | rate (tokens/sec): 6170.85 | learning rate: 1.890E-04 | global batch size:     2 | lm loss: 2.529281E+00 | loss scale: 1.0 | grad norm: 2.925 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/    1000 | consumed samples:          128 | elapsed time per iteration (ms): 1326.2 | rate (tokens/sec): 6177.05 | learning rate: 1.920E-04 | global batch size:     2 | lm loss: 1.912421E+00 | loss scale: 1.0 | grad norm: 9.960 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/    1000 | consumed samples:          130 | elapsed time per iteration (ms): 1326.3 | rate (tokens/sec): 6176.80 | learning rate: 1.950E-04 | global batch size:     2 | lm loss: 2.285483E+00 | loss scale: 1.0 | grad norm: 3.480 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/    1000 | consumed samples:          132 | elapsed time per iteration (ms): 1324.9 | rate (tokens/sec): 6182.89 | learning rate: 1.980E-04 | global batch size:     2 | lm loss: 2.397935E+00 | loss scale: 1.0 | grad norm: 3.266 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/    1000 | consumed samples:          134 | elapsed time per iteration (ms): 1328.3 | rate (tokens/sec): 6167.10 | learning rate: 2.010E-04 | global batch size:     2 | lm loss: 2.649701E+00 | loss scale: 1.0 | grad norm: 3.697 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/    1000 | consumed samples:          136 | elapsed time per iteration (ms): 1326.1 | rate (tokens/sec): 6177.48 | learning rate: 2.040E-04 | global batch size:     2 | lm loss: 2.470281E+00 | loss scale: 1.0 | grad norm: 8.232 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/    1000 | consumed samples:          138 | elapsed time per iteration (ms): 1327.3 | rate (tokens/sec): 6171.74 | learning rate: 2.070E-04 | global batch size:     2 | lm loss: 2.436308E+00 | loss scale: 1.0 | grad norm: 3.429 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/    1000 | consumed samples:          140 | elapsed time per iteration (ms): 1328.2 | rate (tokens/sec): 6167.87 | learning rate: 2.100E-04 | global batch size:     2 | lm loss: 2.266140E+00 | loss scale: 1.0 | grad norm: 4.832 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/    1000 | consumed samples:          142 | elapsed time per iteration (ms): 1326.0 | rate (tokens/sec): 6178.18 | learning rate: 2.130E-04 | global batch size:     2 | lm loss: 2.532948E+00 | loss scale: 1.0 | grad norm: 3.553 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/    1000 | consumed samples:          144 | elapsed time per iteration (ms): 1325.4 | rate (tokens/sec): 6180.90 | learning rate: 2.160E-04 | global batch size:     2 | lm loss: 2.459806E+00 | loss scale: 1.0 | grad norm: 4.097 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/    1000 | consumed samples:          146 | elapsed time per iteration (ms): 1325.6 | rate (tokens/sec): 6180.01 | learning rate: 2.190E-04 | global batch size:     2 | lm loss: 2.645602E+00 | loss scale: 1.0 | grad norm: 5.618 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/    1000 | consumed samples:          148 | elapsed time per iteration (ms): 1326.7 | rate (tokens/sec): 6174.79 | learning rate: 2.220E-04 | global batch size:     2 | lm loss: 2.489230E+00 | loss scale: 1.0 | grad norm: 3.663 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/    1000 | consumed samples:          150 | elapsed time per iteration (ms): 1326.0 | rate (tokens/sec): 6177.95 | learning rate: 2.250E-04 | global batch size:     2 | lm loss: 2.567862E+00 | loss scale: 1.0 | grad norm: 5.380 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/    1000 | consumed samples:          152 | elapsed time per iteration (ms): 1328.4 | rate (tokens/sec): 6166.65 | learning rate: 2.280E-04 | global batch size:     2 | lm loss: 2.773865E+00 | loss scale: 1.0 | grad norm: 16.997 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/    1000 | consumed samples:          154 | elapsed time per iteration (ms): 1326.3 | rate (tokens/sec): 6176.59 | learning rate: 2.310E-04 | global batch size:     2 | lm loss: 2.713177E+00 | loss scale: 1.0 | grad norm: 3.490 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/    1000 | consumed samples:          156 | elapsed time per iteration (ms): 1326.7 | rate (tokens/sec): 6174.82 | learning rate: 2.340E-04 | global batch size:     2 | lm loss: 2.583095E+00 | loss scale: 1.0 | grad norm: 7.692 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/    1000 | consumed samples:          158 | elapsed time per iteration (ms): 1325.8 | rate (tokens/sec): 6178.99 | learning rate: 2.370E-04 | global batch size:     2 | lm loss: 2.620795E+00 | loss scale: 1.0 | grad norm: 5.036 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/    1000 | consumed samples:          160 | elapsed time per iteration (ms): 1324.2 | rate (tokens/sec): 6186.24 | learning rate: 2.400E-04 | global batch size:     2 | lm loss: 2.243221E+00 | loss scale: 1.0 | grad norm: 2.947 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/    1000 | consumed samples:          162 | elapsed time per iteration (ms): 1324.2 | rate (tokens/sec): 6186.32 | learning rate: 2.430E-04 | global batch size:     2 | lm loss: 2.493064E+00 | loss scale: 1.0 | grad norm: 15.119 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/    1000 | consumed samples:          164 | elapsed time per iteration (ms): 1325.6 | rate (tokens/sec): 6179.91 | learning rate: 2.460E-04 | global batch size:     2 | lm loss: 2.496998E+00 | loss scale: 1.0 | grad norm: 4.410 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/    1000 | consumed samples:          166 | elapsed time per iteration (ms): 1327.5 | rate (tokens/sec): 6171.02 | learning rate: 2.490E-04 | global batch size:     2 | lm loss: 2.404893E+00 | loss scale: 1.0 | grad norm: 2.976 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/    1000 | consumed samples:          168 | elapsed time per iteration (ms): 1325.8 | rate (tokens/sec): 6178.78 | learning rate: 2.520E-04 | global batch size:     2 | lm loss: 2.623674E+00 | loss scale: 1.0 | grad norm: 4.138 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/    1000 | consumed samples:          170 | elapsed time per iteration (ms): 1324.0 | rate (tokens/sec): 6187.34 | learning rate: 2.550E-04 | global batch size:     2 | lm loss: 2.595433E+00 | loss scale: 1.0 | grad norm: 4.258 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/    1000 | consumed samples:          172 | elapsed time per iteration (ms): 1325.9 | rate (tokens/sec): 6178.53 | learning rate: 2.580E-04 | global batch size:     2 | lm loss: 2.445133E+00 | loss scale: 1.0 | grad norm: 3.482 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/    1000 | consumed samples:          174 | elapsed time per iteration (ms): 1324.2 | rate (tokens/sec): 6186.18 | learning rate: 2.610E-04 | global batch size:     2 | lm loss: 2.613229E+00 | loss scale: 1.0 | grad norm: 7.797 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/    1000 | consumed samples:          176 | elapsed time per iteration (ms): 1324.6 | rate (tokens/sec): 6184.43 | learning rate: 2.640E-04 | global batch size:     2 | lm loss: 2.456943E+00 | loss scale: 1.0 | grad norm: 4.739 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/    1000 | consumed samples:          178 | elapsed time per iteration (ms): 1334.4 | rate (tokens/sec): 6139.12 | learning rate: 2.670E-04 | global batch size:     2 | lm loss: 2.586753E+00 | loss scale: 1.0 | grad norm: 3.368 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/    1000 | consumed samples:          180 | elapsed time per iteration (ms): 1323.5 | rate (tokens/sec): 6189.57 | learning rate: 2.700E-04 | global batch size:     2 | lm loss: 2.625363E+00 | loss scale: 1.0 | grad norm: 2.865 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/    1000 | consumed samples:          182 | elapsed time per iteration (ms): 1325.5 | rate (tokens/sec): 6180.10 | learning rate: 2.730E-04 | global batch size:     2 | lm loss: 2.460332E+00 | loss scale: 1.0 | grad norm: 3.609 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/    1000 | consumed samples:          184 | elapsed time per iteration (ms): 1323.3 | rate (tokens/sec): 6190.47 | learning rate: 2.760E-04 | global batch size:     2 | lm loss: 2.686116E+00 | loss scale: 1.0 | grad norm: 4.877 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/    1000 | consumed samples:          186 | elapsed time per iteration (ms): 1323.7 | rate (tokens/sec): 6188.93 | learning rate: 2.790E-04 | global batch size:     2 | lm loss: 2.800320E+00 | loss scale: 1.0 | grad norm: 6.365 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/    1000 | consumed samples:          188 | elapsed time per iteration (ms): 1323.9 | rate (tokens/sec): 6187.88 | learning rate: 2.820E-04 | global batch size:     2 | lm loss: 2.530423E+00 | loss scale: 1.0 | grad norm: 4.345 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/    1000 | consumed samples:          190 | elapsed time per iteration (ms): 1323.6 | rate (tokens/sec): 6189.36 | learning rate: 2.850E-04 | global batch size:     2 | lm loss: 2.613473E+00 | loss scale: 1.0 | grad norm: 3.080 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/    1000 | consumed samples:          192 | elapsed time per iteration (ms): 1324.5 | rate (tokens/sec): 6185.15 | learning rate: 2.880E-04 | global batch size:     2 | lm loss: 2.509624E+00 | loss scale: 1.0 | grad norm: 4.066 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/    1000 | consumed samples:          194 | elapsed time per iteration (ms): 1322.4 | rate (tokens/sec): 6194.71 | learning rate: 2.910E-04 | global batch size:     2 | lm loss: 2.937795E+00 | loss scale: 1.0 | grad norm: 5.440 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/    1000 | consumed samples:          196 | elapsed time per iteration (ms): 1324.5 | rate (tokens/sec): 6185.17 | learning rate: 2.940E-04 | global batch size:     2 | lm loss: 2.714191E+00 | loss scale: 1.0 | grad norm: 8.700 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/    1000 | consumed samples:          198 | elapsed time per iteration (ms): 1323.5 | rate (tokens/sec): 6189.62 | learning rate: 2.970E-04 | global batch size:     2 | lm loss: 2.554187E+00 | loss scale: 1.0 | grad norm: 5.248 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/    1000 | consumed samples:          200 | elapsed time per iteration (ms): 1323.8 | rate (tokens/sec): 6188.42 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.679384E+00 | loss scale: 1.0 | grad norm: 3.105 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      101/    1000 | consumed samples:          202 | elapsed time per iteration (ms): 1323.6 | rate (tokens/sec): 6189.05 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.477726E+00 | loss scale: 1.0 | grad norm: 4.820 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      102/    1000 | consumed samples:          204 | elapsed time per iteration (ms): 1324.3 | rate (tokens/sec): 6185.97 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.958451E+00 | loss scale: 1.0 | grad norm: 5.634 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      103/    1000 | consumed samples:          206 | elapsed time per iteration (ms): 1323.1 | rate (tokens/sec): 6191.39 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.830199E+00 | loss scale: 1.0 | grad norm: 3.534 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      104/    1000 | consumed samples:          208 | elapsed time per iteration (ms): 1323.8 | rate (tokens/sec): 6188.31 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.589195E+00 | loss scale: 1.0 | grad norm: 6.407 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      105/    1000 | consumed samples:          210 | elapsed time per iteration (ms): 1325.5 | rate (tokens/sec): 6180.46 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.912572E+00 | loss scale: 1.0 | grad norm: 5.017 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      106/    1000 | consumed samples:          212 | elapsed time per iteration (ms): 1323.2 | rate (tokens/sec): 6190.94 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.620566E+00 | loss scale: 1.0 | grad norm: 3.983 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      107/    1000 | consumed samples:          214 | elapsed time per iteration (ms): 1323.1 | rate (tokens/sec): 6191.72 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.989443E+00 | loss scale: 1.0 | grad norm: 3.962 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      108/    1000 | consumed samples:          216 | elapsed time per iteration (ms): 1324.7 | rate (tokens/sec): 6184.20 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.534422E+00 | loss scale: 1.0 | grad norm: 4.779 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      109/    1000 | consumed samples:          218 | elapsed time per iteration (ms): 1323.9 | rate (tokens/sec): 6187.64 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.861958E+00 | loss scale: 1.0 | grad norm: 3.788 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/    1000 | consumed samples:          220 | elapsed time per iteration (ms): 1324.9 | rate (tokens/sec): 6183.33 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.734628E+00 | loss scale: 1.0 | grad norm: 3.535 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      111/    1000 | consumed samples:          222 | elapsed time per iteration (ms): 1324.7 | rate (tokens/sec): 6184.00 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.884732E+00 | loss scale: 1.0 | grad norm: 6.449 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      112/    1000 | consumed samples:          224 | elapsed time per iteration (ms): 1325.7 | rate (tokens/sec): 6179.19 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.812380E+00 | loss scale: 1.0 | grad norm: 4.774 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      113/    1000 | consumed samples:          226 | elapsed time per iteration (ms): 1324.7 | rate (tokens/sec): 6184.14 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.751185E+00 | loss scale: 1.0 | grad norm: 3.599 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      114/    1000 | consumed samples:          228 | elapsed time per iteration (ms): 1322.0 | rate (tokens/sec): 6196.57 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.600448E+00 | loss scale: 1.0 | grad norm: 3.514 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      115/    1000 | consumed samples:          230 | elapsed time per iteration (ms): 1323.9 | rate (tokens/sec): 6187.75 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.794386E+00 | loss scale: 1.0 | grad norm: 3.913 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      116/    1000 | consumed samples:          232 | elapsed time per iteration (ms): 1323.2 | rate (tokens/sec): 6191.24 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.503634E+00 | loss scale: 1.0 | grad norm: 3.038 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      117/    1000 | consumed samples:          234 | elapsed time per iteration (ms): 1322.3 | rate (tokens/sec): 6195.46 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.946010E+00 | loss scale: 1.0 | grad norm: 6.681 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      118/    1000 | consumed samples:          236 | elapsed time per iteration (ms): 1323.5 | rate (tokens/sec): 6189.56 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.850064E+00 | loss scale: 1.0 | grad norm: 6.165 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      119/    1000 | consumed samples:          238 | elapsed time per iteration (ms): 1323.9 | rate (tokens/sec): 6187.83 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.690301E+00 | loss scale: 1.0 | grad norm: 3.233 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/    1000 | consumed samples:          240 | elapsed time per iteration (ms): 1323.7 | rate (tokens/sec): 6188.54 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.503530E+00 | loss scale: 1.0 | grad norm: 3.339 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      121/    1000 | consumed samples:          242 | elapsed time per iteration (ms): 1323.2 | rate (tokens/sec): 6190.86 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.449766E+00 | loss scale: 1.0 | grad norm: 3.324 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      122/    1000 | consumed samples:          244 | elapsed time per iteration (ms): 1324.5 | rate (tokens/sec): 6185.15 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.044545E+00 | loss scale: 1.0 | grad norm: 8.705 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      123/    1000 | consumed samples:          246 | elapsed time per iteration (ms): 1322.1 | rate (tokens/sec): 6196.23 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.795432E+00 | loss scale: 1.0 | grad norm: 5.661 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      124/    1000 | consumed samples:          248 | elapsed time per iteration (ms): 1322.4 | rate (tokens/sec): 6194.74 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.714685E+00 | loss scale: 1.0 | grad norm: 3.046 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      125/    1000 | consumed samples:          250 | elapsed time per iteration (ms): 1323.3 | rate (tokens/sec): 6190.72 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.533877E+00 | loss scale: 1.0 | grad norm: 3.097 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      126/    1000 | consumed samples:          252 | elapsed time per iteration (ms): 1321.7 | rate (tokens/sec): 6198.25 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.520847E+00 | loss scale: 1.0 | grad norm: 2.750 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      127/    1000 | consumed samples:          254 | elapsed time per iteration (ms): 1323.6 | rate (tokens/sec): 6189.27 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.569192E+00 | loss scale: 1.0 | grad norm: 3.422 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      128/    1000 | consumed samples:          256 | elapsed time per iteration (ms): 1325.0 | rate (tokens/sec): 6182.84 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.730805E+00 | loss scale: 1.0 | grad norm: 4.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      129/    1000 | consumed samples:          258 | elapsed time per iteration (ms): 1327.2 | rate (tokens/sec): 6172.18 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.617012E+00 | loss scale: 1.0 | grad norm: 3.132 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/    1000 | consumed samples:          260 | elapsed time per iteration (ms): 1324.9 | rate (tokens/sec): 6183.19 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.551220E+00 | loss scale: 1.0 | grad norm: 3.454 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      131/    1000 | consumed samples:          262 | elapsed time per iteration (ms): 1324.5 | rate (tokens/sec): 6184.98 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.570419E+00 | loss scale: 1.0 | grad norm: 3.156 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      132/    1000 | consumed samples:          264 | elapsed time per iteration (ms): 1323.4 | rate (tokens/sec): 6190.28 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.614414E+00 | loss scale: 1.0 | grad norm: 3.338 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      133/    1000 | consumed samples:          266 | elapsed time per iteration (ms): 1324.5 | rate (tokens/sec): 6185.10 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.756519E+00 | loss scale: 1.0 | grad norm: 8.759 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      134/    1000 | consumed samples:          268 | elapsed time per iteration (ms): 1323.5 | rate (tokens/sec): 6189.86 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.820038E+00 | loss scale: 1.0 | grad norm: 3.702 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      135/    1000 | consumed samples:          270 | elapsed time per iteration (ms): 1334.0 | rate (tokens/sec): 6140.87 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.540869E+00 | loss scale: 1.0 | grad norm: 3.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      136/    1000 | consumed samples:          272 | elapsed time per iteration (ms): 1324.9 | rate (tokens/sec): 6182.97 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.788352E+00 | loss scale: 1.0 | grad norm: 3.764 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      137/    1000 | consumed samples:          274 | elapsed time per iteration (ms): 1323.6 | rate (tokens/sec): 6189.32 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.832886E+00 | loss scale: 1.0 | grad norm: 4.697 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      138/    1000 | consumed samples:          276 | elapsed time per iteration (ms): 1321.6 | rate (tokens/sec): 6198.52 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.611187E+00 | loss scale: 1.0 | grad norm: 3.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      139/    1000 | consumed samples:          278 | elapsed time per iteration (ms): 1322.3 | rate (tokens/sec): 6195.42 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.674031E+00 | loss scale: 1.0 | grad norm: 2.555 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/    1000 | consumed samples:          280 | elapsed time per iteration (ms): 1323.3 | rate (tokens/sec): 6190.69 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.977731E+00 | loss scale: 1.0 | grad norm: 3.818 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      141/    1000 | consumed samples:          282 | elapsed time per iteration (ms): 1322.4 | rate (tokens/sec): 6194.68 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.722283E+00 | loss scale: 1.0 | grad norm: 3.436 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      142/    1000 | consumed samples:          284 | elapsed time per iteration (ms): 1324.4 | rate (tokens/sec): 6185.26 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.521124E+00 | loss scale: 1.0 | grad norm: 3.094 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      143/    1000 | consumed samples:          286 | elapsed time per iteration (ms): 1323.4 | rate (tokens/sec): 6190.29 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.058551E+00 | loss scale: 1.0 | grad norm: 5.195 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      144/    1000 | consumed samples:          288 | elapsed time per iteration (ms): 1322.1 | rate (tokens/sec): 6196.38 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.414523E+00 | loss scale: 1.0 | grad norm: 3.634 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      145/    1000 | consumed samples:          290 | elapsed time per iteration (ms): 1324.5 | rate (tokens/sec): 6185.05 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.870700E+00 | loss scale: 1.0 | grad norm: 3.227 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      146/    1000 | consumed samples:          292 | elapsed time per iteration (ms): 1322.6 | rate (tokens/sec): 6193.76 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.146979E+00 | loss scale: 1.0 | grad norm: 65.358 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      147/    1000 | consumed samples:          294 | elapsed time per iteration (ms): 1324.7 | rate (tokens/sec): 6183.86 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.097979E+00 | loss scale: 1.0 | grad norm: 5.136 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      148/    1000 | consumed samples:          296 | elapsed time per iteration (ms): 1323.4 | rate (tokens/sec): 6190.12 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.538286E+00 | loss scale: 1.0 | grad norm: 3.948 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      149/    1000 | consumed samples:          298 | elapsed time per iteration (ms): 1324.6 | rate (tokens/sec): 6184.38 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.900561E+00 | loss scale: 1.0 | grad norm: 12.938 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/    1000 | consumed samples:          300 | elapsed time per iteration (ms): 1323.4 | rate (tokens/sec): 6190.25 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.918554E+00 | loss scale: 1.0 | grad norm: 4.169 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      151/    1000 | consumed samples:          302 | elapsed time per iteration (ms): 1322.7 | rate (tokens/sec): 6193.31 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.819723E+00 | loss scale: 1.0 | grad norm: 2.777 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      152/    1000 | consumed samples:          304 | elapsed time per iteration (ms): 1321.5 | rate (tokens/sec): 6198.83 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.251071E+00 | loss scale: 1.0 | grad norm: 4.058 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      153/    1000 | consumed samples:          306 | elapsed time per iteration (ms): 1328.0 | rate (tokens/sec): 6168.72 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.948582E+00 | loss scale: 1.0 | grad norm: 4.043 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      154/    1000 | consumed samples:          308 | elapsed time per iteration (ms): 1321.8 | rate (tokens/sec): 6197.49 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.888521E+00 | loss scale: 1.0 | grad norm: 3.551 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      155/    1000 | consumed samples:          310 | elapsed time per iteration (ms): 1321.1 | rate (tokens/sec): 6200.78 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.372778E+00 | loss scale: 1.0 | grad norm: 2.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      156/    1000 | consumed samples:          312 | elapsed time per iteration (ms): 1324.4 | rate (tokens/sec): 6185.53 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.737019E+00 | loss scale: 1.0 | grad norm: 2.749 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      157/    1000 | consumed samples:          314 | elapsed time per iteration (ms): 1321.3 | rate (tokens/sec): 6200.02 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.624790E+00 | loss scale: 1.0 | grad norm: 3.498 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      158/    1000 | consumed samples:          316 | elapsed time per iteration (ms): 1323.9 | rate (tokens/sec): 6187.68 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.518334E+00 | loss scale: 1.0 | grad norm: 2.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      159/    1000 | consumed samples:          318 | elapsed time per iteration (ms): 1322.6 | rate (tokens/sec): 6193.69 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.675423E+00 | loss scale: 1.0 | grad norm: 2.625 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/    1000 | consumed samples:          320 | elapsed time per iteration (ms): 1322.1 | rate (tokens/sec): 6196.14 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.473140E+00 | loss scale: 1.0 | grad norm: 2.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      161/    1000 | consumed samples:          322 | elapsed time per iteration (ms): 1323.2 | rate (tokens/sec): 6190.94 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.628512E+00 | loss scale: 1.0 | grad norm: 2.472 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      162/    1000 | consumed samples:          324 | elapsed time per iteration (ms): 1322.9 | rate (tokens/sec): 6192.25 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.210428E+00 | loss scale: 1.0 | grad norm: 5.906 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      163/    1000 | consumed samples:          326 | elapsed time per iteration (ms): 1321.8 | rate (tokens/sec): 6197.67 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.991114E+00 | loss scale: 1.0 | grad norm: 3.841 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      164/    1000 | consumed samples:          328 | elapsed time per iteration (ms): 1322.5 | rate (tokens/sec): 6194.30 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.551967E+00 | loss scale: 1.0 | grad norm: 3.178 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      165/    1000 | consumed samples:          330 | elapsed time per iteration (ms): 1321.5 | rate (tokens/sec): 6199.23 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.689125E+00 | loss scale: 1.0 | grad norm: 2.807 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      166/    1000 | consumed samples:          332 | elapsed time per iteration (ms): 1324.4 | rate (tokens/sec): 6185.67 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.597830E+00 | loss scale: 1.0 | grad norm: 2.998 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      167/    1000 | consumed samples:          334 | elapsed time per iteration (ms): 1322.1 | rate (tokens/sec): 6196.13 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.743018E+00 | loss scale: 1.0 | grad norm: 5.048 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      168/    1000 | consumed samples:          336 | elapsed time per iteration (ms): 1323.9 | rate (tokens/sec): 6187.75 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.640133E+00 | loss scale: 1.0 | grad norm: 3.889 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      169/    1000 | consumed samples:          338 | elapsed time per iteration (ms): 1323.4 | rate (tokens/sec): 6189.95 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.743386E+00 | loss scale: 1.0 | grad norm: 3.888 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/    1000 | consumed samples:          340 | elapsed time per iteration (ms): 1323.3 | rate (tokens/sec): 6190.46 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.087597E+00 | loss scale: 1.0 | grad norm: 5.472 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      171/    1000 | consumed samples:          342 | elapsed time per iteration (ms): 1323.5 | rate (tokens/sec): 6189.55 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.474140E+00 | loss scale: 1.0 | grad norm: 2.826 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      172/    1000 | consumed samples:          344 | elapsed time per iteration (ms): 1322.3 | rate (tokens/sec): 6195.26 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.474733E+00 | loss scale: 1.0 | grad norm: 3.398 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      173/    1000 | consumed samples:          346 | elapsed time per iteration (ms): 1322.3 | rate (tokens/sec): 6195.21 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.519970E+00 | loss scale: 1.0 | grad norm: 2.520 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      174/    1000 | consumed samples:          348 | elapsed time per iteration (ms): 1321.9 | rate (tokens/sec): 6196.99 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.485912E+00 | loss scale: 1.0 | grad norm: 2.950 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      175/    1000 | consumed samples:          350 | elapsed time per iteration (ms): 1324.2 | rate (tokens/sec): 6186.28 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.797243E+00 | loss scale: 1.0 | grad norm: 3.559 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      176/    1000 | consumed samples:          352 | elapsed time per iteration (ms): 1323.2 | rate (tokens/sec): 6191.18 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.711022E+00 | loss scale: 1.0 | grad norm: 2.598 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      177/    1000 | consumed samples:          354 | elapsed time per iteration (ms): 1323.9 | rate (tokens/sec): 6187.57 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.813356E+00 | loss scale: 1.0 | grad norm: 3.249 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      178/    1000 | consumed samples:          356 | elapsed time per iteration (ms): 1321.7 | rate (tokens/sec): 6198.26 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.516393E+00 | loss scale: 1.0 | grad norm: 2.320 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      179/    1000 | consumed samples:          358 | elapsed time per iteration (ms): 1321.3 | rate (tokens/sec): 6199.98 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.003644E+00 | loss scale: 1.0 | grad norm: 2.840 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/    1000 | consumed samples:          360 | elapsed time per iteration (ms): 1321.6 | rate (tokens/sec): 6198.68 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.939183E+00 | loss scale: 1.0 | grad norm: 3.576 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      181/    1000 | consumed samples:          362 | elapsed time per iteration (ms): 1321.8 | rate (tokens/sec): 6197.73 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.873918E+00 | loss scale: 1.0 | grad norm: 3.094 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      182/    1000 | consumed samples:          364 | elapsed time per iteration (ms): 1322.1 | rate (tokens/sec): 6196.32 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.772729E+00 | loss scale: 1.0 | grad norm: 2.715 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      183/    1000 | consumed samples:          366 | elapsed time per iteration (ms): 1321.4 | rate (tokens/sec): 6199.50 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.964675E+00 | loss scale: 1.0 | grad norm: 3.016 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      184/    1000 | consumed samples:          368 | elapsed time per iteration (ms): 1322.0 | rate (tokens/sec): 6196.51 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.809912E+00 | loss scale: 1.0 | grad norm: 3.300 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      185/    1000 | consumed samples:          370 | elapsed time per iteration (ms): 1322.3 | rate (tokens/sec): 6195.34 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.122297E+00 | loss scale: 1.0 | grad norm: 3.481 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      186/    1000 | consumed samples:          372 | elapsed time per iteration (ms): 1321.2 | rate (tokens/sec): 6200.53 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.019927E+00 | loss scale: 1.0 | grad norm: 4.392 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      187/    1000 | consumed samples:          374 | elapsed time per iteration (ms): 1321.7 | rate (tokens/sec): 6198.29 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.815769E+00 | loss scale: 1.0 | grad norm: 3.872 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      188/    1000 | consumed samples:          376 | elapsed time per iteration (ms): 1321.5 | rate (tokens/sec): 6198.83 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.273483E+00 | loss scale: 1.0 | grad norm: 4.657 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      189/    1000 | consumed samples:          378 | elapsed time per iteration (ms): 1320.3 | rate (tokens/sec): 6204.70 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.862960E+00 | loss scale: 1.0 | grad norm: 3.235 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/    1000 | consumed samples:          380 | elapsed time per iteration (ms): 1321.7 | rate (tokens/sec): 6197.87 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.585306E+00 | loss scale: 1.0 | grad norm: 2.360 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      191/    1000 | consumed samples:          382 | elapsed time per iteration (ms): 1322.6 | rate (tokens/sec): 6193.97 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.771188E+00 | loss scale: 1.0 | grad norm: 3.220 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      192/    1000 | consumed samples:          384 | elapsed time per iteration (ms): 1322.2 | rate (tokens/sec): 6195.97 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.548198E+00 | loss scale: 1.0 | grad norm: 2.494 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      193/    1000 | consumed samples:          386 | elapsed time per iteration (ms): 1333.2 | rate (tokens/sec): 6144.43 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.723130E+00 | loss scale: 1.0 | grad norm: 2.355 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      194/    1000 | consumed samples:          388 | elapsed time per iteration (ms): 1323.0 | rate (tokens/sec): 6191.79 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.737650E+00 | loss scale: 1.0 | grad norm: 3.471 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      195/    1000 | consumed samples:          390 | elapsed time per iteration (ms): 1322.6 | rate (tokens/sec): 6193.98 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.777502E+00 | loss scale: 1.0 | grad norm: 2.541 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      196/    1000 | consumed samples:          392 | elapsed time per iteration (ms): 1322.3 | rate (tokens/sec): 6195.25 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.566962E+00 | loss scale: 1.0 | grad norm: 5.384 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      197/    1000 | consumed samples:          394 | elapsed time per iteration (ms): 1323.0 | rate (tokens/sec): 6191.98 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.551510E+00 | loss scale: 1.0 | grad norm: 2.688 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      198/    1000 | consumed samples:          396 | elapsed time per iteration (ms): 1319.8 | rate (tokens/sec): 6206.93 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.591590E+00 | loss scale: 1.0 | grad norm: 2.791 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      199/    1000 | consumed samples:          398 | elapsed time per iteration (ms): 1321.6 | rate (tokens/sec): 6198.55 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.826868E+00 | loss scale: 1.0 | grad norm: 3.060 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/    1000 | consumed samples:          400 | elapsed time per iteration (ms): 1321.5 | rate (tokens/sec): 6199.10 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.789511E+00 | loss scale: 1.0 | grad norm: 2.687 | number of skipped iterations:   0 | number of nan iterations:   0 |
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 validation loss at iteration 200 | lm loss value: 2.609763E+00 | lm loss PPL: 1.359582E+01 | ppl value: 1.364894E+01 | lm accuracy value: 4.489258E-01 | count loss mask value: 4.096000E+03 | 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 iteration      201/    1000 | consumed samples:          402 | elapsed time per iteration (ms): 5179.4 | rate (tokens/sec): 1581.65 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.853603E+00 | loss scale: 1.0 | grad norm: 2.407 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      202/    1000 | consumed samples:          404 | elapsed time per iteration (ms): 1320.7 | rate (tokens/sec): 6202.95 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.192470E+00 | loss scale: 1.0 | grad norm: 4.496 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      203/    1000 | consumed samples:          406 | elapsed time per iteration (ms): 1321.8 | rate (tokens/sec): 6197.58 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.752295E+00 | loss scale: 1.0 | grad norm: 2.837 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      204/    1000 | consumed samples:          408 | elapsed time per iteration (ms): 1321.8 | rate (tokens/sec): 6197.40 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.823688E+00 | loss scale: 1.0 | grad norm: 2.851 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      205/    1000 | consumed samples:          410 | elapsed time per iteration (ms): 1323.1 | rate (tokens/sec): 6191.55 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.864389E+00 | loss scale: 1.0 | grad norm: 7.397 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      206/    1000 | consumed samples:          412 | elapsed time per iteration (ms): 1323.1 | rate (tokens/sec): 6191.62 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.830678E+00 | loss scale: 1.0 | grad norm: 2.342 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      207/    1000 | consumed samples:          414 | elapsed time per iteration (ms): 1320.8 | rate (tokens/sec): 6202.14 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.507009E+00 | loss scale: 1.0 | grad norm: 2.274 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      208/    1000 | consumed samples:          416 | elapsed time per iteration (ms): 1401.8 | rate (tokens/sec): 5844.04 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.562608E+00 | loss scale: 1.0 | grad norm: 2.814 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      209/    1000 | consumed samples:          418 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.68 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.543001E+00 | loss scale: 1.0 | grad norm: 2.913 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      210/    1000 | consumed samples:          420 | elapsed time per iteration (ms): 1321.1 | rate (tokens/sec): 6200.88 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.709531E+00 | loss scale: 1.0 | grad norm: 4.123 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      211/    1000 | consumed samples:          422 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.86 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.674223E+00 | loss scale: 1.0 | grad norm: 3.772 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      212/    1000 | consumed samples:          424 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.35 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.502180E+00 | loss scale: 1.0 | grad norm: 3.731 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      213/    1000 | consumed samples:          426 | elapsed time per iteration (ms): 1321.0 | rate (tokens/sec): 6201.52 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.648924E+00 | loss scale: 1.0 | grad norm: 2.988 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      214/    1000 | consumed samples:          428 | elapsed time per iteration (ms): 1320.6 | rate (tokens/sec): 6203.18 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.744302E+00 | loss scale: 1.0 | grad norm: 2.673 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      215/    1000 | consumed samples:          430 | elapsed time per iteration (ms): 1321.4 | rate (tokens/sec): 6199.61 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.695495E+00 | loss scale: 1.0 | grad norm: 2.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      216/    1000 | consumed samples:          432 | elapsed time per iteration (ms): 1324.2 | rate (tokens/sec): 6186.20 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.628291E+00 | loss scale: 1.0 | grad norm: 2.383 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      217/    1000 | consumed samples:          434 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.47 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.754937E+00 | loss scale: 1.0 | grad norm: 2.321 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      218/    1000 | consumed samples:          436 | elapsed time per iteration (ms): 1320.2 | rate (tokens/sec): 6205.32 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.447978E+00 | loss scale: 1.0 | grad norm: 2.788 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      219/    1000 | consumed samples:          438 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.53 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.901406E+00 | loss scale: 1.0 | grad norm: 3.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      220/    1000 | consumed samples:          440 | elapsed time per iteration (ms): 1321.0 | rate (tokens/sec): 6201.38 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.642701E+00 | loss scale: 1.0 | grad norm: 2.803 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      221/    1000 | consumed samples:          442 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.62 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.655083E+00 | loss scale: 1.0 | grad norm: 3.109 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      222/    1000 | consumed samples:          444 | elapsed time per iteration (ms): 1322.9 | rate (tokens/sec): 6192.58 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.167689E+00 | loss scale: 1.0 | grad norm: 4.498 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      223/    1000 | consumed samples:          446 | elapsed time per iteration (ms): 1321.9 | rate (tokens/sec): 6197.34 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.842919E+00 | loss scale: 1.0 | grad norm: 3.097 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      224/    1000 | consumed samples:          448 | elapsed time per iteration (ms): 1320.0 | rate (tokens/sec): 6205.94 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.201757E+00 | loss scale: 1.0 | grad norm: 3.792 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      225/    1000 | consumed samples:          450 | elapsed time per iteration (ms): 1322.7 | rate (tokens/sec): 6193.40 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.436485E+00 | loss scale: 1.0 | grad norm: 4.149 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      226/    1000 | consumed samples:          452 | elapsed time per iteration (ms): 1322.2 | rate (tokens/sec): 6195.95 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.757929E+00 | loss scale: 1.0 | grad norm: 2.793 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      227/    1000 | consumed samples:          454 | elapsed time per iteration (ms): 1322.2 | rate (tokens/sec): 6195.93 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.413810E+00 | loss scale: 1.0 | grad norm: 3.737 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      228/    1000 | consumed samples:          456 | elapsed time per iteration (ms): 1320.7 | rate (tokens/sec): 6202.80 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.894416E+00 | loss scale: 1.0 | grad norm: 2.407 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      229/    1000 | consumed samples:          458 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.59 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.667270E+00 | loss scale: 1.0 | grad norm: 2.482 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      230/    1000 | consumed samples:          460 | elapsed time per iteration (ms): 1320.4 | rate (tokens/sec): 6204.40 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.978531E+00 | loss scale: 1.0 | grad norm: 2.853 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      231/    1000 | consumed samples:          462 | elapsed time per iteration (ms): 1320.8 | rate (tokens/sec): 6202.37 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.856053E+00 | loss scale: 1.0 | grad norm: 2.919 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      232/    1000 | consumed samples:          464 | elapsed time per iteration (ms): 1320.9 | rate (tokens/sec): 6202.04 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.801039E+00 | loss scale: 1.0 | grad norm: 3.397 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      233/    1000 | consumed samples:          466 | elapsed time per iteration (ms): 1319.4 | rate (tokens/sec): 6209.10 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.557867E+00 | loss scale: 1.0 | grad norm: 2.335 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      234/    1000 | consumed samples:          468 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.70 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.759951E+00 | loss scale: 1.0 | grad norm: 2.673 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      235/    1000 | consumed samples:          470 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.51 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.810833E+00 | loss scale: 1.0 | grad norm: 2.411 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      236/    1000 | consumed samples:          472 | elapsed time per iteration (ms): 1320.8 | rate (tokens/sec): 6202.21 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.396346E+00 | loss scale: 1.0 | grad norm: 2.899 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      237/    1000 | consumed samples:          474 | elapsed time per iteration (ms): 1320.2 | rate (tokens/sec): 6205.17 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.896478E+00 | loss scale: 1.0 | grad norm: 5.746 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      238/    1000 | consumed samples:          476 | elapsed time per iteration (ms): 1322.0 | rate (tokens/sec): 6196.55 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.616603E+00 | loss scale: 1.0 | grad norm: 3.769 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      239/    1000 | consumed samples:          478 | elapsed time per iteration (ms): 1390.7 | rate (tokens/sec): 5890.74 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.689354E+00 | loss scale: 1.0 | grad norm: 2.685 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      240/    1000 | consumed samples:          480 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.67 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.649784E+00 | loss scale: 1.0 | grad norm: 2.677 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      241/    1000 | consumed samples:          482 | elapsed time per iteration (ms): 1321.1 | rate (tokens/sec): 6200.95 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.745792E+00 | loss scale: 1.0 | grad norm: 2.917 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      242/    1000 | consumed samples:          484 | elapsed time per iteration (ms): 1320.3 | rate (tokens/sec): 6204.67 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.809811E+00 | loss scale: 1.0 | grad norm: 3.407 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      243/    1000 | consumed samples:          486 | elapsed time per iteration (ms): 1318.4 | rate (tokens/sec): 6213.51 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.507311E+00 | loss scale: 1.0 | grad norm: 2.979 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      244/    1000 | consumed samples:          488 | elapsed time per iteration (ms): 1319.4 | rate (tokens/sec): 6209.07 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.902547E+00 | loss scale: 1.0 | grad norm: 2.370 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      245/    1000 | consumed samples:          490 | elapsed time per iteration (ms): 1321.4 | rate (tokens/sec): 6199.46 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.607853E+00 | loss scale: 1.0 | grad norm: 2.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      246/    1000 | consumed samples:          492 | elapsed time per iteration (ms): 1320.3 | rate (tokens/sec): 6204.42 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.752656E+00 | loss scale: 1.0 | grad norm: 2.288 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      247/    1000 | consumed samples:          494 | elapsed time per iteration (ms): 1320.4 | rate (tokens/sec): 6203.98 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.655994E+00 | loss scale: 1.0 | grad norm: 2.305 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      248/    1000 | consumed samples:          496 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.71 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 1.769630E+00 | loss scale: 1.0 | grad norm: 2.613 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      249/    1000 | consumed samples:          498 | elapsed time per iteration (ms): 1319.8 | rate (tokens/sec): 6206.99 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.682451E+00 | loss scale: 1.0 | grad norm: 5.080 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      250/    1000 | consumed samples:          500 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.67 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.061935E+00 | loss scale: 1.0 | grad norm: 3.532 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      251/    1000 | consumed samples:          502 | elapsed time per iteration (ms): 1319.8 | rate (tokens/sec): 6207.22 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.829554E+00 | loss scale: 1.0 | grad norm: 8.909 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      252/    1000 | consumed samples:          504 | elapsed time per iteration (ms): 1320.7 | rate (tokens/sec): 6202.59 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.822848E+00 | loss scale: 1.0 | grad norm: 2.484 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      253/    1000 | consumed samples:          506 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.58 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.628772E+00 | loss scale: 1.0 | grad norm: 2.105 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      254/    1000 | consumed samples:          508 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.54 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.640535E+00 | loss scale: 1.0 | grad norm: 2.389 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      255/    1000 | consumed samples:          510 | elapsed time per iteration (ms): 1321.5 | rate (tokens/sec): 6199.09 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.847744E+00 | loss scale: 1.0 | grad norm: 3.139 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      256/    1000 | consumed samples:          512 | elapsed time per iteration (ms): 1330.3 | rate (tokens/sec): 6157.87 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.686489E+00 | loss scale: 1.0 | grad norm: 2.952 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      257/    1000 | consumed samples:          514 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.29 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.711485E+00 | loss scale: 1.0 | grad norm: 2.707 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      258/    1000 | consumed samples:          516 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.69 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.760434E+00 | loss scale: 1.0 | grad norm: 2.324 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      259/    1000 | consumed samples:          518 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.32 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.851259E+00 | loss scale: 1.0 | grad norm: 2.277 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      260/    1000 | consumed samples:          520 | elapsed time per iteration (ms): 1321.0 | rate (tokens/sec): 6201.37 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.842286E+00 | loss scale: 1.0 | grad norm: 2.243 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      261/    1000 | consumed samples:          522 | elapsed time per iteration (ms): 1318.7 | rate (tokens/sec): 6212.36 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.795198E+00 | loss scale: 1.0 | grad norm: 2.844 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      262/    1000 | consumed samples:          524 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.60 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.794151E+00 | loss scale: 1.0 | grad norm: 2.556 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      263/    1000 | consumed samples:          526 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.76 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.770133E+00 | loss scale: 1.0 | grad norm: 2.268 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      264/    1000 | consumed samples:          528 | elapsed time per iteration (ms): 1320.8 | rate (tokens/sec): 6202.26 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.601275E+00 | loss scale: 1.0 | grad norm: 2.151 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      265/    1000 | consumed samples:          530 | elapsed time per iteration (ms): 1321.9 | rate (tokens/sec): 6197.00 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.494347E+00 | loss scale: 1.0 | grad norm: 10.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      266/    1000 | consumed samples:          532 | elapsed time per iteration (ms): 1321.9 | rate (tokens/sec): 6197.25 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.446664E+00 | loss scale: 1.0 | grad norm: 2.649 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      267/    1000 | consumed samples:          534 | elapsed time per iteration (ms): 1319.8 | rate (tokens/sec): 6206.83 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.521818E+00 | loss scale: 1.0 | grad norm: 3.032 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      268/    1000 | consumed samples:          536 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.84 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.869080E+00 | loss scale: 1.0 | grad norm: 2.833 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      269/    1000 | consumed samples:          538 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.46 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.627773E+00 | loss scale: 1.0 | grad norm: 2.404 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      270/    1000 | consumed samples:          540 | elapsed time per iteration (ms): 1322.2 | rate (tokens/sec): 6195.80 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.828346E+00 | loss scale: 1.0 | grad norm: 2.920 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      271/    1000 | consumed samples:          542 | elapsed time per iteration (ms): 1322.9 | rate (tokens/sec): 6192.52 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.656510E+00 | loss scale: 1.0 | grad norm: 1.953 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      272/    1000 | consumed samples:          544 | elapsed time per iteration (ms): 1321.9 | rate (tokens/sec): 6197.14 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.865415E+00 | loss scale: 1.0 | grad norm: 3.439 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      273/    1000 | consumed samples:          546 | elapsed time per iteration (ms): 1322.6 | rate (tokens/sec): 6193.74 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.472661E+00 | loss scale: 1.0 | grad norm: 2.285 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      274/    1000 | consumed samples:          548 | elapsed time per iteration (ms): 1322.6 | rate (tokens/sec): 6194.03 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.297600E+00 | loss scale: 1.0 | grad norm: 1.901 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      275/    1000 | consumed samples:          550 | elapsed time per iteration (ms): 1320.2 | rate (tokens/sec): 6205.17 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.857380E+00 | loss scale: 1.0 | grad norm: 2.444 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      276/    1000 | consumed samples:          552 | elapsed time per iteration (ms): 1321.2 | rate (tokens/sec): 6200.26 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.778982E+00 | loss scale: 1.0 | grad norm: 3.318 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      277/    1000 | consumed samples:          554 | elapsed time per iteration (ms): 1323.5 | rate (tokens/sec): 6189.72 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.681959E+00 | loss scale: 1.0 | grad norm: 2.443 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      278/    1000 | consumed samples:          556 | elapsed time per iteration (ms): 1320.9 | rate (tokens/sec): 6202.02 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.573195E+00 | loss scale: 1.0 | grad norm: 2.183 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      279/    1000 | consumed samples:          558 | elapsed time per iteration (ms): 1322.4 | rate (tokens/sec): 6194.64 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.827633E+00 | loss scale: 1.0 | grad norm: 2.213 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      280/    1000 | consumed samples:          560 | elapsed time per iteration (ms): 1321.7 | rate (tokens/sec): 6197.87 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.834447E+00 | loss scale: 1.0 | grad norm: 2.471 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      281/    1000 | consumed samples:          562 | elapsed time per iteration (ms): 1322.6 | rate (tokens/sec): 6193.66 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.717025E+00 | loss scale: 1.0 | grad norm: 2.975 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      282/    1000 | consumed samples:          564 | elapsed time per iteration (ms): 1324.5 | rate (tokens/sec): 6185.11 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.596355E+00 | loss scale: 1.0 | grad norm: 2.418 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      283/    1000 | consumed samples:          566 | elapsed time per iteration (ms): 1323.7 | rate (tokens/sec): 6188.84 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.734953E+00 | loss scale: 1.0 | grad norm: 2.579 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      284/    1000 | consumed samples:          568 | elapsed time per iteration (ms): 1320.4 | rate (tokens/sec): 6204.36 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.784313E+00 | loss scale: 1.0 | grad norm: 3.336 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      285/    1000 | consumed samples:          570 | elapsed time per iteration (ms): 1320.8 | rate (tokens/sec): 6202.27 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.848704E+00 | loss scale: 1.0 | grad norm: 3.136 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      286/    1000 | consumed samples:          572 | elapsed time per iteration (ms): 1320.1 | rate (tokens/sec): 6205.46 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.932567E+00 | loss scale: 1.0 | grad norm: 2.653 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      287/    1000 | consumed samples:          574 | elapsed time per iteration (ms): 1320.7 | rate (tokens/sec): 6202.97 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.892097E+00 | loss scale: 1.0 | grad norm: 2.876 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      288/    1000 | consumed samples:          576 | elapsed time per iteration (ms): 1322.3 | rate (tokens/sec): 6195.22 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.801844E+00 | loss scale: 1.0 | grad norm: 2.425 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      289/    1000 | consumed samples:          578 | elapsed time per iteration (ms): 1320.0 | rate (tokens/sec): 6205.87 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.712466E+00 | loss scale: 1.0 | grad norm: 2.134 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      290/    1000 | consumed samples:          580 | elapsed time per iteration (ms): 1321.3 | rate (tokens/sec): 6199.84 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.469549E+00 | loss scale: 1.0 | grad norm: 2.138 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      291/    1000 | consumed samples:          582 | elapsed time per iteration (ms): 1320.7 | rate (tokens/sec): 6202.97 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.815275E+00 | loss scale: 1.0 | grad norm: 2.199 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      292/    1000 | consumed samples:          584 | elapsed time per iteration (ms): 1321.2 | rate (tokens/sec): 6200.28 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.777063E+00 | loss scale: 1.0 | grad norm: 2.456 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      293/    1000 | consumed samples:          586 | elapsed time per iteration (ms): 1320.7 | rate (tokens/sec): 6202.85 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.429874E+00 | loss scale: 1.0 | grad norm: 2.448 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      294/    1000 | consumed samples:          588 | elapsed time per iteration (ms): 1320.8 | rate (tokens/sec): 6202.51 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.780199E+00 | loss scale: 1.0 | grad norm: 2.306 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      295/    1000 | consumed samples:          590 | elapsed time per iteration (ms): 1322.1 | rate (tokens/sec): 6195.98 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.087479E+00 | loss scale: 1.0 | grad norm: 2.537 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      296/    1000 | consumed samples:          592 | elapsed time per iteration (ms): 1321.4 | rate (tokens/sec): 6199.35 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.719915E+00 | loss scale: 1.0 | grad norm: 2.497 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      297/    1000 | consumed samples:          594 | elapsed time per iteration (ms): 1320.1 | rate (tokens/sec): 6205.59 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.855352E+00 | loss scale: 1.0 | grad norm: 2.947 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      298/    1000 | consumed samples:          596 | elapsed time per iteration (ms): 1321.1 | rate (tokens/sec): 6201.05 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.781399E+00 | loss scale: 1.0 | grad norm: 4.966 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      299/    1000 | consumed samples:          598 | elapsed time per iteration (ms): 1320.4 | rate (tokens/sec): 6204.16 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.699577E+00 | loss scale: 1.0 | grad norm: 2.528 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      300/    1000 | consumed samples:          600 | elapsed time per iteration (ms): 1320.1 | rate (tokens/sec): 6205.70 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.926908E+00 | loss scale: 1.0 | grad norm: 2.313 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      301/    1000 | consumed samples:          602 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.70 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.775710E+00 | loss scale: 1.0 | grad norm: 2.142 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      302/    1000 | consumed samples:          604 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.76 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.245477E+00 | loss scale: 1.0 | grad norm: 2.933 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      303/    1000 | consumed samples:          606 | elapsed time per iteration (ms): 1319.3 | rate (tokens/sec): 6209.30 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.552613E+00 | loss scale: 1.0 | grad norm: 2.808 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      304/    1000 | consumed samples:          608 | elapsed time per iteration (ms): 1318.7 | rate (tokens/sec): 6212.24 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.166557E+00 | loss scale: 1.0 | grad norm: 12.292 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      305/    1000 | consumed samples:          610 | elapsed time per iteration (ms): 1320.0 | rate (tokens/sec): 6206.22 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.800646E+00 | loss scale: 1.0 | grad norm: 3.073 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      306/    1000 | consumed samples:          612 | elapsed time per iteration (ms): 1320.3 | rate (tokens/sec): 6204.86 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.638393E+00 | loss scale: 1.0 | grad norm: 2.234 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      307/    1000 | consumed samples:          614 | elapsed time per iteration (ms): 1323.1 | rate (tokens/sec): 6191.46 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.796809E+00 | loss scale: 1.0 | grad norm: 2.490 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      308/    1000 | consumed samples:          616 | elapsed time per iteration (ms): 1321.2 | rate (tokens/sec): 6200.39 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.825407E+00 | loss scale: 1.0 | grad norm: 2.568 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      309/    1000 | consumed samples:          618 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.76 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.781519E+00 | loss scale: 1.0 | grad norm: 2.088 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      310/    1000 | consumed samples:          620 | elapsed time per iteration (ms): 1320.3 | rate (tokens/sec): 6204.51 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.731491E+00 | loss scale: 1.0 | grad norm: 2.013 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      311/    1000 | consumed samples:          622 | elapsed time per iteration (ms): 1319.3 | rate (tokens/sec): 6209.24 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.501227E+00 | loss scale: 1.0 | grad norm: 2.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      312/    1000 | consumed samples:          624 | elapsed time per iteration (ms): 1320.2 | rate (tokens/sec): 6205.07 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.624204E+00 | loss scale: 1.0 | grad norm: 2.360 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      313/    1000 | consumed samples:          626 | elapsed time per iteration (ms): 1321.6 | rate (tokens/sec): 6198.37 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.623454E+00 | loss scale: 1.0 | grad norm: 2.195 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      314/    1000 | consumed samples:          628 | elapsed time per iteration (ms): 1318.7 | rate (tokens/sec): 6212.03 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.762640E+00 | loss scale: 1.0 | grad norm: 2.838 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      315/    1000 | consumed samples:          630 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.60 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.073801E+00 | loss scale: 1.0 | grad norm: 2.549 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      316/    1000 | consumed samples:          632 | elapsed time per iteration (ms): 1321.8 | rate (tokens/sec): 6197.60 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.674414E+00 | loss scale: 1.0 | grad norm: 2.239 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      317/    1000 | consumed samples:          634 | elapsed time per iteration (ms): 1321.0 | rate (tokens/sec): 6201.33 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.436135E+00 | loss scale: 1.0 | grad norm: 3.350 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      318/    1000 | consumed samples:          636 | elapsed time per iteration (ms): 1321.1 | rate (tokens/sec): 6200.88 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.757177E+00 | loss scale: 1.0 | grad norm: 2.216 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      319/    1000 | consumed samples:          638 | elapsed time per iteration (ms): 1322.0 | rate (tokens/sec): 6196.69 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.707729E+00 | loss scale: 1.0 | grad norm: 2.064 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      320/    1000 | consumed samples:          640 | elapsed time per iteration (ms): 1320.7 | rate (tokens/sec): 6202.57 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.608254E+00 | loss scale: 1.0 | grad norm: 2.054 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      321/    1000 | consumed samples:          642 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.22 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.317707E+00 | loss scale: 1.0 | grad norm: 2.369 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      322/    1000 | consumed samples:          644 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.53 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.829151E+00 | loss scale: 1.0 | grad norm: 2.286 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      323/    1000 | consumed samples:          646 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6208.12 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.760297E+00 | loss scale: 1.0 | grad norm: 2.730 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      324/    1000 | consumed samples:          648 | elapsed time per iteration (ms): 1320.7 | rate (tokens/sec): 6202.65 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.710175E+00 | loss scale: 1.0 | grad norm: 2.332 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      325/    1000 | consumed samples:          650 | elapsed time per iteration (ms): 1330.3 | rate (tokens/sec): 6157.79 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.682063E+00 | loss scale: 1.0 | grad norm: 2.109 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      326/    1000 | consumed samples:          652 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.36 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.858487E+00 | loss scale: 1.0 | grad norm: 2.119 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      327/    1000 | consumed samples:          654 | elapsed time per iteration (ms): 1318.1 | rate (tokens/sec): 6214.77 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.816636E+00 | loss scale: 1.0 | grad norm: 2.111 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      328/    1000 | consumed samples:          656 | elapsed time per iteration (ms): 1318.2 | rate (tokens/sec): 6214.46 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.428723E+00 | loss scale: 1.0 | grad norm: 2.275 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      329/    1000 | consumed samples:          658 | elapsed time per iteration (ms): 1317.9 | rate (tokens/sec): 6215.97 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.901046E+00 | loss scale: 1.0 | grad norm: 2.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      330/    1000 | consumed samples:          660 | elapsed time per iteration (ms): 1318.7 | rate (tokens/sec): 6212.22 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.085731E+00 | loss scale: 1.0 | grad norm: 2.572 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      331/    1000 | consumed samples:          662 | elapsed time per iteration (ms): 1320.0 | rate (tokens/sec): 6206.21 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.523878E+00 | loss scale: 1.0 | grad norm: 2.327 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      332/    1000 | consumed samples:          664 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.63 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.710269E+00 | loss scale: 1.0 | grad norm: 2.453 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      333/    1000 | consumed samples:          666 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.85 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.778054E+00 | loss scale: 1.0 | grad norm: 2.246 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      334/    1000 | consumed samples:          668 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6207.73 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.839246E+00 | loss scale: 1.0 | grad norm: 2.250 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      335/    1000 | consumed samples:          670 | elapsed time per iteration (ms): 1319.4 | rate (tokens/sec): 6208.91 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.730112E+00 | loss scale: 1.0 | grad norm: 1.945 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      336/    1000 | consumed samples:          672 | elapsed time per iteration (ms): 1319.4 | rate (tokens/sec): 6208.89 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.798337E+00 | loss scale: 1.0 | grad norm: 1.998 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      337/    1000 | consumed samples:          674 | elapsed time per iteration (ms): 1318.8 | rate (tokens/sec): 6211.79 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.525646E+00 | loss scale: 1.0 | grad norm: 1.934 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      338/    1000 | consumed samples:          676 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.74 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.823323E+00 | loss scale: 1.0 | grad norm: 4.092 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      339/    1000 | consumed samples:          678 | elapsed time per iteration (ms): 1320.1 | rate (tokens/sec): 6205.47 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.778519E+00 | loss scale: 1.0 | grad norm: 2.518 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      340/    1000 | consumed samples:          680 | elapsed time per iteration (ms): 1319.8 | rate (tokens/sec): 6207.10 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.732405E+00 | loss scale: 1.0 | grad norm: 2.322 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      341/    1000 | consumed samples:          682 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.45 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.229423E+00 | loss scale: 1.0 | grad norm: 4.254 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      342/    1000 | consumed samples:          684 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.66 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.808255E+00 | loss scale: 1.0 | grad norm: 2.817 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      343/    1000 | consumed samples:          686 | elapsed time per iteration (ms): 1317.9 | rate (tokens/sec): 6215.92 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.999987E+00 | loss scale: 1.0 | grad norm: 2.978 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      344/    1000 | consumed samples:          688 | elapsed time per iteration (ms): 1317.9 | rate (tokens/sec): 6216.14 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.601187E+00 | loss scale: 1.0 | grad norm: 2.513 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      345/    1000 | consumed samples:          690 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.71 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.785045E+00 | loss scale: 1.0 | grad norm: 3.501 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      346/    1000 | consumed samples:          692 | elapsed time per iteration (ms): 1320.7 | rate (tokens/sec): 6202.71 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.610494E+00 | loss scale: 1.0 | grad norm: 5.088 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      347/    1000 | consumed samples:          694 | elapsed time per iteration (ms): 1319.4 | rate (tokens/sec): 6208.96 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.664749E+00 | loss scale: 1.0 | grad norm: 2.507 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      348/    1000 | consumed samples:          696 | elapsed time per iteration (ms): 1320.1 | rate (tokens/sec): 6205.80 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.099323E+00 | loss scale: 1.0 | grad norm: 16.567 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      349/    1000 | consumed samples:          698 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.42 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.680067E+00 | loss scale: 1.0 | grad norm: 3.226 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      350/    1000 | consumed samples:          700 | elapsed time per iteration (ms): 1319.8 | rate (tokens/sec): 6207.16 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.485811E+00 | loss scale: 1.0 | grad norm: 2.872 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      351/    1000 | consumed samples:          702 | elapsed time per iteration (ms): 1318.7 | rate (tokens/sec): 6211.99 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.554834E+00 | loss scale: 1.0 | grad norm: 2.385 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      352/    1000 | consumed samples:          704 | elapsed time per iteration (ms): 1320.4 | rate (tokens/sec): 6204.33 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.927958E+00 | loss scale: 1.0 | grad norm: 2.274 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      353/    1000 | consumed samples:          706 | elapsed time per iteration (ms): 1319.4 | rate (tokens/sec): 6208.66 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.024876E+00 | loss scale: 1.0 | grad norm: 3.254 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      354/    1000 | consumed samples:          708 | elapsed time per iteration (ms): 1318.2 | rate (tokens/sec): 6214.49 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.714574E+00 | loss scale: 1.0 | grad norm: 2.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      355/    1000 | consumed samples:          710 | elapsed time per iteration (ms): 1318.3 | rate (tokens/sec): 6214.20 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.607984E+00 | loss scale: 1.0 | grad norm: 1.954 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      356/    1000 | consumed samples:          712 | elapsed time per iteration (ms): 1320.0 | rate (tokens/sec): 6206.13 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.380611E+00 | loss scale: 1.0 | grad norm: 2.256 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      357/    1000 | consumed samples:          714 | elapsed time per iteration (ms): 1318.7 | rate (tokens/sec): 6212.13 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.740464E+00 | loss scale: 1.0 | grad norm: 2.396 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      358/    1000 | consumed samples:          716 | elapsed time per iteration (ms): 1320.4 | rate (tokens/sec): 6204.22 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.521229E+00 | loss scale: 1.0 | grad norm: 2.199 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      359/    1000 | consumed samples:          718 | elapsed time per iteration (ms): 1321.2 | rate (tokens/sec): 6200.21 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.813822E+00 | loss scale: 1.0 | grad norm: 3.104 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      360/    1000 | consumed samples:          720 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6207.87 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.554211E+00 | loss scale: 1.0 | grad norm: 2.822 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      361/    1000 | consumed samples:          722 | elapsed time per iteration (ms): 1319.1 | rate (tokens/sec): 6210.24 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.330805E+00 | loss scale: 1.0 | grad norm: 2.311 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      362/    1000 | consumed samples:          724 | elapsed time per iteration (ms): 1320.3 | rate (tokens/sec): 6204.46 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.656992E+00 | loss scale: 1.0 | grad norm: 2.234 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      363/    1000 | consumed samples:          726 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.42 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.659487E+00 | loss scale: 1.0 | grad norm: 2.077 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      364/    1000 | consumed samples:          728 | elapsed time per iteration (ms): 1392.7 | rate (tokens/sec): 5882.17 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.439769E+00 | loss scale: 1.0 | grad norm: 2.759 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      365/    1000 | consumed samples:          730 | elapsed time per iteration (ms): 1320.8 | rate (tokens/sec): 6202.49 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.884980E+00 | loss scale: 1.0 | grad norm: 2.515 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      366/    1000 | consumed samples:          732 | elapsed time per iteration (ms): 1321.3 | rate (tokens/sec): 6199.76 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.668385E+00 | loss scale: 1.0 | grad norm: 2.050 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      367/    1000 | consumed samples:          734 | elapsed time per iteration (ms): 1322.7 | rate (tokens/sec): 6193.62 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.791992E+00 | loss scale: 1.0 | grad norm: 2.963 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      368/    1000 | consumed samples:          736 | elapsed time per iteration (ms): 1320.3 | rate (tokens/sec): 6204.64 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.398103E+00 | loss scale: 1.0 | grad norm: 3.178 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      369/    1000 | consumed samples:          738 | elapsed time per iteration (ms): 1320.7 | rate (tokens/sec): 6202.65 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.586079E+00 | loss scale: 1.0 | grad norm: 2.288 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      370/    1000 | consumed samples:          740 | elapsed time per iteration (ms): 1319.1 | rate (tokens/sec): 6210.33 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.778710E+00 | loss scale: 1.0 | grad norm: 2.400 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      371/    1000 | consumed samples:          742 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.49 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.904764E+00 | loss scale: 1.0 | grad norm: 2.092 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      372/    1000 | consumed samples:          744 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.75 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.613134E+00 | loss scale: 1.0 | grad norm: 2.172 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      373/    1000 | consumed samples:          746 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6208.17 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 1.869924E+00 | loss scale: 1.0 | grad norm: 2.073 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      374/    1000 | consumed samples:          748 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6207.96 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.754822E+00 | loss scale: 1.0 | grad norm: 2.280 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      375/    1000 | consumed samples:          750 | elapsed time per iteration (ms): 1318.4 | rate (tokens/sec): 6213.74 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.500355E+00 | loss scale: 1.0 | grad norm: 4.959 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      376/    1000 | consumed samples:          752 | elapsed time per iteration (ms): 1320.1 | rate (tokens/sec): 6205.67 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.768721E+00 | loss scale: 1.0 | grad norm: 2.438 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      377/    1000 | consumed samples:          754 | elapsed time per iteration (ms): 1319.4 | rate (tokens/sec): 6208.79 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.885550E+00 | loss scale: 1.0 | grad norm: 2.565 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      378/    1000 | consumed samples:          756 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6207.99 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.297669E+00 | loss scale: 1.0 | grad norm: 2.540 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      379/    1000 | consumed samples:          758 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.72 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.801075E+00 | loss scale: 1.0 | grad norm: 2.448 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      380/    1000 | consumed samples:          760 | elapsed time per iteration (ms): 1321.3 | rate (tokens/sec): 6200.09 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.200159E+00 | loss scale: 1.0 | grad norm: 2.234 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      381/    1000 | consumed samples:          762 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.36 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.707944E+00 | loss scale: 1.0 | grad norm: 2.443 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      382/    1000 | consumed samples:          764 | elapsed time per iteration (ms): 1320.4 | rate (tokens/sec): 6204.35 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.704844E+00 | loss scale: 1.0 | grad norm: 2.312 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      383/    1000 | consumed samples:          766 | elapsed time per iteration (ms): 1318.2 | rate (tokens/sec): 6214.42 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.584641E+00 | loss scale: 1.0 | grad norm: 2.551 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      384/    1000 | consumed samples:          768 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.38 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.402379E+00 | loss scale: 1.0 | grad norm: 3.139 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      385/    1000 | consumed samples:          770 | elapsed time per iteration (ms): 1319.4 | rate (tokens/sec): 6208.81 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.832484E+00 | loss scale: 1.0 | grad norm: 2.102 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      386/    1000 | consumed samples:          772 | elapsed time per iteration (ms): 1373.7 | rate (tokens/sec): 5963.34 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.820347E+00 | loss scale: 1.0 | grad norm: 3.117 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      387/    1000 | consumed samples:          774 | elapsed time per iteration (ms): 1319.2 | rate (tokens/sec): 6210.04 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.696393E+00 | loss scale: 1.0 | grad norm: 3.172 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      388/    1000 | consumed samples:          776 | elapsed time per iteration (ms): 1320.4 | rate (tokens/sec): 6204.37 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.979414E+00 | loss scale: 1.0 | grad norm: 2.930 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      389/    1000 | consumed samples:          778 | elapsed time per iteration (ms): 1321.3 | rate (tokens/sec): 6199.73 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.887547E+00 | loss scale: 1.0 | grad norm: 2.510 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      390/    1000 | consumed samples:          780 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.76 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.805471E+00 | loss scale: 1.0 | grad norm: 2.271 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      391/    1000 | consumed samples:          782 | elapsed time per iteration (ms): 1330.4 | rate (tokens/sec): 6157.69 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.458360E+00 | loss scale: 1.0 | grad norm: 2.266 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      392/    1000 | consumed samples:          784 | elapsed time per iteration (ms): 1320.3 | rate (tokens/sec): 6204.55 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.754855E+00 | loss scale: 1.0 | grad norm: 2.084 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      393/    1000 | consumed samples:          786 | elapsed time per iteration (ms): 1321.1 | rate (tokens/sec): 6200.68 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.683675E+00 | loss scale: 1.0 | grad norm: 2.167 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      394/    1000 | consumed samples:          788 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.71 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.749482E+00 | loss scale: 1.0 | grad norm: 2.633 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      395/    1000 | consumed samples:          790 | elapsed time per iteration (ms): 1319.3 | rate (tokens/sec): 6209.32 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.496043E+00 | loss scale: 1.0 | grad norm: 3.700 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      396/    1000 | consumed samples:          792 | elapsed time per iteration (ms): 1320.1 | rate (tokens/sec): 6205.62 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.725669E+00 | loss scale: 1.0 | grad norm: 3.345 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      397/    1000 | consumed samples:          794 | elapsed time per iteration (ms): 1320.6 | rate (tokens/sec): 6203.14 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.724867E+00 | loss scale: 1.0 | grad norm: 3.131 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      398/    1000 | consumed samples:          796 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.93 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.659990E+00 | loss scale: 1.0 | grad norm: 3.221 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      399/    1000 | consumed samples:          798 | elapsed time per iteration (ms): 1318.5 | rate (tokens/sec): 6213.13 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.790940E+00 | loss scale: 1.0 | grad norm: 2.570 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      400/    1000 | consumed samples:          800 | elapsed time per iteration (ms): 1318.7 | rate (tokens/sec): 6211.95 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.885448E+00 | loss scale: 1.0 | grad norm: 2.145 | number of skipped iterations:   0 | number of nan iterations:   0 |
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 validation loss at iteration 400 | lm loss value: 2.797956E+00 | lm loss PPL: 1.641107E+01 | ppl value: 1.678094E+01 | lm accuracy value: 4.296143E-01 | count loss mask value: 4.096000E+03 | 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 iteration      401/    1000 | consumed samples:          802 | elapsed time per iteration (ms): 5178.8 | rate (tokens/sec): 1581.85 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.214220E+00 | loss scale: 1.0 | grad norm: 1.986 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      402/    1000 | consumed samples:          804 | elapsed time per iteration (ms): 1320.0 | rate (tokens/sec): 6205.89 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.484875E+00 | loss scale: 1.0 | grad norm: 3.329 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      403/    1000 | consumed samples:          806 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.51 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.562086E+00 | loss scale: 1.0 | grad norm: 3.489 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      404/    1000 | consumed samples:          808 | elapsed time per iteration (ms): 1322.2 | rate (tokens/sec): 6195.69 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.599296E+00 | loss scale: 1.0 | grad norm: 2.962 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      405/    1000 | consumed samples:          810 | elapsed time per iteration (ms): 1320.2 | rate (tokens/sec): 6205.27 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.015698E+00 | loss scale: 1.0 | grad norm: 2.814 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      406/    1000 | consumed samples:          812 | elapsed time per iteration (ms): 1320.7 | rate (tokens/sec): 6202.89 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.763269E+00 | loss scale: 1.0 | grad norm: 2.395 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      407/    1000 | consumed samples:          814 | elapsed time per iteration (ms): 1322.1 | rate (tokens/sec): 6196.15 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.005714E+00 | loss scale: 1.0 | grad norm: 3.109 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      408/    1000 | consumed samples:          816 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.54 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.074864E+00 | loss scale: 1.0 | grad norm: 2.738 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      409/    1000 | consumed samples:          818 | elapsed time per iteration (ms): 1320.1 | rate (tokens/sec): 6205.55 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.206806E+00 | loss scale: 1.0 | grad norm: 3.757 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      410/    1000 | consumed samples:          820 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6207.77 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.727849E+00 | loss scale: 1.0 | grad norm: 3.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      411/    1000 | consumed samples:          822 | elapsed time per iteration (ms): 1321.2 | rate (tokens/sec): 6200.40 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.697685E+00 | loss scale: 1.0 | grad norm: 4.547 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      412/    1000 | consumed samples:          824 | elapsed time per iteration (ms): 1320.2 | rate (tokens/sec): 6205.20 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.521751E+00 | loss scale: 1.0 | grad norm: 3.544 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      413/    1000 | consumed samples:          826 | elapsed time per iteration (ms): 1321.1 | rate (tokens/sec): 6200.95 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.458465E+00 | loss scale: 1.0 | grad norm: 2.769 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      414/    1000 | consumed samples:          828 | elapsed time per iteration (ms): 1322.8 | rate (tokens/sec): 6192.86 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.646300E+00 | loss scale: 1.0 | grad norm: 2.501 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      415/    1000 | consumed samples:          830 | elapsed time per iteration (ms): 1320.6 | rate (tokens/sec): 6203.06 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.655073E+00 | loss scale: 1.0 | grad norm: 2.243 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      416/    1000 | consumed samples:          832 | elapsed time per iteration (ms): 1322.7 | rate (tokens/sec): 6193.31 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.729462E+00 | loss scale: 1.0 | grad norm: 2.443 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      417/    1000 | consumed samples:          834 | elapsed time per iteration (ms): 1321.2 | rate (tokens/sec): 6200.57 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.634628E+00 | loss scale: 1.0 | grad norm: 2.583 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      418/    1000 | consumed samples:          836 | elapsed time per iteration (ms): 1321.0 | rate (tokens/sec): 6201.29 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.936395E+00 | loss scale: 1.0 | grad norm: 3.543 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      419/    1000 | consumed samples:          838 | elapsed time per iteration (ms): 1321.9 | rate (tokens/sec): 6196.92 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.765108E+00 | loss scale: 1.0 | grad norm: 2.195 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      420/    1000 | consumed samples:          840 | elapsed time per iteration (ms): 1320.9 | rate (tokens/sec): 6201.78 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.734109E+00 | loss scale: 1.0 | grad norm: 2.247 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      421/    1000 | consumed samples:          842 | elapsed time per iteration (ms): 1323.3 | rate (tokens/sec): 6190.49 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.632926E+00 | loss scale: 1.0 | grad norm: 2.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      422/    1000 | consumed samples:          844 | elapsed time per iteration (ms): 1319.1 | rate (tokens/sec): 6210.40 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.215578E+00 | loss scale: 1.0 | grad norm: 2.092 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      423/    1000 | consumed samples:          846 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.55 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.861974E+00 | loss scale: 1.0 | grad norm: 2.053 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      424/    1000 | consumed samples:          848 | elapsed time per iteration (ms): 1320.1 | rate (tokens/sec): 6205.56 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 1.852920E+00 | loss scale: 1.0 | grad norm: 2.717 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      425/    1000 | consumed samples:          850 | elapsed time per iteration (ms): 1318.9 | rate (tokens/sec): 6211.28 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.790953E+00 | loss scale: 1.0 | grad norm: 3.186 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      426/    1000 | consumed samples:          852 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6207.77 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.921075E+00 | loss scale: 1.0 | grad norm: 3.556 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      427/    1000 | consumed samples:          854 | elapsed time per iteration (ms): 1319.4 | rate (tokens/sec): 6208.76 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.720161E+00 | loss scale: 1.0 | grad norm: 2.807 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      428/    1000 | consumed samples:          856 | elapsed time per iteration (ms): 1320.7 | rate (tokens/sec): 6202.75 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.203997E+00 | loss scale: 1.0 | grad norm: 2.922 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      429/    1000 | consumed samples:          858 | elapsed time per iteration (ms): 1321.2 | rate (tokens/sec): 6200.28 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.727124E+00 | loss scale: 1.0 | grad norm: 2.552 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      430/    1000 | consumed samples:          860 | elapsed time per iteration (ms): 1321.3 | rate (tokens/sec): 6200.06 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.742920E+00 | loss scale: 1.0 | grad norm: 3.232 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      431/    1000 | consumed samples:          862 | elapsed time per iteration (ms): 1318.6 | rate (tokens/sec): 6212.48 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.781377E+00 | loss scale: 1.0 | grad norm: 2.431 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      432/    1000 | consumed samples:          864 | elapsed time per iteration (ms): 1320.6 | rate (tokens/sec): 6203.34 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.576784E+00 | loss scale: 1.0 | grad norm: 3.111 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      433/    1000 | consumed samples:          866 | elapsed time per iteration (ms): 1321.2 | rate (tokens/sec): 6200.37 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.162822E+00 | loss scale: 1.0 | grad norm: 2.553 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      434/    1000 | consumed samples:          868 | elapsed time per iteration (ms): 1321.7 | rate (tokens/sec): 6198.08 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.854836E+00 | loss scale: 1.0 | grad norm: 2.124 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      435/    1000 | consumed samples:          870 | elapsed time per iteration (ms): 1320.7 | rate (tokens/sec): 6202.94 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.939297E+00 | loss scale: 1.0 | grad norm: 2.729 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      436/    1000 | consumed samples:          872 | elapsed time per iteration (ms): 1321.0 | rate (tokens/sec): 6201.43 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.757687E+00 | loss scale: 1.0 | grad norm: 1.951 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      437/    1000 | consumed samples:          874 | elapsed time per iteration (ms): 1320.4 | rate (tokens/sec): 6204.31 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.918015E+00 | loss scale: 1.0 | grad norm: 2.067 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      438/    1000 | consumed samples:          876 | elapsed time per iteration (ms): 1319.2 | rate (tokens/sec): 6210.03 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.597555E+00 | loss scale: 1.0 | grad norm: 3.386 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      439/    1000 | consumed samples:          878 | elapsed time per iteration (ms): 1320.7 | rate (tokens/sec): 6202.70 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.669827E+00 | loss scale: 1.0 | grad norm: 2.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      440/    1000 | consumed samples:          880 | elapsed time per iteration (ms): 1320.4 | rate (tokens/sec): 6204.26 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.736831E+00 | loss scale: 1.0 | grad norm: 2.511 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      441/    1000 | consumed samples:          882 | elapsed time per iteration (ms): 1321.3 | rate (tokens/sec): 6199.75 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.946784E+00 | loss scale: 1.0 | grad norm: 3.244 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      442/    1000 | consumed samples:          884 | elapsed time per iteration (ms): 1321.0 | rate (tokens/sec): 6201.29 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.758001E+00 | loss scale: 1.0 | grad norm: 2.412 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      443/    1000 | consumed samples:          886 | elapsed time per iteration (ms): 1321.0 | rate (tokens/sec): 6201.14 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.479832E+00 | loss scale: 1.0 | grad norm: 4.829 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      444/    1000 | consumed samples:          888 | elapsed time per iteration (ms): 1320.3 | rate (tokens/sec): 6204.84 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.534746E+00 | loss scale: 1.0 | grad norm: 2.047 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      445/    1000 | consumed samples:          890 | elapsed time per iteration (ms): 1320.2 | rate (tokens/sec): 6205.13 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.667352E+00 | loss scale: 1.0 | grad norm: 2.093 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      446/    1000 | consumed samples:          892 | elapsed time per iteration (ms): 1321.0 | rate (tokens/sec): 6201.59 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.272103E+00 | loss scale: 1.0 | grad norm: 2.024 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      447/    1000 | consumed samples:          894 | elapsed time per iteration (ms): 1320.6 | rate (tokens/sec): 6203.31 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.674005E+00 | loss scale: 1.0 | grad norm: 2.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      448/    1000 | consumed samples:          896 | elapsed time per iteration (ms): 1321.1 | rate (tokens/sec): 6200.90 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.679161E+00 | loss scale: 1.0 | grad norm: 2.232 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      449/    1000 | consumed samples:          898 | elapsed time per iteration (ms): 1323.4 | rate (tokens/sec): 6190.31 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.743426E+00 | loss scale: 1.0 | grad norm: 2.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      450/    1000 | consumed samples:          900 | elapsed time per iteration (ms): 1321.1 | rate (tokens/sec): 6200.90 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.593587E+00 | loss scale: 1.0 | grad norm: 2.201 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      451/    1000 | consumed samples:          902 | elapsed time per iteration (ms): 1322.4 | rate (tokens/sec): 6194.71 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.827585E+00 | loss scale: 1.0 | grad norm: 2.118 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      452/    1000 | consumed samples:          904 | elapsed time per iteration (ms): 1322.8 | rate (tokens/sec): 6192.81 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.908013E+00 | loss scale: 1.0 | grad norm: 1.794 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      453/    1000 | consumed samples:          906 | elapsed time per iteration (ms): 1323.4 | rate (tokens/sec): 6190.02 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.599611E+00 | loss scale: 1.0 | grad norm: 2.101 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      454/    1000 | consumed samples:          908 | elapsed time per iteration (ms): 1321.6 | rate (tokens/sec): 6198.57 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.708869E+00 | loss scale: 1.0 | grad norm: 2.443 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      455/    1000 | consumed samples:          910 | elapsed time per iteration (ms): 1331.8 | rate (tokens/sec): 6150.88 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.320510E+00 | loss scale: 1.0 | grad norm: 2.117 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      456/    1000 | consumed samples:          912 | elapsed time per iteration (ms): 1319.8 | rate (tokens/sec): 6207.16 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.687613E+00 | loss scale: 1.0 | grad norm: 2.866 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      457/    1000 | consumed samples:          914 | elapsed time per iteration (ms): 1319.4 | rate (tokens/sec): 6208.75 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.476876E+00 | loss scale: 1.0 | grad norm: 4.567 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      458/    1000 | consumed samples:          916 | elapsed time per iteration (ms): 1320.4 | rate (tokens/sec): 6204.14 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.347255E+00 | loss scale: 1.0 | grad norm: 2.494 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      459/    1000 | consumed samples:          918 | elapsed time per iteration (ms): 1319.3 | rate (tokens/sec): 6209.44 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.970698E+00 | loss scale: 1.0 | grad norm: 2.891 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      460/    1000 | consumed samples:          920 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.74 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.579883E+00 | loss scale: 1.0 | grad norm: 1.996 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      461/    1000 | consumed samples:          922 | elapsed time per iteration (ms): 1320.1 | rate (tokens/sec): 6205.48 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.701674E+00 | loss scale: 1.0 | grad norm: 1.991 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      462/    1000 | consumed samples:          924 | elapsed time per iteration (ms): 1322.3 | rate (tokens/sec): 6195.36 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.550450E+00 | loss scale: 1.0 | grad norm: 2.919 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      463/    1000 | consumed samples:          926 | elapsed time per iteration (ms): 1322.3 | rate (tokens/sec): 6195.05 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.423760E+00 | loss scale: 1.0 | grad norm: 1.913 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      464/    1000 | consumed samples:          928 | elapsed time per iteration (ms): 1320.0 | rate (tokens/sec): 6206.21 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.573099E+00 | loss scale: 1.0 | grad norm: 2.262 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      465/    1000 | consumed samples:          930 | elapsed time per iteration (ms): 1321.0 | rate (tokens/sec): 6201.54 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.573248E+00 | loss scale: 1.0 | grad norm: 2.179 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      466/    1000 | consumed samples:          932 | elapsed time per iteration (ms): 1320.0 | rate (tokens/sec): 6205.84 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.596394E+00 | loss scale: 1.0 | grad norm: 2.878 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      467/    1000 | consumed samples:          934 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.71 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.746874E+00 | loss scale: 1.0 | grad norm: 2.138 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      468/    1000 | consumed samples:          936 | elapsed time per iteration (ms): 1322.5 | rate (tokens/sec): 6194.16 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.696904E+00 | loss scale: 1.0 | grad norm: 2.281 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      469/    1000 | consumed samples:          938 | elapsed time per iteration (ms): 1322.0 | rate (tokens/sec): 6196.44 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.770368E+00 | loss scale: 1.0 | grad norm: 2.521 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      470/    1000 | consumed samples:          940 | elapsed time per iteration (ms): 1321.7 | rate (tokens/sec): 6198.09 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.730950E+00 | loss scale: 1.0 | grad norm: 1.995 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      471/    1000 | consumed samples:          942 | elapsed time per iteration (ms): 1321.9 | rate (tokens/sec): 6196.92 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.744788E+00 | loss scale: 1.0 | grad norm: 1.853 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      472/    1000 | consumed samples:          944 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6208.03 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.211766E+00 | loss scale: 1.0 | grad norm: 4.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      473/    1000 | consumed samples:          946 | elapsed time per iteration (ms): 1321.1 | rate (tokens/sec): 6201.09 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.661079E+00 | loss scale: 1.0 | grad norm: 2.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      474/    1000 | consumed samples:          948 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.77 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.790390E+00 | loss scale: 1.0 | grad norm: 2.027 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      475/    1000 | consumed samples:          950 | elapsed time per iteration (ms): 1321.2 | rate (tokens/sec): 6200.52 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.907156E+00 | loss scale: 1.0 | grad norm: 4.390 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      476/    1000 | consumed samples:          952 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.48 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.685490E+00 | loss scale: 1.0 | grad norm: 2.104 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      477/    1000 | consumed samples:          954 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.25 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.707430E+00 | loss scale: 1.0 | grad norm: 2.565 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      478/    1000 | consumed samples:          956 | elapsed time per iteration (ms): 1320.4 | rate (tokens/sec): 6204.06 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.343113E+00 | loss scale: 1.0 | grad norm: 2.118 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      479/    1000 | consumed samples:          958 | elapsed time per iteration (ms): 1320.4 | rate (tokens/sec): 6204.16 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.026585E+00 | loss scale: 1.0 | grad norm: 2.829 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      480/    1000 | consumed samples:          960 | elapsed time per iteration (ms): 1321.7 | rate (tokens/sec): 6198.26 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.003931E+00 | loss scale: 1.0 | grad norm: 2.273 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      481/    1000 | consumed samples:          962 | elapsed time per iteration (ms): 1321.8 | rate (tokens/sec): 6197.73 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.832067E+00 | loss scale: 1.0 | grad norm: 2.434 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      482/    1000 | consumed samples:          964 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.48 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.736544E+00 | loss scale: 1.0 | grad norm: 2.307 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      483/    1000 | consumed samples:          966 | elapsed time per iteration (ms): 1320.3 | rate (tokens/sec): 6204.88 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.916520E+00 | loss scale: 1.0 | grad norm: 2.049 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      484/    1000 | consumed samples:          968 | elapsed time per iteration (ms): 1321.1 | rate (tokens/sec): 6200.83 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.698753E+00 | loss scale: 1.0 | grad norm: 2.427 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      485/    1000 | consumed samples:          970 | elapsed time per iteration (ms): 1319.8 | rate (tokens/sec): 6207.19 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.434223E+00 | loss scale: 1.0 | grad norm: 1.971 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      486/    1000 | consumed samples:          972 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.60 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.561698E+00 | loss scale: 1.0 | grad norm: 2.410 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      487/    1000 | consumed samples:          974 | elapsed time per iteration (ms): 1318.2 | rate (tokens/sec): 6214.74 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.485185E+00 | loss scale: 1.0 | grad norm: 2.403 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      488/    1000 | consumed samples:          976 | elapsed time per iteration (ms): 1318.5 | rate (tokens/sec): 6212.98 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.725613E+00 | loss scale: 1.0 | grad norm: 2.059 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      489/    1000 | consumed samples:          978 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.57 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.717908E+00 | loss scale: 1.0 | grad norm: 2.154 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      490/    1000 | consumed samples:          980 | elapsed time per iteration (ms): 1318.4 | rate (tokens/sec): 6213.74 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.415293E+00 | loss scale: 1.0 | grad norm: 2.070 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      491/    1000 | consumed samples:          982 | elapsed time per iteration (ms): 1319.3 | rate (tokens/sec): 6209.53 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.168034E+00 | loss scale: 1.0 | grad norm: 2.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      492/    1000 | consumed samples:          984 | elapsed time per iteration (ms): 1320.2 | rate (tokens/sec): 6205.27 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.015645E+00 | loss scale: 1.0 | grad norm: 2.033 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      493/    1000 | consumed samples:          986 | elapsed time per iteration (ms): 1322.3 | rate (tokens/sec): 6195.46 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.466671E+00 | loss scale: 1.0 | grad norm: 2.789 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      494/    1000 | consumed samples:          988 | elapsed time per iteration (ms): 1321.3 | rate (tokens/sec): 6199.83 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.772456E+00 | loss scale: 1.0 | grad norm: 1.875 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      495/    1000 | consumed samples:          990 | elapsed time per iteration (ms): 1321.0 | rate (tokens/sec): 6201.29 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.841307E+00 | loss scale: 1.0 | grad norm: 2.248 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      496/    1000 | consumed samples:          992 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6207.92 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.781123E+00 | loss scale: 1.0 | grad norm: 3.285 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      497/    1000 | consumed samples:          994 | elapsed time per iteration (ms): 1319.3 | rate (tokens/sec): 6209.23 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.555199E+00 | loss scale: 1.0 | grad norm: 2.195 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      498/    1000 | consumed samples:          996 | elapsed time per iteration (ms): 1320.3 | rate (tokens/sec): 6204.49 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.756401E+00 | loss scale: 1.0 | grad norm: 2.018 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      499/    1000 | consumed samples:          998 | elapsed time per iteration (ms): 1320.3 | rate (tokens/sec): 6204.72 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.758544E+00 | loss scale: 1.0 | grad norm: 1.993 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      500/    1000 | consumed samples:         1000 | elapsed time per iteration (ms): 1322.2 | rate (tokens/sec): 6195.88 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.840440E+00 | loss scale: 1.0 | grad norm: 1.906 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      501/    1000 | consumed samples:         1002 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.45 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.765990E+00 | loss scale: 1.0 | grad norm: 4.715 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      502/    1000 | consumed samples:         1004 | elapsed time per iteration (ms): 1320.0 | rate (tokens/sec): 6206.06 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.654643E+00 | loss scale: 1.0 | grad norm: 2.353 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      503/    1000 | consumed samples:         1006 | elapsed time per iteration (ms): 1319.3 | rate (tokens/sec): 6209.18 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.679630E+00 | loss scale: 1.0 | grad norm: 1.863 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      504/    1000 | consumed samples:         1008 | elapsed time per iteration (ms): 1318.6 | rate (tokens/sec): 6212.84 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.513379E+00 | loss scale: 1.0 | grad norm: 2.017 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      505/    1000 | consumed samples:         1010 | elapsed time per iteration (ms): 1319.1 | rate (tokens/sec): 6210.39 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.791394E+00 | loss scale: 1.0 | grad norm: 2.156 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      506/    1000 | consumed samples:         1012 | elapsed time per iteration (ms): 1319.3 | rate (tokens/sec): 6209.40 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.487760E+00 | loss scale: 1.0 | grad norm: 2.125 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      507/    1000 | consumed samples:         1014 | elapsed time per iteration (ms): 1320.8 | rate (tokens/sec): 6202.10 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.035813E+00 | loss scale: 1.0 | grad norm: 2.652 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      508/    1000 | consumed samples:         1016 | elapsed time per iteration (ms): 1318.1 | rate (tokens/sec): 6214.79 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.376865E+00 | loss scale: 1.0 | grad norm: 2.072 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      509/    1000 | consumed samples:         1018 | elapsed time per iteration (ms): 1318.6 | rate (tokens/sec): 6212.64 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.245054E+00 | loss scale: 1.0 | grad norm: 9.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      510/    1000 | consumed samples:         1020 | elapsed time per iteration (ms): 1318.2 | rate (tokens/sec): 6214.59 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.443962E+00 | loss scale: 1.0 | grad norm: 2.102 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      511/    1000 | consumed samples:         1022 | elapsed time per iteration (ms): 1322.5 | rate (tokens/sec): 6194.20 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.343040E+00 | loss scale: 1.0 | grad norm: 2.338 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      512/    1000 | consumed samples:         1024 | elapsed time per iteration (ms): 1318.8 | rate (tokens/sec): 6211.59 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.310836E+00 | loss scale: 1.0 | grad norm: 2.429 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      513/    1000 | consumed samples:         1026 | elapsed time per iteration (ms): 1321.0 | rate (tokens/sec): 6201.42 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.543391E+00 | loss scale: 1.0 | grad norm: 2.345 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      514/    1000 | consumed samples:         1028 | elapsed time per iteration (ms): 1317.0 | rate (tokens/sec): 6220.06 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.998966E+00 | loss scale: 1.0 | grad norm: 2.811 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      515/    1000 | consumed samples:         1030 | elapsed time per iteration (ms): 1317.8 | rate (tokens/sec): 6216.51 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.853680E+00 | loss scale: 1.0 | grad norm: 2.792 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      516/    1000 | consumed samples:         1032 | elapsed time per iteration (ms): 1319.4 | rate (tokens/sec): 6209.11 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.900615E+00 | loss scale: 1.0 | grad norm: 2.698 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      517/    1000 | consumed samples:         1034 | elapsed time per iteration (ms): 1320.7 | rate (tokens/sec): 6202.91 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.925861E+00 | loss scale: 1.0 | grad norm: 5.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      518/    1000 | consumed samples:         1036 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.25 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.975320E+00 | loss scale: 1.0 | grad norm: 2.124 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      519/    1000 | consumed samples:         1038 | elapsed time per iteration (ms): 1321.2 | rate (tokens/sec): 6200.52 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.854301E+00 | loss scale: 1.0 | grad norm: 2.478 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      520/    1000 | consumed samples:         1040 | elapsed time per iteration (ms): 1319.4 | rate (tokens/sec): 6208.80 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.190448E+00 | loss scale: 1.0 | grad norm: 4.377 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      521/    1000 | consumed samples:         1042 | elapsed time per iteration (ms): 1319.2 | rate (tokens/sec): 6209.90 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.578513E+00 | loss scale: 1.0 | grad norm: 2.341 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      522/    1000 | consumed samples:         1044 | elapsed time per iteration (ms): 1322.8 | rate (tokens/sec): 6192.97 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.868351E+00 | loss scale: 1.0 | grad norm: 2.356 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      523/    1000 | consumed samples:         1046 | elapsed time per iteration (ms): 1329.5 | rate (tokens/sec): 6161.53 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.220651E+00 | loss scale: 1.0 | grad norm: 2.382 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      524/    1000 | consumed samples:         1048 | elapsed time per iteration (ms): 1320.1 | rate (tokens/sec): 6205.59 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.111795E+00 | loss scale: 1.0 | grad norm: 2.472 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      525/    1000 | consumed samples:         1050 | elapsed time per iteration (ms): 1319.1 | rate (tokens/sec): 6210.20 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.827443E+00 | loss scale: 1.0 | grad norm: 1.975 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      526/    1000 | consumed samples:         1052 | elapsed time per iteration (ms): 1321.9 | rate (tokens/sec): 6196.94 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.626719E+00 | loss scale: 1.0 | grad norm: 2.796 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      527/    1000 | consumed samples:         1054 | elapsed time per iteration (ms): 1317.5 | rate (tokens/sec): 6217.77 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.707292E+00 | loss scale: 1.0 | grad norm: 3.110 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      528/    1000 | consumed samples:         1056 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.30 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.682140E+00 | loss scale: 1.0 | grad norm: 2.722 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      529/    1000 | consumed samples:         1058 | elapsed time per iteration (ms): 1319.2 | rate (tokens/sec): 6209.94 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.654042E+00 | loss scale: 1.0 | grad norm: 2.179 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      530/    1000 | consumed samples:         1060 | elapsed time per iteration (ms): 1318.6 | rate (tokens/sec): 6212.76 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.517292E+00 | loss scale: 1.0 | grad norm: 2.225 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      531/    1000 | consumed samples:         1062 | elapsed time per iteration (ms): 1319.2 | rate (tokens/sec): 6210.03 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.567600E+00 | loss scale: 1.0 | grad norm: 3.458 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      532/    1000 | consumed samples:         1064 | elapsed time per iteration (ms): 1320.6 | rate (tokens/sec): 6203.17 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.741544E+00 | loss scale: 1.0 | grad norm: 1.906 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      533/    1000 | consumed samples:         1066 | elapsed time per iteration (ms): 1379.1 | rate (tokens/sec): 5939.90 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.508453E+00 | loss scale: 1.0 | grad norm: 2.163 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      534/    1000 | consumed samples:         1068 | elapsed time per iteration (ms): 1322.7 | rate (tokens/sec): 6193.34 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.509982E+00 | loss scale: 1.0 | grad norm: 2.097 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      535/    1000 | consumed samples:         1070 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.83 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.425551E+00 | loss scale: 1.0 | grad norm: 2.050 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      536/    1000 | consumed samples:         1072 | elapsed time per iteration (ms): 1318.7 | rate (tokens/sec): 6212.41 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.052628E+00 | loss scale: 1.0 | grad norm: 2.229 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      537/    1000 | consumed samples:         1074 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6207.77 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.618759E+00 | loss scale: 1.0 | grad norm: 2.645 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      538/    1000 | consumed samples:         1076 | elapsed time per iteration (ms): 1318.8 | rate (tokens/sec): 6211.74 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.858735E+00 | loss scale: 1.0 | grad norm: 3.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      539/    1000 | consumed samples:         1078 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.50 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.720784E+00 | loss scale: 1.0 | grad norm: 2.026 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      540/    1000 | consumed samples:         1080 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.57 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.655323E+00 | loss scale: 1.0 | grad norm: 2.491 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      541/    1000 | consumed samples:         1082 | elapsed time per iteration (ms): 1320.9 | rate (tokens/sec): 6201.76 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.686800E+00 | loss scale: 1.0 | grad norm: 1.994 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      542/    1000 | consumed samples:         1084 | elapsed time per iteration (ms): 1318.8 | rate (tokens/sec): 6211.48 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.838606E+00 | loss scale: 1.0 | grad norm: 2.146 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      543/    1000 | consumed samples:         1086 | elapsed time per iteration (ms): 1318.5 | rate (tokens/sec): 6213.05 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.662202E+00 | loss scale: 1.0 | grad norm: 2.202 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      544/    1000 | consumed samples:         1088 | elapsed time per iteration (ms): 1317.7 | rate (tokens/sec): 6216.68 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.359755E+00 | loss scale: 1.0 | grad norm: 2.317 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      545/    1000 | consumed samples:         1090 | elapsed time per iteration (ms): 1318.1 | rate (tokens/sec): 6215.06 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.721165E+00 | loss scale: 1.0 | grad norm: 2.545 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      546/    1000 | consumed samples:         1092 | elapsed time per iteration (ms): 1318.4 | rate (tokens/sec): 6213.55 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.354839E+00 | loss scale: 1.0 | grad norm: 2.582 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      547/    1000 | consumed samples:         1094 | elapsed time per iteration (ms): 1319.8 | rate (tokens/sec): 6207.09 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.811277E+00 | loss scale: 1.0 | grad norm: 2.329 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      548/    1000 | consumed samples:         1096 | elapsed time per iteration (ms): 1318.2 | rate (tokens/sec): 6214.42 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.609201E+00 | loss scale: 1.0 | grad norm: 2.293 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      549/    1000 | consumed samples:         1098 | elapsed time per iteration (ms): 1318.6 | rate (tokens/sec): 6212.75 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.865478E+00 | loss scale: 1.0 | grad norm: 2.077 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      550/    1000 | consumed samples:         1100 | elapsed time per iteration (ms): 1320.8 | rate (tokens/sec): 6202.48 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.529245E+00 | loss scale: 1.0 | grad norm: 2.552 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      551/    1000 | consumed samples:         1102 | elapsed time per iteration (ms): 1318.7 | rate (tokens/sec): 6212.04 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.427020E+00 | loss scale: 1.0 | grad norm: 2.245 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      552/    1000 | consumed samples:         1104 | elapsed time per iteration (ms): 1318.4 | rate (tokens/sec): 6213.45 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.591032E+00 | loss scale: 1.0 | grad norm: 2.198 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      553/    1000 | consumed samples:         1106 | elapsed time per iteration (ms): 1320.0 | rate (tokens/sec): 6205.90 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.588589E+00 | loss scale: 1.0 | grad norm: 2.393 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      554/    1000 | consumed samples:         1108 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.58 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.863905E+00 | loss scale: 1.0 | grad norm: 2.483 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      555/    1000 | consumed samples:         1110 | elapsed time per iteration (ms): 1318.7 | rate (tokens/sec): 6212.31 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.827614E+00 | loss scale: 1.0 | grad norm: 2.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      556/    1000 | consumed samples:         1112 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6208.06 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.460585E+00 | loss scale: 1.0 | grad norm: 2.956 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      557/    1000 | consumed samples:         1114 | elapsed time per iteration (ms): 1322.6 | rate (tokens/sec): 6193.69 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.832675E+00 | loss scale: 1.0 | grad norm: 1.872 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      558/    1000 | consumed samples:         1116 | elapsed time per iteration (ms): 1320.7 | rate (tokens/sec): 6202.85 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.563464E+00 | loss scale: 1.0 | grad norm: 1.625 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      559/    1000 | consumed samples:         1118 | elapsed time per iteration (ms): 1318.8 | rate (tokens/sec): 6211.61 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.514484E+00 | loss scale: 1.0 | grad norm: 1.798 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      560/    1000 | consumed samples:         1120 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.94 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.191058E+00 | loss scale: 1.0 | grad norm: 1.974 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      561/    1000 | consumed samples:         1122 | elapsed time per iteration (ms): 1319.1 | rate (tokens/sec): 6210.19 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.850385E+00 | loss scale: 1.0 | grad norm: 1.834 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      562/    1000 | consumed samples:         1124 | elapsed time per iteration (ms): 1317.5 | rate (tokens/sec): 6217.76 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.578124E+00 | loss scale: 1.0 | grad norm: 1.964 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      563/    1000 | consumed samples:         1126 | elapsed time per iteration (ms): 1317.9 | rate (tokens/sec): 6215.85 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.229696E+00 | loss scale: 1.0 | grad norm: 1.808 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      564/    1000 | consumed samples:         1128 | elapsed time per iteration (ms): 1318.0 | rate (tokens/sec): 6215.47 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.267588E+00 | loss scale: 1.0 | grad norm: 1.984 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      565/    1000 | consumed samples:         1130 | elapsed time per iteration (ms): 1320.0 | rate (tokens/sec): 6206.01 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.189577E+00 | loss scale: 1.0 | grad norm: 2.034 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      566/    1000 | consumed samples:         1132 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.58 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.520564E+00 | loss scale: 1.0 | grad norm: 2.129 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      567/    1000 | consumed samples:         1134 | elapsed time per iteration (ms): 1318.9 | rate (tokens/sec): 6211.08 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.876931E+00 | loss scale: 1.0 | grad norm: 2.950 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      568/    1000 | consumed samples:         1136 | elapsed time per iteration (ms): 1317.5 | rate (tokens/sec): 6218.01 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.067691E+00 | loss scale: 1.0 | grad norm: 2.209 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      569/    1000 | consumed samples:         1138 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.54 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.858779E+00 | loss scale: 1.0 | grad norm: 1.968 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      570/    1000 | consumed samples:         1140 | elapsed time per iteration (ms): 1318.8 | rate (tokens/sec): 6211.64 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.685430E+00 | loss scale: 1.0 | grad norm: 2.038 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      571/    1000 | consumed samples:         1142 | elapsed time per iteration (ms): 1319.8 | rate (tokens/sec): 6207.10 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.391325E+00 | loss scale: 1.0 | grad norm: 2.111 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      572/    1000 | consumed samples:         1144 | elapsed time per iteration (ms): 1318.8 | rate (tokens/sec): 6211.55 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.607298E+00 | loss scale: 1.0 | grad norm: 1.989 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      573/    1000 | consumed samples:         1146 | elapsed time per iteration (ms): 1318.0 | rate (tokens/sec): 6215.44 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.727175E+00 | loss scale: 1.0 | grad norm: 2.155 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      574/    1000 | consumed samples:         1148 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.97 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.785617E+00 | loss scale: 1.0 | grad norm: 2.106 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      575/    1000 | consumed samples:         1150 | elapsed time per iteration (ms): 1318.6 | rate (tokens/sec): 6212.70 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.832994E+00 | loss scale: 1.0 | grad norm: 2.314 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      576/    1000 | consumed samples:         1152 | elapsed time per iteration (ms): 1318.1 | rate (tokens/sec): 6214.83 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.728149E+00 | loss scale: 1.0 | grad norm: 2.426 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      577/    1000 | consumed samples:         1154 | elapsed time per iteration (ms): 1317.6 | rate (tokens/sec): 6217.17 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.712933E+00 | loss scale: 1.0 | grad norm: 2.505 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      578/    1000 | consumed samples:         1156 | elapsed time per iteration (ms): 1317.9 | rate (tokens/sec): 6215.91 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.721240E+00 | loss scale: 1.0 | grad norm: 1.816 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      579/    1000 | consumed samples:         1158 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6208.04 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.515913E+00 | loss scale: 1.0 | grad norm: 2.036 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      580/    1000 | consumed samples:         1160 | elapsed time per iteration (ms): 1388.2 | rate (tokens/sec): 5901.23 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.641944E+00 | loss scale: 1.0 | grad norm: 1.901 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      581/    1000 | consumed samples:         1162 | elapsed time per iteration (ms): 1318.3 | rate (tokens/sec): 6214.09 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.628118E+00 | loss scale: 1.0 | grad norm: 2.121 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      582/    1000 | consumed samples:         1164 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.62 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.542034E+00 | loss scale: 1.0 | grad norm: 1.887 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      583/    1000 | consumed samples:         1166 | elapsed time per iteration (ms): 1318.8 | rate (tokens/sec): 6211.58 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.909343E+00 | loss scale: 1.0 | grad norm: 2.204 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      584/    1000 | consumed samples:         1168 | elapsed time per iteration (ms): 1317.9 | rate (tokens/sec): 6216.06 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.394643E+00 | loss scale: 1.0 | grad norm: 2.481 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      585/    1000 | consumed samples:         1170 | elapsed time per iteration (ms): 1318.4 | rate (tokens/sec): 6213.67 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.051197E+00 | loss scale: 1.0 | grad norm: 2.381 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      586/    1000 | consumed samples:         1172 | elapsed time per iteration (ms): 1317.4 | rate (tokens/sec): 6218.33 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.043262E+00 | loss scale: 1.0 | grad norm: 1.829 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      587/    1000 | consumed samples:         1174 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.64 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.672184E+00 | loss scale: 1.0 | grad norm: 2.075 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      588/    1000 | consumed samples:         1176 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.41 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.205387E+00 | loss scale: 1.0 | grad norm: 2.574 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      589/    1000 | consumed samples:         1178 | elapsed time per iteration (ms): 1331.3 | rate (tokens/sec): 6153.22 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.378857E+00 | loss scale: 1.0 | grad norm: 1.799 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      590/    1000 | consumed samples:         1180 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6207.73 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.612329E+00 | loss scale: 1.0 | grad norm: 1.713 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      591/    1000 | consumed samples:         1182 | elapsed time per iteration (ms): 1318.9 | rate (tokens/sec): 6211.04 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.727025E+00 | loss scale: 1.0 | grad norm: 1.967 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      592/    1000 | consumed samples:         1184 | elapsed time per iteration (ms): 1320.1 | rate (tokens/sec): 6205.63 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.734963E+00 | loss scale: 1.0 | grad norm: 2.121 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      593/    1000 | consumed samples:         1186 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.52 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.273424E+00 | loss scale: 1.0 | grad norm: 2.762 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      594/    1000 | consumed samples:         1188 | elapsed time per iteration (ms): 1318.3 | rate (tokens/sec): 6214.24 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.167387E+00 | loss scale: 1.0 | grad norm: 1.975 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      595/    1000 | consumed samples:         1190 | elapsed time per iteration (ms): 1318.9 | rate (tokens/sec): 6211.20 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.894202E+00 | loss scale: 1.0 | grad norm: 2.056 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      596/    1000 | consumed samples:         1192 | elapsed time per iteration (ms): 1318.5 | rate (tokens/sec): 6213.30 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.702734E+00 | loss scale: 1.0 | grad norm: 1.975 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      597/    1000 | consumed samples:         1194 | elapsed time per iteration (ms): 1318.5 | rate (tokens/sec): 6213.06 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.933609E+00 | loss scale: 1.0 | grad norm: 3.325 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      598/    1000 | consumed samples:         1196 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.70 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.835072E+00 | loss scale: 1.0 | grad norm: 2.538 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      599/    1000 | consumed samples:         1198 | elapsed time per iteration (ms): 1318.9 | rate (tokens/sec): 6211.23 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.075097E+00 | loss scale: 1.0 | grad norm: 2.435 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      600/    1000 | consumed samples:         1200 | elapsed time per iteration (ms): 1318.2 | rate (tokens/sec): 6214.67 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.547102E+00 | loss scale: 1.0 | grad norm: 2.080 | number of skipped iterations:   0 | number of nan iterations:   0 |
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 validation loss at iteration 600 | lm loss value: 2.881130E+00 | lm loss PPL: 1.783441E+01 | ppl value: 1.834219E+01 | lm accuracy value: 4.105957E-01 | count loss mask value: 4.096000E+03 | 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 iteration      601/    1000 | consumed samples:         1202 | elapsed time per iteration (ms): 5176.5 | rate (tokens/sec): 1582.53 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.503073E+00 | loss scale: 1.0 | grad norm: 2.515 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      602/    1000 | consumed samples:         1204 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.51 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.496646E+00 | loss scale: 1.0 | grad norm: 2.364 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      603/    1000 | consumed samples:         1206 | elapsed time per iteration (ms): 1317.9 | rate (tokens/sec): 6216.16 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.749137E+00 | loss scale: 1.0 | grad norm: 2.282 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      604/    1000 | consumed samples:         1208 | elapsed time per iteration (ms): 1318.3 | rate (tokens/sec): 6213.99 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.499246E+00 | loss scale: 1.0 | grad norm: 2.667 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      605/    1000 | consumed samples:         1210 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.49 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.506657E+00 | loss scale: 1.0 | grad norm: 2.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      606/    1000 | consumed samples:         1212 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.62 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.841305E+00 | loss scale: 1.0 | grad norm: 1.831 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      607/    1000 | consumed samples:         1214 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.80 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.795358E+00 | loss scale: 1.0 | grad norm: 1.821 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      608/    1000 | consumed samples:         1216 | elapsed time per iteration (ms): 1319.3 | rate (tokens/sec): 6209.29 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.789057E+00 | loss scale: 1.0 | grad norm: 1.780 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      609/    1000 | consumed samples:         1218 | elapsed time per iteration (ms): 1319.1 | rate (tokens/sec): 6210.24 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.599072E+00 | loss scale: 1.0 | grad norm: 1.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      610/    1000 | consumed samples:         1220 | elapsed time per iteration (ms): 1319.3 | rate (tokens/sec): 6209.22 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.252043E+00 | loss scale: 1.0 | grad norm: 2.441 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      611/    1000 | consumed samples:         1222 | elapsed time per iteration (ms): 1318.2 | rate (tokens/sec): 6214.46 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 1.967875E+00 | loss scale: 1.0 | grad norm: 2.105 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      612/    1000 | consumed samples:         1224 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.64 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.213975E+00 | loss scale: 1.0 | grad norm: 1.853 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      613/    1000 | consumed samples:         1226 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.54 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.102107E+00 | loss scale: 1.0 | grad norm: 1.911 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      614/    1000 | consumed samples:         1228 | elapsed time per iteration (ms): 1321.4 | rate (tokens/sec): 6199.41 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.224558E+00 | loss scale: 1.0 | grad norm: 3.318 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      615/    1000 | consumed samples:         1230 | elapsed time per iteration (ms): 1320.8 | rate (tokens/sec): 6202.22 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.792745E+00 | loss scale: 1.0 | grad norm: 2.123 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      616/    1000 | consumed samples:         1232 | elapsed time per iteration (ms): 1321.7 | rate (tokens/sec): 6198.04 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.759256E+00 | loss scale: 1.0 | grad norm: 2.204 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      617/    1000 | consumed samples:         1234 | elapsed time per iteration (ms): 1317.9 | rate (tokens/sec): 6215.80 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.688178E+00 | loss scale: 1.0 | grad norm: 2.169 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      618/    1000 | consumed samples:         1236 | elapsed time per iteration (ms): 1319.2 | rate (tokens/sec): 6209.81 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.297690E+00 | loss scale: 1.0 | grad norm: 1.795 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      619/    1000 | consumed samples:         1238 | elapsed time per iteration (ms): 1319.1 | rate (tokens/sec): 6210.39 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.007674E+00 | loss scale: 1.0 | grad norm: 1.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      620/    1000 | consumed samples:         1240 | elapsed time per iteration (ms): 1320.2 | rate (tokens/sec): 6205.33 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.734976E+00 | loss scale: 1.0 | grad norm: 2.035 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      621/    1000 | consumed samples:         1242 | elapsed time per iteration (ms): 1318.7 | rate (tokens/sec): 6212.34 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.790025E+00 | loss scale: 1.0 | grad norm: 1.855 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      622/    1000 | consumed samples:         1244 | elapsed time per iteration (ms): 1321.4 | rate (tokens/sec): 6199.51 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.429383E+00 | loss scale: 1.0 | grad norm: 2.129 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      623/    1000 | consumed samples:         1246 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.56 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.583716E+00 | loss scale: 1.0 | grad norm: 2.362 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      624/    1000 | consumed samples:         1248 | elapsed time per iteration (ms): 1320.2 | rate (tokens/sec): 6205.29 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.079202E+00 | loss scale: 1.0 | grad norm: 3.356 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      625/    1000 | consumed samples:         1250 | elapsed time per iteration (ms): 1317.5 | rate (tokens/sec): 6217.82 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.361316E+00 | loss scale: 1.0 | grad norm: 2.348 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      626/    1000 | consumed samples:         1252 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6207.94 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.728866E+00 | loss scale: 1.0 | grad norm: 3.553 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      627/    1000 | consumed samples:         1254 | elapsed time per iteration (ms): 1320.4 | rate (tokens/sec): 6204.31 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.568492E+00 | loss scale: 1.0 | grad norm: 2.664 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      628/    1000 | consumed samples:         1256 | elapsed time per iteration (ms): 1320.6 | rate (tokens/sec): 6203.34 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.910482E+00 | loss scale: 1.0 | grad norm: 2.692 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      629/    1000 | consumed samples:         1258 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.76 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.457344E+00 | loss scale: 1.0 | grad norm: 3.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      630/    1000 | consumed samples:         1260 | elapsed time per iteration (ms): 1320.4 | rate (tokens/sec): 6204.07 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.080611E+00 | loss scale: 1.0 | grad norm: 2.967 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      631/    1000 | consumed samples:         1262 | elapsed time per iteration (ms): 1320.8 | rate (tokens/sec): 6202.12 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.958948E+00 | loss scale: 1.0 | grad norm: 2.241 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      632/    1000 | consumed samples:         1264 | elapsed time per iteration (ms): 1320.1 | rate (tokens/sec): 6205.48 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.394386E+00 | loss scale: 1.0 | grad norm: 1.890 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      633/    1000 | consumed samples:         1266 | elapsed time per iteration (ms): 1319.4 | rate (tokens/sec): 6208.71 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.433148E+00 | loss scale: 1.0 | grad norm: 3.379 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      634/    1000 | consumed samples:         1268 | elapsed time per iteration (ms): 1317.5 | rate (tokens/sec): 6217.90 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 1.848819E+00 | loss scale: 1.0 | grad norm: 1.976 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      635/    1000 | consumed samples:         1270 | elapsed time per iteration (ms): 1317.1 | rate (tokens/sec): 6219.55 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.824331E+00 | loss scale: 1.0 | grad norm: 2.446 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      636/    1000 | consumed samples:         1272 | elapsed time per iteration (ms): 1317.4 | rate (tokens/sec): 6218.33 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.469608E+00 | loss scale: 1.0 | grad norm: 2.059 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      637/    1000 | consumed samples:         1274 | elapsed time per iteration (ms): 1317.9 | rate (tokens/sec): 6215.86 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.784189E+00 | loss scale: 1.0 | grad norm: 2.180 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      638/    1000 | consumed samples:         1276 | elapsed time per iteration (ms): 1317.5 | rate (tokens/sec): 6217.63 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.813880E+00 | loss scale: 1.0 | grad norm: 1.829 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      639/    1000 | consumed samples:         1278 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.35 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.825746E+00 | loss scale: 1.0 | grad norm: 1.942 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      640/    1000 | consumed samples:         1280 | elapsed time per iteration (ms): 1320.7 | rate (tokens/sec): 6202.54 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.672052E+00 | loss scale: 1.0 | grad norm: 1.835 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      641/    1000 | consumed samples:         1282 | elapsed time per iteration (ms): 1321.5 | rate (tokens/sec): 6198.97 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.105631E+00 | loss scale: 1.0 | grad norm: 1.993 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      642/    1000 | consumed samples:         1284 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.76 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.579710E+00 | loss scale: 1.0 | grad norm: 2.013 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      643/    1000 | consumed samples:         1286 | elapsed time per iteration (ms): 1318.0 | rate (tokens/sec): 6215.60 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.586545E+00 | loss scale: 1.0 | grad norm: 1.929 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      644/    1000 | consumed samples:         1288 | elapsed time per iteration (ms): 1318.6 | rate (tokens/sec): 6212.43 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.467189E+00 | loss scale: 1.0 | grad norm: 1.894 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      645/    1000 | consumed samples:         1290 | elapsed time per iteration (ms): 1318.3 | rate (tokens/sec): 6214.24 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.539851E+00 | loss scale: 1.0 | grad norm: 4.911 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      646/    1000 | consumed samples:         1292 | elapsed time per iteration (ms): 1321.0 | rate (tokens/sec): 6201.49 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.648347E+00 | loss scale: 1.0 | grad norm: 2.442 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      647/    1000 | consumed samples:         1294 | elapsed time per iteration (ms): 1320.9 | rate (tokens/sec): 6201.89 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.811414E+00 | loss scale: 1.0 | grad norm: 2.819 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      648/    1000 | consumed samples:         1296 | elapsed time per iteration (ms): 1319.4 | rate (tokens/sec): 6208.95 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.525736E+00 | loss scale: 1.0 | grad norm: 2.884 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      649/    1000 | consumed samples:         1298 | elapsed time per iteration (ms): 1318.9 | rate (tokens/sec): 6211.41 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.001178E+00 | loss scale: 1.0 | grad norm: 2.195 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      650/    1000 | consumed samples:         1300 | elapsed time per iteration (ms): 1318.2 | rate (tokens/sec): 6214.75 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.720548E+00 | loss scale: 1.0 | grad norm: 1.831 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      651/    1000 | consumed samples:         1302 | elapsed time per iteration (ms): 1315.6 | rate (tokens/sec): 6226.81 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.531289E+00 | loss scale: 1.0 | grad norm: 1.673 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      652/    1000 | consumed samples:         1304 | elapsed time per iteration (ms): 1318.6 | rate (tokens/sec): 6212.45 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.542701E+00 | loss scale: 1.0 | grad norm: 1.858 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      653/    1000 | consumed samples:         1306 | elapsed time per iteration (ms): 1328.2 | rate (tokens/sec): 6167.61 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.827578E+00 | loss scale: 1.0 | grad norm: 2.228 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      654/    1000 | consumed samples:         1308 | elapsed time per iteration (ms): 1320.0 | rate (tokens/sec): 6206.17 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.434233E+00 | loss scale: 1.0 | grad norm: 2.141 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      655/    1000 | consumed samples:         1310 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.66 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.507802E+00 | loss scale: 1.0 | grad norm: 1.705 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      656/    1000 | consumed samples:         1312 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.40 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.913005E+00 | loss scale: 1.0 | grad norm: 2.116 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      657/    1000 | consumed samples:         1314 | elapsed time per iteration (ms): 1318.7 | rate (tokens/sec): 6212.25 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.774950E+00 | loss scale: 1.0 | grad norm: 1.970 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      658/    1000 | consumed samples:         1316 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.56 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.920475E+00 | loss scale: 1.0 | grad norm: 2.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      659/    1000 | consumed samples:         1318 | elapsed time per iteration (ms): 1317.6 | rate (tokens/sec): 6217.30 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.833873E+00 | loss scale: 1.0 | grad norm: 1.815 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      660/    1000 | consumed samples:         1320 | elapsed time per iteration (ms): 1318.2 | rate (tokens/sec): 6214.35 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.623517E+00 | loss scale: 1.0 | grad norm: 2.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      661/    1000 | consumed samples:         1322 | elapsed time per iteration (ms): 1316.2 | rate (tokens/sec): 6224.12 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.523951E+00 | loss scale: 1.0 | grad norm: 1.882 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      662/    1000 | consumed samples:         1324 | elapsed time per iteration (ms): 1318.2 | rate (tokens/sec): 6214.70 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.808908E+00 | loss scale: 1.0 | grad norm: 1.909 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      663/    1000 | consumed samples:         1326 | elapsed time per iteration (ms): 1317.8 | rate (tokens/sec): 6216.31 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.773532E+00 | loss scale: 1.0 | grad norm: 1.852 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      664/    1000 | consumed samples:         1328 | elapsed time per iteration (ms): 1318.6 | rate (tokens/sec): 6212.51 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.362981E+00 | loss scale: 1.0 | grad norm: 1.979 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      665/    1000 | consumed samples:         1330 | elapsed time per iteration (ms): 1318.4 | rate (tokens/sec): 6213.53 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.697722E+00 | loss scale: 1.0 | grad norm: 2.165 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      666/    1000 | consumed samples:         1332 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.60 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.716501E+00 | loss scale: 1.0 | grad norm: 1.766 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      667/    1000 | consumed samples:         1334 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.77 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.672855E+00 | loss scale: 1.0 | grad norm: 1.695 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      668/    1000 | consumed samples:         1336 | elapsed time per iteration (ms): 1317.9 | rate (tokens/sec): 6215.93 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.548273E+00 | loss scale: 1.0 | grad norm: 2.499 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      669/    1000 | consumed samples:         1338 | elapsed time per iteration (ms): 1320.0 | rate (tokens/sec): 6205.84 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.576280E+00 | loss scale: 1.0 | grad norm: 3.140 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      670/    1000 | consumed samples:         1340 | elapsed time per iteration (ms): 1319.8 | rate (tokens/sec): 6207.09 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.629385E+00 | loss scale: 1.0 | grad norm: 1.756 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      671/    1000 | consumed samples:         1342 | elapsed time per iteration (ms): 1321.0 | rate (tokens/sec): 6201.33 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.773736E+00 | loss scale: 1.0 | grad norm: 6.861 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      672/    1000 | consumed samples:         1344 | elapsed time per iteration (ms): 1320.9 | rate (tokens/sec): 6201.60 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.574359E+00 | loss scale: 1.0 | grad norm: 2.294 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      673/    1000 | consumed samples:         1346 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6208.07 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.059615E+00 | loss scale: 1.0 | grad norm: 1.754 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      674/    1000 | consumed samples:         1348 | elapsed time per iteration (ms): 1318.1 | rate (tokens/sec): 6214.83 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.545925E+00 | loss scale: 1.0 | grad norm: 9.705 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      675/    1000 | consumed samples:         1350 | elapsed time per iteration (ms): 1318.5 | rate (tokens/sec): 6213.25 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.868893E+00 | loss scale: 1.0 | grad norm: 2.201 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      676/    1000 | consumed samples:         1352 | elapsed time per iteration (ms): 1318.4 | rate (tokens/sec): 6213.63 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.777820E+00 | loss scale: 1.0 | grad norm: 2.089 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      677/    1000 | consumed samples:         1354 | elapsed time per iteration (ms): 1317.4 | rate (tokens/sec): 6218.54 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.293795E+00 | loss scale: 1.0 | grad norm: 2.436 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      678/    1000 | consumed samples:         1356 | elapsed time per iteration (ms): 1320.9 | rate (tokens/sec): 6202.06 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.778302E+00 | loss scale: 1.0 | grad norm: 2.131 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      679/    1000 | consumed samples:         1358 | elapsed time per iteration (ms): 1320.2 | rate (tokens/sec): 6205.28 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.640781E+00 | loss scale: 1.0 | grad norm: 1.890 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      680/    1000 | consumed samples:         1360 | elapsed time per iteration (ms): 1317.8 | rate (tokens/sec): 6216.30 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.794759E+00 | loss scale: 1.0 | grad norm: 3.066 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      681/    1000 | consumed samples:         1362 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.33 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.510186E+00 | loss scale: 1.0 | grad norm: 2.047 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      682/    1000 | consumed samples:         1364 | elapsed time per iteration (ms): 1319.8 | rate (tokens/sec): 6207.10 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.700459E+00 | loss scale: 1.0 | grad norm: 1.713 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      683/    1000 | consumed samples:         1366 | elapsed time per iteration (ms): 1317.3 | rate (tokens/sec): 6218.67 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.763524E+00 | loss scale: 1.0 | grad norm: 2.034 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      684/    1000 | consumed samples:         1368 | elapsed time per iteration (ms): 1318.5 | rate (tokens/sec): 6213.11 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.699496E+00 | loss scale: 1.0 | grad norm: 1.828 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      685/    1000 | consumed samples:         1370 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.62 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.552116E+00 | loss scale: 1.0 | grad norm: 1.975 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      686/    1000 | consumed samples:         1372 | elapsed time per iteration (ms): 1321.1 | rate (tokens/sec): 6200.75 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.467457E+00 | loss scale: 1.0 | grad norm: 2.068 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      687/    1000 | consumed samples:         1374 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.45 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.652323E+00 | loss scale: 1.0 | grad norm: 2.081 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      688/    1000 | consumed samples:         1376 | elapsed time per iteration (ms): 1317.9 | rate (tokens/sec): 6215.85 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.681716E+00 | loss scale: 1.0 | grad norm: 2.193 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      689/    1000 | consumed samples:         1378 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.58 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.617588E+00 | loss scale: 1.0 | grad norm: 2.462 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      690/    1000 | consumed samples:         1380 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6207.84 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.953827E+00 | loss scale: 1.0 | grad norm: 2.322 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      691/    1000 | consumed samples:         1382 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.74 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.689016E+00 | loss scale: 1.0 | grad norm: 2.454 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      692/    1000 | consumed samples:         1384 | elapsed time per iteration (ms): 1320.1 | rate (tokens/sec): 6205.81 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.255835E+00 | loss scale: 1.0 | grad norm: 2.350 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      693/    1000 | consumed samples:         1386 | elapsed time per iteration (ms): 1320.7 | rate (tokens/sec): 6202.79 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.493315E+00 | loss scale: 1.0 | grad norm: 2.494 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      694/    1000 | consumed samples:         1388 | elapsed time per iteration (ms): 1317.6 | rate (tokens/sec): 6217.17 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.964367E+00 | loss scale: 1.0 | grad norm: 2.323 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      695/    1000 | consumed samples:         1390 | elapsed time per iteration (ms): 1317.2 | rate (tokens/sec): 6219.14 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.563853E+00 | loss scale: 1.0 | grad norm: 2.053 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      696/    1000 | consumed samples:         1392 | elapsed time per iteration (ms): 1318.8 | rate (tokens/sec): 6211.71 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.389919E+00 | loss scale: 1.0 | grad norm: 2.335 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      697/    1000 | consumed samples:         1394 | elapsed time per iteration (ms): 1317.9 | rate (tokens/sec): 6215.72 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.427883E+00 | loss scale: 1.0 | grad norm: 4.683 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      698/    1000 | consumed samples:         1396 | elapsed time per iteration (ms): 1318.8 | rate (tokens/sec): 6211.55 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.392622E+00 | loss scale: 1.0 | grad norm: 2.197 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      699/    1000 | consumed samples:         1398 | elapsed time per iteration (ms): 1317.0 | rate (tokens/sec): 6219.96 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.407303E+00 | loss scale: 1.0 | grad norm: 2.091 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      700/    1000 | consumed samples:         1400 | elapsed time per iteration (ms): 1318.5 | rate (tokens/sec): 6213.27 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 1.721533E+00 | loss scale: 1.0 | grad norm: 2.354 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      701/    1000 | consumed samples:         1402 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6207.95 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.658017E+00 | loss scale: 1.0 | grad norm: 2.646 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      702/    1000 | consumed samples:         1404 | elapsed time per iteration (ms): 1318.8 | rate (tokens/sec): 6211.67 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.707774E+00 | loss scale: 1.0 | grad norm: 2.587 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      703/    1000 | consumed samples:         1406 | elapsed time per iteration (ms): 1317.9 | rate (tokens/sec): 6216.02 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.792423E+00 | loss scale: 1.0 | grad norm: 2.523 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      704/    1000 | consumed samples:         1408 | elapsed time per iteration (ms): 1317.9 | rate (tokens/sec): 6216.01 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.617640E+00 | loss scale: 1.0 | grad norm: 2.404 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      705/    1000 | consumed samples:         1410 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.89 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.074021E+00 | loss scale: 1.0 | grad norm: 1.998 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      706/    1000 | consumed samples:         1412 | elapsed time per iteration (ms): 1317.5 | rate (tokens/sec): 6217.68 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.079489E+00 | loss scale: 1.0 | grad norm: 2.486 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      707/    1000 | consumed samples:         1414 | elapsed time per iteration (ms): 1318.7 | rate (tokens/sec): 6212.07 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.472602E+00 | loss scale: 1.0 | grad norm: 1.770 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      708/    1000 | consumed samples:         1416 | elapsed time per iteration (ms): 1318.6 | rate (tokens/sec): 6212.87 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.402653E+00 | loss scale: 1.0 | grad norm: 1.920 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      709/    1000 | consumed samples:         1418 | elapsed time per iteration (ms): 1317.9 | rate (tokens/sec): 6216.07 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.828738E+00 | loss scale: 1.0 | grad norm: 2.573 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      710/    1000 | consumed samples:         1420 | elapsed time per iteration (ms): 1317.1 | rate (tokens/sec): 6219.49 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.610343E+00 | loss scale: 1.0 | grad norm: 2.631 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      711/    1000 | consumed samples:         1422 | elapsed time per iteration (ms): 1318.7 | rate (tokens/sec): 6212.10 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.882341E+00 | loss scale: 1.0 | grad norm: 2.212 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      712/    1000 | consumed samples:         1424 | elapsed time per iteration (ms): 1317.6 | rate (tokens/sec): 6217.19 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 1.975419E+00 | loss scale: 1.0 | grad norm: 1.795 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      713/    1000 | consumed samples:         1426 | elapsed time per iteration (ms): 1318.1 | rate (tokens/sec): 6215.18 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.652327E+00 | loss scale: 1.0 | grad norm: 1.888 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      714/    1000 | consumed samples:         1428 | elapsed time per iteration (ms): 1318.2 | rate (tokens/sec): 6214.48 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.938395E+00 | loss scale: 1.0 | grad norm: 2.374 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      715/    1000 | consumed samples:         1430 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.97 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.671114E+00 | loss scale: 1.0 | grad norm: 1.873 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      716/    1000 | consumed samples:         1432 | elapsed time per iteration (ms): 1318.5 | rate (tokens/sec): 6213.33 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.510835E+00 | loss scale: 1.0 | grad norm: 1.968 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      717/    1000 | consumed samples:         1434 | elapsed time per iteration (ms): 1317.4 | rate (tokens/sec): 6218.37 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.384753E+00 | loss scale: 1.0 | grad norm: 1.959 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      718/    1000 | consumed samples:         1436 | elapsed time per iteration (ms): 1316.8 | rate (tokens/sec): 6221.35 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.466424E+00 | loss scale: 1.0 | grad norm: 2.039 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      719/    1000 | consumed samples:         1438 | elapsed time per iteration (ms): 1317.4 | rate (tokens/sec): 6218.24 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.758026E+00 | loss scale: 1.0 | grad norm: 2.331 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      720/    1000 | consumed samples:         1440 | elapsed time per iteration (ms): 1321.0 | rate (tokens/sec): 6201.53 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.707376E+00 | loss scale: 1.0 | grad norm: 2.253 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      721/    1000 | consumed samples:         1442 | elapsed time per iteration (ms): 1326.1 | rate (tokens/sec): 6177.58 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 1.709045E+00 | loss scale: 1.0 | grad norm: 2.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      722/    1000 | consumed samples:         1444 | elapsed time per iteration (ms): 1319.5 | rate (tokens/sec): 6208.44 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.697526E+00 | loss scale: 1.0 | grad norm: 1.853 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      723/    1000 | consumed samples:         1446 | elapsed time per iteration (ms): 1317.5 | rate (tokens/sec): 6218.05 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.624354E+00 | loss scale: 1.0 | grad norm: 2.016 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      724/    1000 | consumed samples:         1448 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.85 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.661604E+00 | loss scale: 1.0 | grad norm: 1.512 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      725/    1000 | consumed samples:         1450 | elapsed time per iteration (ms): 1318.7 | rate (tokens/sec): 6212.26 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.287858E+00 | loss scale: 1.0 | grad norm: 1.950 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      726/    1000 | consumed samples:         1452 | elapsed time per iteration (ms): 1318.8 | rate (tokens/sec): 6211.70 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.861629E+00 | loss scale: 1.0 | grad norm: 2.150 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      727/    1000 | consumed samples:         1454 | elapsed time per iteration (ms): 1317.6 | rate (tokens/sec): 6217.30 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.717564E+00 | loss scale: 1.0 | grad norm: 2.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      728/    1000 | consumed samples:         1456 | elapsed time per iteration (ms): 1374.3 | rate (tokens/sec): 5960.96 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.165604E+00 | loss scale: 1.0 | grad norm: 2.037 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      729/    1000 | consumed samples:         1458 | elapsed time per iteration (ms): 1318.4 | rate (tokens/sec): 6213.65 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.719639E+00 | loss scale: 1.0 | grad norm: 2.167 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      730/    1000 | consumed samples:         1460 | elapsed time per iteration (ms): 1317.8 | rate (tokens/sec): 6216.47 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.986685E+00 | loss scale: 1.0 | grad norm: 9.276 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      731/    1000 | consumed samples:         1462 | elapsed time per iteration (ms): 1318.7 | rate (tokens/sec): 6212.37 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.495657E+00 | loss scale: 1.0 | grad norm: 2.032 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      732/    1000 | consumed samples:         1464 | elapsed time per iteration (ms): 1318.2 | rate (tokens/sec): 6214.39 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.748073E+00 | loss scale: 1.0 | grad norm: 1.864 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      733/    1000 | consumed samples:         1466 | elapsed time per iteration (ms): 1317.5 | rate (tokens/sec): 6217.75 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 1.869933E+00 | loss scale: 1.0 | grad norm: 2.750 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      734/    1000 | consumed samples:         1468 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.67 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.664551E+00 | loss scale: 1.0 | grad norm: 1.881 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      735/    1000 | consumed samples:         1470 | elapsed time per iteration (ms): 1318.9 | rate (tokens/sec): 6211.05 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.707573E+00 | loss scale: 1.0 | grad norm: 2.350 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      736/    1000 | consumed samples:         1472 | elapsed time per iteration (ms): 1317.0 | rate (tokens/sec): 6220.01 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 1.974917E+00 | loss scale: 1.0 | grad norm: 2.109 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      737/    1000 | consumed samples:         1474 | elapsed time per iteration (ms): 1317.5 | rate (tokens/sec): 6217.71 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.802615E+00 | loss scale: 1.0 | grad norm: 2.455 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      738/    1000 | consumed samples:         1476 | elapsed time per iteration (ms): 1318.3 | rate (tokens/sec): 6214.08 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.794783E+00 | loss scale: 1.0 | grad norm: 1.787 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      739/    1000 | consumed samples:         1478 | elapsed time per iteration (ms): 1317.6 | rate (tokens/sec): 6217.17 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.543572E+00 | loss scale: 1.0 | grad norm: 1.889 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      740/    1000 | consumed samples:         1480 | elapsed time per iteration (ms): 1316.5 | rate (tokens/sec): 6222.45 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.252202E+00 | loss scale: 1.0 | grad norm: 1.955 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      741/    1000 | consumed samples:         1482 | elapsed time per iteration (ms): 1320.8 | rate (tokens/sec): 6202.46 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.499799E+00 | loss scale: 1.0 | grad norm: 1.684 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      742/    1000 | consumed samples:         1484 | elapsed time per iteration (ms): 1319.1 | rate (tokens/sec): 6210.37 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.783109E+00 | loss scale: 1.0 | grad norm: 3.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      743/    1000 | consumed samples:         1486 | elapsed time per iteration (ms): 1321.1 | rate (tokens/sec): 6200.89 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.820451E+00 | loss scale: 1.0 | grad norm: 2.061 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      744/    1000 | consumed samples:         1488 | elapsed time per iteration (ms): 1317.6 | rate (tokens/sec): 6217.22 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.766044E+00 | loss scale: 1.0 | grad norm: 1.975 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      745/    1000 | consumed samples:         1490 | elapsed time per iteration (ms): 1321.7 | rate (tokens/sec): 6198.16 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.642656E+00 | loss scale: 1.0 | grad norm: 1.799 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      746/    1000 | consumed samples:         1492 | elapsed time per iteration (ms): 1319.2 | rate (tokens/sec): 6209.93 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.666588E+00 | loss scale: 1.0 | grad norm: 2.398 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      747/    1000 | consumed samples:         1494 | elapsed time per iteration (ms): 1318.3 | rate (tokens/sec): 6213.84 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.922887E+00 | loss scale: 1.0 | grad norm: 2.137 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      748/    1000 | consumed samples:         1496 | elapsed time per iteration (ms): 1317.1 | rate (tokens/sec): 6219.88 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.090554E+00 | loss scale: 1.0 | grad norm: 1.980 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      749/    1000 | consumed samples:         1498 | elapsed time per iteration (ms): 1317.9 | rate (tokens/sec): 6216.00 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.680239E+00 | loss scale: 1.0 | grad norm: 1.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      750/    1000 | consumed samples:         1500 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.72 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.647411E+00 | loss scale: 1.0 | grad norm: 2.242 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      751/    1000 | consumed samples:         1502 | elapsed time per iteration (ms): 1316.9 | rate (tokens/sec): 6220.54 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.711540E+00 | loss scale: 1.0 | grad norm: 2.994 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      752/    1000 | consumed samples:         1504 | elapsed time per iteration (ms): 1317.0 | rate (tokens/sec): 6220.30 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.227894E+00 | loss scale: 1.0 | grad norm: 1.896 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      753/    1000 | consumed samples:         1506 | elapsed time per iteration (ms): 1317.3 | rate (tokens/sec): 6218.67 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.772423E+00 | loss scale: 1.0 | grad norm: 1.820 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      754/    1000 | consumed samples:         1508 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.74 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.909126E+00 | loss scale: 1.0 | grad norm: 2.347 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      755/    1000 | consumed samples:         1510 | elapsed time per iteration (ms): 1318.9 | rate (tokens/sec): 6211.28 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.265731E+00 | loss scale: 1.0 | grad norm: 1.967 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      756/    1000 | consumed samples:         1512 | elapsed time per iteration (ms): 1317.5 | rate (tokens/sec): 6218.00 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.883013E+00 | loss scale: 1.0 | grad norm: 1.771 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      757/    1000 | consumed samples:         1514 | elapsed time per iteration (ms): 1318.8 | rate (tokens/sec): 6211.79 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.719584E+00 | loss scale: 1.0 | grad norm: 1.925 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      758/    1000 | consumed samples:         1516 | elapsed time per iteration (ms): 1318.7 | rate (tokens/sec): 6212.26 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.355554E+00 | loss scale: 1.0 | grad norm: 1.840 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      759/    1000 | consumed samples:         1518 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6207.73 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.602917E+00 | loss scale: 1.0 | grad norm: 1.646 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      760/    1000 | consumed samples:         1520 | elapsed time per iteration (ms): 1318.0 | rate (tokens/sec): 6215.48 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.824907E+00 | loss scale: 1.0 | grad norm: 2.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      761/    1000 | consumed samples:         1522 | elapsed time per iteration (ms): 1318.6 | rate (tokens/sec): 6212.55 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.146058E+00 | loss scale: 1.0 | grad norm: 1.827 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      762/    1000 | consumed samples:         1524 | elapsed time per iteration (ms): 1318.0 | rate (tokens/sec): 6215.52 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.924763E+00 | loss scale: 1.0 | grad norm: 2.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      763/    1000 | consumed samples:         1526 | elapsed time per iteration (ms): 1318.6 | rate (tokens/sec): 6212.64 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.530806E+00 | loss scale: 1.0 | grad norm: 1.835 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      764/    1000 | consumed samples:         1528 | elapsed time per iteration (ms): 1317.7 | rate (tokens/sec): 6217.04 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.694655E+00 | loss scale: 1.0 | grad norm: 2.672 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      765/    1000 | consumed samples:         1530 | elapsed time per iteration (ms): 1318.3 | rate (tokens/sec): 6214.29 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.907876E+00 | loss scale: 1.0 | grad norm: 1.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      766/    1000 | consumed samples:         1532 | elapsed time per iteration (ms): 1317.2 | rate (tokens/sec): 6219.38 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.409522E+00 | loss scale: 1.0 | grad norm: 2.026 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      767/    1000 | consumed samples:         1534 | elapsed time per iteration (ms): 1316.4 | rate (tokens/sec): 6223.10 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 1.978822E+00 | loss scale: 1.0 | grad norm: 1.968 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      768/    1000 | consumed samples:         1536 | elapsed time per iteration (ms): 1319.3 | rate (tokens/sec): 6209.36 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.631603E+00 | loss scale: 1.0 | grad norm: 2.260 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      769/    1000 | consumed samples:         1538 | elapsed time per iteration (ms): 1318.1 | rate (tokens/sec): 6215.04 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.830977E+00 | loss scale: 1.0 | grad norm: 2.194 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      770/    1000 | consumed samples:         1540 | elapsed time per iteration (ms): 1316.8 | rate (tokens/sec): 6220.94 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.535817E+00 | loss scale: 1.0 | grad norm: 2.037 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      771/    1000 | consumed samples:         1542 | elapsed time per iteration (ms): 1318.1 | rate (tokens/sec): 6214.81 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.863345E+00 | loss scale: 1.0 | grad norm: 2.266 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      772/    1000 | consumed samples:         1544 | elapsed time per iteration (ms): 1317.0 | rate (tokens/sec): 6220.31 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.904979E+00 | loss scale: 1.0 | grad norm: 1.949 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      773/    1000 | consumed samples:         1546 | elapsed time per iteration (ms): 1317.9 | rate (tokens/sec): 6216.04 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.667294E+00 | loss scale: 1.0 | grad norm: 1.964 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      774/    1000 | consumed samples:         1548 | elapsed time per iteration (ms): 1318.6 | rate (tokens/sec): 6212.83 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.072645E+00 | loss scale: 1.0 | grad norm: 3.112 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      775/    1000 | consumed samples:         1550 | elapsed time per iteration (ms): 1318.0 | rate (tokens/sec): 6215.30 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.641761E+00 | loss scale: 1.0 | grad norm: 1.630 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      776/    1000 | consumed samples:         1552 | elapsed time per iteration (ms): 1317.9 | rate (tokens/sec): 6215.81 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.046803E+00 | loss scale: 1.0 | grad norm: 2.433 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      777/    1000 | consumed samples:         1554 | elapsed time per iteration (ms): 1319.1 | rate (tokens/sec): 6210.24 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.728040E+00 | loss scale: 1.0 | grad norm: 1.712 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      778/    1000 | consumed samples:         1556 | elapsed time per iteration (ms): 1318.2 | rate (tokens/sec): 6214.45 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.375709E+00 | loss scale: 1.0 | grad norm: 1.704 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      779/    1000 | consumed samples:         1558 | elapsed time per iteration (ms): 1318.1 | rate (tokens/sec): 6215.20 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.787710E+00 | loss scale: 1.0 | grad norm: 1.650 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      780/    1000 | consumed samples:         1560 | elapsed time per iteration (ms): 1318.2 | rate (tokens/sec): 6214.47 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.462807E+00 | loss scale: 1.0 | grad norm: 1.857 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      781/    1000 | consumed samples:         1562 | elapsed time per iteration (ms): 1318.4 | rate (tokens/sec): 6213.36 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.406718E+00 | loss scale: 1.0 | grad norm: 2.637 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      782/    1000 | consumed samples:         1564 | elapsed time per iteration (ms): 1318.0 | rate (tokens/sec): 6215.66 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.538475E+00 | loss scale: 1.0 | grad norm: 1.875 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      783/    1000 | consumed samples:         1566 | elapsed time per iteration (ms): 1317.5 | rate (tokens/sec): 6217.68 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.699987E+00 | loss scale: 1.0 | grad norm: 2.138 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      784/    1000 | consumed samples:         1568 | elapsed time per iteration (ms): 1389.3 | rate (tokens/sec): 5896.35 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.522309E+00 | loss scale: 1.0 | grad norm: 2.161 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      785/    1000 | consumed samples:         1570 | elapsed time per iteration (ms): 1317.6 | rate (tokens/sec): 6217.55 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.834777E+00 | loss scale: 1.0 | grad norm: 1.766 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      786/    1000 | consumed samples:         1572 | elapsed time per iteration (ms): 1315.8 | rate (tokens/sec): 6226.01 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.004460E+00 | loss scale: 1.0 | grad norm: 3.040 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      787/    1000 | consumed samples:         1574 | elapsed time per iteration (ms): 1327.2 | rate (tokens/sec): 6172.22 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.439733E+00 | loss scale: 1.0 | grad norm: 1.882 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      788/    1000 | consumed samples:         1576 | elapsed time per iteration (ms): 1317.5 | rate (tokens/sec): 6217.65 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.304565E+00 | loss scale: 1.0 | grad norm: 1.574 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      789/    1000 | consumed samples:         1578 | elapsed time per iteration (ms): 1317.2 | rate (tokens/sec): 6219.47 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.458129E+00 | loss scale: 1.0 | grad norm: 1.814 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      790/    1000 | consumed samples:         1580 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.44 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.405282E+00 | loss scale: 1.0 | grad norm: 1.655 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      791/    1000 | consumed samples:         1582 | elapsed time per iteration (ms): 1318.3 | rate (tokens/sec): 6214.26 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.252115E+00 | loss scale: 1.0 | grad norm: 1.752 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      792/    1000 | consumed samples:         1584 | elapsed time per iteration (ms): 1317.1 | rate (tokens/sec): 6219.84 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.352726E+00 | loss scale: 1.0 | grad norm: 2.177 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      793/    1000 | consumed samples:         1586 | elapsed time per iteration (ms): 1318.0 | rate (tokens/sec): 6215.71 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.674066E+00 | loss scale: 1.0 | grad norm: 1.850 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      794/    1000 | consumed samples:         1588 | elapsed time per iteration (ms): 1316.9 | rate (tokens/sec): 6220.63 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.660681E+00 | loss scale: 1.0 | grad norm: 1.900 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      795/    1000 | consumed samples:         1590 | elapsed time per iteration (ms): 1316.5 | rate (tokens/sec): 6222.66 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.636906E+00 | loss scale: 1.0 | grad norm: 2.074 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      796/    1000 | consumed samples:         1592 | elapsed time per iteration (ms): 1317.5 | rate (tokens/sec): 6218.06 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.533385E+00 | loss scale: 1.0 | grad norm: 2.558 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      797/    1000 | consumed samples:         1594 | elapsed time per iteration (ms): 1315.4 | rate (tokens/sec): 6227.59 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.546175E+00 | loss scale: 1.0 | grad norm: 2.052 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      798/    1000 | consumed samples:         1596 | elapsed time per iteration (ms): 1316.1 | rate (tokens/sec): 6224.52 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.397156E+00 | loss scale: 1.0 | grad norm: 1.936 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      799/    1000 | consumed samples:         1598 | elapsed time per iteration (ms): 1317.1 | rate (tokens/sec): 6219.69 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.632169E+00 | loss scale: 1.0 | grad norm: 1.657 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      800/    1000 | consumed samples:         1600 | elapsed time per iteration (ms): 1318.0 | rate (tokens/sec): 6215.56 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.249309E+00 | loss scale: 1.0 | grad norm: 1.918 | number of skipped iterations:   0 | number of nan iterations:   0 |
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 validation loss at iteration 800 | lm loss value: 2.749856E+00 | lm loss PPL: 1.564038E+01 | ppl value: 1.597229E+01 | lm accuracy value: 4.334717E-01 | count loss mask value: 4.096000E+03 | 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
saving checkpoint at iteration     800 to /root/models/llama-2-7b-chat-hf-megatron_shard_tp_2_pp_1-pretrained
  successfully saved checkpoint at iteration     800 to /root/models/llama-2-7b-chat-hf-megatron_shard_tp_2_pp_1-pretrained
(min, max) time across ranks (ms):
    save-checkpoint ................................: (76288.71, 76288.73)
 iteration      801/    1000 | consumed samples:         1602 | elapsed time per iteration (ms): 81479.3 | rate (tokens/sec): 100.54 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.450737E+00 | loss scale: 1.0 | grad norm: 1.635 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      802/    1000 | consumed samples:         1604 | elapsed time per iteration (ms): 1306.8 | rate (tokens/sec): 6268.83 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.543203E+00 | loss scale: 1.0 | grad norm: 1.932 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      803/    1000 | consumed samples:         1606 | elapsed time per iteration (ms): 1309.7 | rate (tokens/sec): 6254.90 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.513334E+00 | loss scale: 1.0 | grad norm: 1.811 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      804/    1000 | consumed samples:         1608 | elapsed time per iteration (ms): 1309.6 | rate (tokens/sec): 6255.47 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.481560E+00 | loss scale: 1.0 | grad norm: 1.771 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      805/    1000 | consumed samples:         1610 | elapsed time per iteration (ms): 1313.1 | rate (tokens/sec): 6238.50 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.672384E+00 | loss scale: 1.0 | grad norm: 2.019 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      806/    1000 | consumed samples:         1612 | elapsed time per iteration (ms): 1310.2 | rate (tokens/sec): 6252.63 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.974180E+00 | loss scale: 1.0 | grad norm: 1.909 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      807/    1000 | consumed samples:         1614 | elapsed time per iteration (ms): 1309.9 | rate (tokens/sec): 6253.97 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.747288E+00 | loss scale: 1.0 | grad norm: 3.653 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      808/    1000 | consumed samples:         1616 | elapsed time per iteration (ms): 1312.8 | rate (tokens/sec): 6240.27 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.597836E+00 | loss scale: 1.0 | grad norm: 1.888 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      809/    1000 | consumed samples:         1618 | elapsed time per iteration (ms): 1314.7 | rate (tokens/sec): 6231.29 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.833055E+00 | loss scale: 1.0 | grad norm: 2.596 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      810/    1000 | consumed samples:         1620 | elapsed time per iteration (ms): 1314.7 | rate (tokens/sec): 6231.08 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.425451E+00 | loss scale: 1.0 | grad norm: 2.116 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      811/    1000 | consumed samples:         1622 | elapsed time per iteration (ms): 1311.4 | rate (tokens/sec): 6246.74 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.521286E+00 | loss scale: 1.0 | grad norm: 1.888 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      812/    1000 | consumed samples:         1624 | elapsed time per iteration (ms): 1314.9 | rate (tokens/sec): 6230.16 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.644789E+00 | loss scale: 1.0 | grad norm: 2.405 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      813/    1000 | consumed samples:         1626 | elapsed time per iteration (ms): 1315.7 | rate (tokens/sec): 6226.16 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.439192E+00 | loss scale: 1.0 | grad norm: 3.122 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      814/    1000 | consumed samples:         1628 | elapsed time per iteration (ms): 1316.4 | rate (tokens/sec): 6223.18 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.654889E+00 | loss scale: 1.0 | grad norm: 1.937 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      815/    1000 | consumed samples:         1630 | elapsed time per iteration (ms): 1317.0 | rate (tokens/sec): 6220.19 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 1.989508E+00 | loss scale: 1.0 | grad norm: 2.197 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      816/    1000 | consumed samples:         1632 | elapsed time per iteration (ms): 1319.0 | rate (tokens/sec): 6210.78 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.422346E+00 | loss scale: 1.0 | grad norm: 2.036 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      817/    1000 | consumed samples:         1634 | elapsed time per iteration (ms): 1320.1 | rate (tokens/sec): 6205.45 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.812368E+00 | loss scale: 1.0 | grad norm: 1.824 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      818/    1000 | consumed samples:         1636 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6208.05 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.603599E+00 | loss scale: 1.0 | grad norm: 2.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      819/    1000 | consumed samples:         1638 | elapsed time per iteration (ms): 1321.2 | rate (tokens/sec): 6200.19 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.802601E+00 | loss scale: 1.0 | grad norm: 1.933 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      820/    1000 | consumed samples:         1640 | elapsed time per iteration (ms): 1320.9 | rate (tokens/sec): 6202.04 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.735389E+00 | loss scale: 1.0 | grad norm: 2.388 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      821/    1000 | consumed samples:         1642 | elapsed time per iteration (ms): 1320.9 | rate (tokens/sec): 6201.78 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.488185E+00 | loss scale: 1.0 | grad norm: 2.245 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      822/    1000 | consumed samples:         1644 | elapsed time per iteration (ms): 1320.4 | rate (tokens/sec): 6204.22 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.732592E+00 | loss scale: 1.0 | grad norm: 2.233 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      823/    1000 | consumed samples:         1646 | elapsed time per iteration (ms): 1321.9 | rate (tokens/sec): 6197.20 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.510590E+00 | loss scale: 1.0 | grad norm: 1.918 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      824/    1000 | consumed samples:         1648 | elapsed time per iteration (ms): 1322.8 | rate (tokens/sec): 6192.99 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.633472E+00 | loss scale: 1.0 | grad norm: 2.314 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      825/    1000 | consumed samples:         1650 | elapsed time per iteration (ms): 1323.0 | rate (tokens/sec): 6191.82 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.565881E+00 | loss scale: 1.0 | grad norm: 1.831 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      826/    1000 | consumed samples:         1652 | elapsed time per iteration (ms): 1323.0 | rate (tokens/sec): 6192.12 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.163416E+00 | loss scale: 1.0 | grad norm: 1.752 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      827/    1000 | consumed samples:         1654 | elapsed time per iteration (ms): 1323.9 | rate (tokens/sec): 6187.92 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.029644E+00 | loss scale: 1.0 | grad norm: 2.373 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      828/    1000 | consumed samples:         1656 | elapsed time per iteration (ms): 1325.5 | rate (tokens/sec): 6180.48 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.260561E+00 | loss scale: 1.0 | grad norm: 1.845 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      829/    1000 | consumed samples:         1658 | elapsed time per iteration (ms): 1323.8 | rate (tokens/sec): 6188.29 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.569019E+00 | loss scale: 1.0 | grad norm: 2.449 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      830/    1000 | consumed samples:         1660 | elapsed time per iteration (ms): 1323.8 | rate (tokens/sec): 6188.31 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.834558E+00 | loss scale: 1.0 | grad norm: 2.148 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      831/    1000 | consumed samples:         1662 | elapsed time per iteration (ms): 1322.7 | rate (tokens/sec): 6193.49 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.799757E+00 | loss scale: 1.0 | grad norm: 2.061 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      832/    1000 | consumed samples:         1664 | elapsed time per iteration (ms): 1321.8 | rate (tokens/sec): 6197.55 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.718284E+00 | loss scale: 1.0 | grad norm: 1.895 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      833/    1000 | consumed samples:         1666 | elapsed time per iteration (ms): 1323.8 | rate (tokens/sec): 6188.27 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.787017E+00 | loss scale: 1.0 | grad norm: 1.842 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      834/    1000 | consumed samples:         1668 | elapsed time per iteration (ms): 1322.7 | rate (tokens/sec): 6193.48 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.289172E+00 | loss scale: 1.0 | grad norm: 1.979 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      835/    1000 | consumed samples:         1670 | elapsed time per iteration (ms): 1324.5 | rate (tokens/sec): 6185.01 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.680041E+00 | loss scale: 1.0 | grad norm: 1.899 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      836/    1000 | consumed samples:         1672 | elapsed time per iteration (ms): 1323.1 | rate (tokens/sec): 6191.55 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.213461E+00 | loss scale: 1.0 | grad norm: 1.764 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      837/    1000 | consumed samples:         1674 | elapsed time per iteration (ms): 1322.8 | rate (tokens/sec): 6192.72 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.646550E+00 | loss scale: 1.0 | grad norm: 1.858 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      838/    1000 | consumed samples:         1676 | elapsed time per iteration (ms): 1323.2 | rate (tokens/sec): 6191.07 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.574005E+00 | loss scale: 1.0 | grad norm: 2.098 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      839/    1000 | consumed samples:         1678 | elapsed time per iteration (ms): 1324.6 | rate (tokens/sec): 6184.73 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.506354E+00 | loss scale: 1.0 | grad norm: 2.306 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      840/    1000 | consumed samples:         1680 | elapsed time per iteration (ms): 1325.3 | rate (tokens/sec): 6181.06 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.609637E+00 | loss scale: 1.0 | grad norm: 1.968 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      841/    1000 | consumed samples:         1682 | elapsed time per iteration (ms): 1323.7 | rate (tokens/sec): 6188.61 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.591034E+00 | loss scale: 1.0 | grad norm: 1.853 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      842/    1000 | consumed samples:         1684 | elapsed time per iteration (ms): 1322.5 | rate (tokens/sec): 6194.14 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.313725E+00 | loss scale: 1.0 | grad norm: 1.732 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      843/    1000 | consumed samples:         1686 | elapsed time per iteration (ms): 1321.9 | rate (tokens/sec): 6197.00 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.551934E+00 | loss scale: 1.0 | grad norm: 7.341 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      844/    1000 | consumed samples:         1688 | elapsed time per iteration (ms): 1322.1 | rate (tokens/sec): 6196.31 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.625797E+00 | loss scale: 1.0 | grad norm: 1.667 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      845/    1000 | consumed samples:         1690 | elapsed time per iteration (ms): 1323.5 | rate (tokens/sec): 6189.63 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.723905E+00 | loss scale: 1.0 | grad norm: 1.979 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      846/    1000 | consumed samples:         1692 | elapsed time per iteration (ms): 1323.0 | rate (tokens/sec): 6192.00 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.775390E+00 | loss scale: 1.0 | grad norm: 1.828 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      847/    1000 | consumed samples:         1694 | elapsed time per iteration (ms): 1324.6 | rate (tokens/sec): 6184.45 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.252454E+00 | loss scale: 1.0 | grad norm: 2.039 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      848/    1000 | consumed samples:         1696 | elapsed time per iteration (ms): 1323.2 | rate (tokens/sec): 6191.24 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.844022E+00 | loss scale: 1.0 | grad norm: 1.779 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      849/    1000 | consumed samples:         1698 | elapsed time per iteration (ms): 1324.4 | rate (tokens/sec): 6185.28 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.825809E+00 | loss scale: 1.0 | grad norm: 1.714 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      850/    1000 | consumed samples:         1700 | elapsed time per iteration (ms): 1329.7 | rate (tokens/sec): 6160.84 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.470778E+00 | loss scale: 1.0 | grad norm: 1.611 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      851/    1000 | consumed samples:         1702 | elapsed time per iteration (ms): 1329.2 | rate (tokens/sec): 6163.13 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 1.939420E+00 | loss scale: 1.0 | grad norm: 1.972 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      852/    1000 | consumed samples:         1704 | elapsed time per iteration (ms): 1322.5 | rate (tokens/sec): 6194.31 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.631528E+00 | loss scale: 1.0 | grad norm: 2.325 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      853/    1000 | consumed samples:         1706 | elapsed time per iteration (ms): 1323.9 | rate (tokens/sec): 6187.83 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.468220E+00 | loss scale: 1.0 | grad norm: 3.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      854/    1000 | consumed samples:         1708 | elapsed time per iteration (ms): 1322.0 | rate (tokens/sec): 6196.55 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.009074E+00 | loss scale: 1.0 | grad norm: 2.164 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      855/    1000 | consumed samples:         1710 | elapsed time per iteration (ms): 1323.7 | rate (tokens/sec): 6188.87 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.761927E+00 | loss scale: 1.0 | grad norm: 1.998 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      856/    1000 | consumed samples:         1712 | elapsed time per iteration (ms): 1322.4 | rate (tokens/sec): 6194.89 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 1.840095E+00 | loss scale: 1.0 | grad norm: 4.156 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      857/    1000 | consumed samples:         1714 | elapsed time per iteration (ms): 1323.1 | rate (tokens/sec): 6191.39 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.562311E+00 | loss scale: 1.0 | grad norm: 3.201 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      858/    1000 | consumed samples:         1716 | elapsed time per iteration (ms): 1322.0 | rate (tokens/sec): 6196.64 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.589181E+00 | loss scale: 1.0 | grad norm: 1.731 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      859/    1000 | consumed samples:         1718 | elapsed time per iteration (ms): 1323.0 | rate (tokens/sec): 6192.05 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.696121E+00 | loss scale: 1.0 | grad norm: 2.116 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      860/    1000 | consumed samples:         1720 | elapsed time per iteration (ms): 1325.7 | rate (tokens/sec): 6179.33 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.718629E+00 | loss scale: 1.0 | grad norm: 2.595 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      861/    1000 | consumed samples:         1722 | elapsed time per iteration (ms): 1323.5 | rate (tokens/sec): 6189.45 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.501438E+00 | loss scale: 1.0 | grad norm: 2.147 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      862/    1000 | consumed samples:         1724 | elapsed time per iteration (ms): 1322.5 | rate (tokens/sec): 6194.45 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.742609E+00 | loss scale: 1.0 | grad norm: 1.809 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      863/    1000 | consumed samples:         1726 | elapsed time per iteration (ms): 1322.8 | rate (tokens/sec): 6192.88 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 1.734092E+00 | loss scale: 1.0 | grad norm: 2.405 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      864/    1000 | consumed samples:         1728 | elapsed time per iteration (ms): 1323.1 | rate (tokens/sec): 6191.71 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.651193E+00 | loss scale: 1.0 | grad norm: 1.678 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      865/    1000 | consumed samples:         1730 | elapsed time per iteration (ms): 1322.1 | rate (tokens/sec): 6195.99 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 1.916723E+00 | loss scale: 1.0 | grad norm: 2.030 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      866/    1000 | consumed samples:         1732 | elapsed time per iteration (ms): 1322.7 | rate (tokens/sec): 6193.33 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.686649E+00 | loss scale: 1.0 | grad norm: 1.677 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      867/    1000 | consumed samples:         1734 | elapsed time per iteration (ms): 1323.7 | rate (tokens/sec): 6188.48 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.513104E+00 | loss scale: 1.0 | grad norm: 2.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      868/    1000 | consumed samples:         1736 | elapsed time per iteration (ms): 1321.5 | rate (tokens/sec): 6198.87 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.592331E+00 | loss scale: 1.0 | grad norm: 2.047 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      869/    1000 | consumed samples:         1738 | elapsed time per iteration (ms): 1323.0 | rate (tokens/sec): 6191.83 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.550256E+00 | loss scale: 1.0 | grad norm: 2.307 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      870/    1000 | consumed samples:         1740 | elapsed time per iteration (ms): 1321.8 | rate (tokens/sec): 6197.81 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.491156E+00 | loss scale: 1.0 | grad norm: 2.841 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      871/    1000 | consumed samples:         1742 | elapsed time per iteration (ms): 1322.7 | rate (tokens/sec): 6193.60 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.056674E+00 | loss scale: 1.0 | grad norm: 2.066 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      872/    1000 | consumed samples:         1744 | elapsed time per iteration (ms): 1323.8 | rate (tokens/sec): 6188.42 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.464379E+00 | loss scale: 1.0 | grad norm: 2.332 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      873/    1000 | consumed samples:         1746 | elapsed time per iteration (ms): 1325.7 | rate (tokens/sec): 6179.57 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.771268E+00 | loss scale: 1.0 | grad norm: 2.095 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      874/    1000 | consumed samples:         1748 | elapsed time per iteration (ms): 1321.2 | rate (tokens/sec): 6200.47 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.854232E+00 | loss scale: 1.0 | grad norm: 1.966 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      875/    1000 | consumed samples:         1750 | elapsed time per iteration (ms): 1324.8 | rate (tokens/sec): 6183.39 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.379218E+00 | loss scale: 1.0 | grad norm: 2.519 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      876/    1000 | consumed samples:         1752 | elapsed time per iteration (ms): 1322.7 | rate (tokens/sec): 6193.38 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.362434E+00 | loss scale: 1.0 | grad norm: 1.802 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      877/    1000 | consumed samples:         1754 | elapsed time per iteration (ms): 1322.1 | rate (tokens/sec): 6196.17 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.375198E+00 | loss scale: 1.0 | grad norm: 1.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      878/    1000 | consumed samples:         1756 | elapsed time per iteration (ms): 1321.2 | rate (tokens/sec): 6200.35 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.758000E+00 | loss scale: 1.0 | grad norm: 1.951 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      879/    1000 | consumed samples:         1758 | elapsed time per iteration (ms): 1322.4 | rate (tokens/sec): 6194.69 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.791918E+00 | loss scale: 1.0 | grad norm: 1.931 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      880/    1000 | consumed samples:         1760 | elapsed time per iteration (ms): 1323.6 | rate (tokens/sec): 6189.31 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.179538E+00 | loss scale: 1.0 | grad norm: 2.307 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      881/    1000 | consumed samples:         1762 | elapsed time per iteration (ms): 1324.4 | rate (tokens/sec): 6185.55 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.661768E+00 | loss scale: 1.0 | grad norm: 1.999 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      882/    1000 | consumed samples:         1764 | elapsed time per iteration (ms): 1379.8 | rate (tokens/sec): 5937.31 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.995030E+00 | loss scale: 1.0 | grad norm: 1.995 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      883/    1000 | consumed samples:         1766 | elapsed time per iteration (ms): 1322.3 | rate (tokens/sec): 6195.22 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.413284E+00 | loss scale: 1.0 | grad norm: 1.799 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      884/    1000 | consumed samples:         1768 | elapsed time per iteration (ms): 1324.2 | rate (tokens/sec): 6186.54 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.719527E+00 | loss scale: 1.0 | grad norm: 1.745 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      885/    1000 | consumed samples:         1770 | elapsed time per iteration (ms): 1323.4 | rate (tokens/sec): 6189.92 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.391922E+00 | loss scale: 1.0 | grad norm: 1.715 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      886/    1000 | consumed samples:         1772 | elapsed time per iteration (ms): 1322.3 | rate (tokens/sec): 6195.24 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.743423E+00 | loss scale: 1.0 | grad norm: 1.631 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      887/    1000 | consumed samples:         1774 | elapsed time per iteration (ms): 1322.4 | rate (tokens/sec): 6194.75 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.624853E+00 | loss scale: 1.0 | grad norm: 1.835 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      888/    1000 | consumed samples:         1776 | elapsed time per iteration (ms): 1321.9 | rate (tokens/sec): 6196.91 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.821431E+00 | loss scale: 1.0 | grad norm: 1.830 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      889/    1000 | consumed samples:         1778 | elapsed time per iteration (ms): 1323.5 | rate (tokens/sec): 6189.78 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.459177E+00 | loss scale: 1.0 | grad norm: 2.736 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      890/    1000 | consumed samples:         1780 | elapsed time per iteration (ms): 1322.8 | rate (tokens/sec): 6193.01 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.776803E+00 | loss scale: 1.0 | grad norm: 4.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      891/    1000 | consumed samples:         1782 | elapsed time per iteration (ms): 1323.4 | rate (tokens/sec): 6190.29 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.657277E+00 | loss scale: 1.0 | grad norm: 1.979 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      892/    1000 | consumed samples:         1784 | elapsed time per iteration (ms): 1323.6 | rate (tokens/sec): 6189.05 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.192229E+00 | loss scale: 1.0 | grad norm: 2.307 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      893/    1000 | consumed samples:         1786 | elapsed time per iteration (ms): 1323.4 | rate (tokens/sec): 6190.24 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.676953E+00 | loss scale: 1.0 | grad norm: 1.678 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      894/    1000 | consumed samples:         1788 | elapsed time per iteration (ms): 1325.6 | rate (tokens/sec): 6179.71 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.788455E+00 | loss scale: 1.0 | grad norm: 1.799 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      895/    1000 | consumed samples:         1790 | elapsed time per iteration (ms): 1325.2 | rate (tokens/sec): 6181.87 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.643849E+00 | loss scale: 1.0 | grad norm: 1.773 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      896/    1000 | consumed samples:         1792 | elapsed time per iteration (ms): 1325.4 | rate (tokens/sec): 6180.92 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.409773E+00 | loss scale: 1.0 | grad norm: 1.877 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      897/    1000 | consumed samples:         1794 | elapsed time per iteration (ms): 1323.7 | rate (tokens/sec): 6188.82 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.164983E+00 | loss scale: 1.0 | grad norm: 1.705 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      898/    1000 | consumed samples:         1796 | elapsed time per iteration (ms): 1325.2 | rate (tokens/sec): 6181.50 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.838003E+00 | loss scale: 1.0 | grad norm: 2.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      899/    1000 | consumed samples:         1798 | elapsed time per iteration (ms): 1321.6 | rate (tokens/sec): 6198.37 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.817495E+00 | loss scale: 1.0 | grad norm: 1.916 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      900/    1000 | consumed samples:         1800 | elapsed time per iteration (ms): 1323.1 | rate (tokens/sec): 6191.63 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.584463E+00 | loss scale: 1.0 | grad norm: 1.790 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      901/    1000 | consumed samples:         1802 | elapsed time per iteration (ms): 1323.5 | rate (tokens/sec): 6189.64 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.209366E+00 | loss scale: 1.0 | grad norm: 1.799 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      902/    1000 | consumed samples:         1804 | elapsed time per iteration (ms): 1323.0 | rate (tokens/sec): 6191.90 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.577280E+00 | loss scale: 1.0 | grad norm: 1.820 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      903/    1000 | consumed samples:         1806 | elapsed time per iteration (ms): 1322.7 | rate (tokens/sec): 6193.47 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.771852E+00 | loss scale: 1.0 | grad norm: 1.618 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      904/    1000 | consumed samples:         1808 | elapsed time per iteration (ms): 1324.0 | rate (tokens/sec): 6187.29 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.859108E+00 | loss scale: 1.0 | grad norm: 2.061 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      905/    1000 | consumed samples:         1810 | elapsed time per iteration (ms): 1323.4 | rate (tokens/sec): 6190.35 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.756155E+00 | loss scale: 1.0 | grad norm: 1.665 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      906/    1000 | consumed samples:         1812 | elapsed time per iteration (ms): 1323.0 | rate (tokens/sec): 6192.12 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.665290E+00 | loss scale: 1.0 | grad norm: 2.261 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      907/    1000 | consumed samples:         1814 | elapsed time per iteration (ms): 1323.2 | rate (tokens/sec): 6191.16 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.506244E+00 | loss scale: 1.0 | grad norm: 1.917 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      908/    1000 | consumed samples:         1816 | elapsed time per iteration (ms): 1322.3 | rate (tokens/sec): 6195.06 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.706096E+00 | loss scale: 1.0 | grad norm: 2.734 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      909/    1000 | consumed samples:         1818 | elapsed time per iteration (ms): 1322.9 | rate (tokens/sec): 6192.68 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.773426E+00 | loss scale: 1.0 | grad norm: 1.764 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      910/    1000 | consumed samples:         1820 | elapsed time per iteration (ms): 1324.8 | rate (tokens/sec): 6183.51 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.444717E+00 | loss scale: 1.0 | grad norm: 1.672 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      911/    1000 | consumed samples:         1822 | elapsed time per iteration (ms): 1324.1 | rate (tokens/sec): 6186.70 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.559810E+00 | loss scale: 1.0 | grad norm: 1.819 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      912/    1000 | consumed samples:         1824 | elapsed time per iteration (ms): 1323.6 | rate (tokens/sec): 6189.32 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.654062E+00 | loss scale: 1.0 | grad norm: 2.628 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      913/    1000 | consumed samples:         1826 | elapsed time per iteration (ms): 1323.4 | rate (tokens/sec): 6190.27 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.644613E+00 | loss scale: 1.0 | grad norm: 2.249 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      914/    1000 | consumed samples:         1828 | elapsed time per iteration (ms): 1323.4 | rate (tokens/sec): 6189.92 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.812782E+00 | loss scale: 1.0 | grad norm: 2.251 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      915/    1000 | consumed samples:         1830 | elapsed time per iteration (ms): 1323.4 | rate (tokens/sec): 6189.97 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.493430E+00 | loss scale: 1.0 | grad norm: 1.912 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      916/    1000 | consumed samples:         1832 | elapsed time per iteration (ms): 1322.7 | rate (tokens/sec): 6193.54 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.409960E+00 | loss scale: 1.0 | grad norm: 1.986 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      917/    1000 | consumed samples:         1834 | elapsed time per iteration (ms): 1322.7 | rate (tokens/sec): 6193.54 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.263727E+00 | loss scale: 1.0 | grad norm: 2.390 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      918/    1000 | consumed samples:         1836 | elapsed time per iteration (ms): 1325.0 | rate (tokens/sec): 6182.83 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.724254E+00 | loss scale: 1.0 | grad norm: 1.976 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      919/    1000 | consumed samples:         1838 | elapsed time per iteration (ms): 1324.1 | rate (tokens/sec): 6186.85 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.570366E+00 | loss scale: 1.0 | grad norm: 1.966 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      920/    1000 | consumed samples:         1840 | elapsed time per iteration (ms): 1324.0 | rate (tokens/sec): 6187.18 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.652291E+00 | loss scale: 1.0 | grad norm: 1.967 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      921/    1000 | consumed samples:         1842 | elapsed time per iteration (ms): 1323.1 | rate (tokens/sec): 6191.45 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.680153E+00 | loss scale: 1.0 | grad norm: 1.795 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      922/    1000 | consumed samples:         1844 | elapsed time per iteration (ms): 1323.3 | rate (tokens/sec): 6190.64 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.664734E+00 | loss scale: 1.0 | grad norm: 1.746 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      923/    1000 | consumed samples:         1846 | elapsed time per iteration (ms): 1323.7 | rate (tokens/sec): 6188.49 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.817780E+00 | loss scale: 1.0 | grad norm: 1.763 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      924/    1000 | consumed samples:         1848 | elapsed time per iteration (ms): 1324.5 | rate (tokens/sec): 6184.87 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.281446E+00 | loss scale: 1.0 | grad norm: 1.848 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      925/    1000 | consumed samples:         1850 | elapsed time per iteration (ms): 1323.2 | rate (tokens/sec): 6190.88 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.313398E+00 | loss scale: 1.0 | grad norm: 1.887 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      926/    1000 | consumed samples:         1852 | elapsed time per iteration (ms): 1323.2 | rate (tokens/sec): 6191.17 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.393397E+00 | loss scale: 1.0 | grad norm: 2.148 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      927/    1000 | consumed samples:         1854 | elapsed time per iteration (ms): 1322.3 | rate (tokens/sec): 6195.22 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.595477E+00 | loss scale: 1.0 | grad norm: 2.282 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      928/    1000 | consumed samples:         1856 | elapsed time per iteration (ms): 1324.4 | rate (tokens/sec): 6185.32 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.657322E+00 | loss scale: 1.0 | grad norm: 1.972 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      929/    1000 | consumed samples:         1858 | elapsed time per iteration (ms): 1335.4 | rate (tokens/sec): 6134.49 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.020227E+00 | loss scale: 1.0 | grad norm: 2.572 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      930/    1000 | consumed samples:         1860 | elapsed time per iteration (ms): 1323.4 | rate (tokens/sec): 6190.10 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.695207E+00 | loss scale: 1.0 | grad norm: 1.857 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      931/    1000 | consumed samples:         1862 | elapsed time per iteration (ms): 1322.2 | rate (tokens/sec): 6195.90 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.509226E+00 | loss scale: 1.0 | grad norm: 1.923 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      932/    1000 | consumed samples:         1864 | elapsed time per iteration (ms): 1323.2 | rate (tokens/sec): 6191.03 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.656511E+00 | loss scale: 1.0 | grad norm: 1.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      933/    1000 | consumed samples:         1866 | elapsed time per iteration (ms): 1321.1 | rate (tokens/sec): 6200.85 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.395254E+00 | loss scale: 1.0 | grad norm: 1.563 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      934/    1000 | consumed samples:         1868 | elapsed time per iteration (ms): 1321.8 | rate (tokens/sec): 6197.40 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.781324E+00 | loss scale: 1.0 | grad norm: 1.661 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      935/    1000 | consumed samples:         1870 | elapsed time per iteration (ms): 1321.4 | rate (tokens/sec): 6199.48 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.268064E+00 | loss scale: 1.0 | grad norm: 1.794 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      936/    1000 | consumed samples:         1872 | elapsed time per iteration (ms): 1320.6 | rate (tokens/sec): 6203.35 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.432073E+00 | loss scale: 1.0 | grad norm: 1.701 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      937/    1000 | consumed samples:         1874 | elapsed time per iteration (ms): 1321.8 | rate (tokens/sec): 6197.58 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.272586E+00 | loss scale: 1.0 | grad norm: 2.719 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      938/    1000 | consumed samples:         1876 | elapsed time per iteration (ms): 1321.0 | rate (tokens/sec): 6201.43 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.399505E+00 | loss scale: 1.0 | grad norm: 1.642 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      939/    1000 | consumed samples:         1878 | elapsed time per iteration (ms): 1321.6 | rate (tokens/sec): 6198.63 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.407468E+00 | loss scale: 1.0 | grad norm: 1.962 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      940/    1000 | consumed samples:         1880 | elapsed time per iteration (ms): 1322.3 | rate (tokens/sec): 6195.32 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.799492E+00 | loss scale: 1.0 | grad norm: 1.884 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      941/    1000 | consumed samples:         1882 | elapsed time per iteration (ms): 1322.4 | rate (tokens/sec): 6195.01 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.512599E+00 | loss scale: 1.0 | grad norm: 1.788 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      942/    1000 | consumed samples:         1884 | elapsed time per iteration (ms): 1324.5 | rate (tokens/sec): 6184.90 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.634542E+00 | loss scale: 1.0 | grad norm: 1.954 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      943/    1000 | consumed samples:         1886 | elapsed time per iteration (ms): 1322.5 | rate (tokens/sec): 6194.40 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.291040E+00 | loss scale: 1.0 | grad norm: 1.738 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      944/    1000 | consumed samples:         1888 | elapsed time per iteration (ms): 1322.8 | rate (tokens/sec): 6192.97 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.672533E+00 | loss scale: 1.0 | grad norm: 2.779 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      945/    1000 | consumed samples:         1890 | elapsed time per iteration (ms): 1323.6 | rate (tokens/sec): 6189.25 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.853238E+00 | loss scale: 1.0 | grad norm: 1.802 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      946/    1000 | consumed samples:         1892 | elapsed time per iteration (ms): 1322.5 | rate (tokens/sec): 6194.44 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.271720E+00 | loss scale: 1.0 | grad norm: 1.732 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      947/    1000 | consumed samples:         1894 | elapsed time per iteration (ms): 1322.1 | rate (tokens/sec): 6196.36 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 1.934337E+00 | loss scale: 1.0 | grad norm: 1.899 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      948/    1000 | consumed samples:         1896 | elapsed time per iteration (ms): 1323.1 | rate (tokens/sec): 6191.37 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.553029E+00 | loss scale: 1.0 | grad norm: 1.748 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      949/    1000 | consumed samples:         1898 | elapsed time per iteration (ms): 1323.0 | rate (tokens/sec): 6192.07 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.626784E+00 | loss scale: 1.0 | grad norm: 1.755 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      950/    1000 | consumed samples:         1900 | elapsed time per iteration (ms): 1322.8 | rate (tokens/sec): 6193.03 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.589031E+00 | loss scale: 1.0 | grad norm: 1.864 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      951/    1000 | consumed samples:         1902 | elapsed time per iteration (ms): 1324.0 | rate (tokens/sec): 6187.43 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.422612E+00 | loss scale: 1.0 | grad norm: 1.813 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      952/    1000 | consumed samples:         1904 | elapsed time per iteration (ms): 1323.3 | rate (tokens/sec): 6190.72 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.859937E+00 | loss scale: 1.0 | grad norm: 1.689 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      953/    1000 | consumed samples:         1906 | elapsed time per iteration (ms): 1321.5 | rate (tokens/sec): 6199.16 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.553815E+00 | loss scale: 1.0 | grad norm: 1.939 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      954/    1000 | consumed samples:         1908 | elapsed time per iteration (ms): 1319.8 | rate (tokens/sec): 6206.88 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.492201E+00 | loss scale: 1.0 | grad norm: 1.645 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      955/    1000 | consumed samples:         1910 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.48 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.981480E+00 | loss scale: 1.0 | grad norm: 1.719 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      956/    1000 | consumed samples:         1912 | elapsed time per iteration (ms): 1320.3 | rate (tokens/sec): 6204.61 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.510887E+00 | loss scale: 1.0 | grad norm: 1.679 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      957/    1000 | consumed samples:         1914 | elapsed time per iteration (ms): 1323.7 | rate (tokens/sec): 6188.94 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.851163E+00 | loss scale: 1.0 | grad norm: 2.044 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      958/    1000 | consumed samples:         1916 | elapsed time per iteration (ms): 1321.9 | rate (tokens/sec): 6196.92 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.509824E+00 | loss scale: 1.0 | grad norm: 1.596 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      959/    1000 | consumed samples:         1918 | elapsed time per iteration (ms): 1321.2 | rate (tokens/sec): 6200.30 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.686709E+00 | loss scale: 1.0 | grad norm: 1.695 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      960/    1000 | consumed samples:         1920 | elapsed time per iteration (ms): 1322.0 | rate (tokens/sec): 6196.71 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.574314E+00 | loss scale: 1.0 | grad norm: 1.987 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      961/    1000 | consumed samples:         1922 | elapsed time per iteration (ms): 1321.5 | rate (tokens/sec): 6198.90 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.666401E+00 | loss scale: 1.0 | grad norm: 2.029 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      962/    1000 | consumed samples:         1924 | elapsed time per iteration (ms): 1321.3 | rate (tokens/sec): 6199.99 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.368127E+00 | loss scale: 1.0 | grad norm: 2.527 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      963/    1000 | consumed samples:         1926 | elapsed time per iteration (ms): 1321.1 | rate (tokens/sec): 6201.04 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.727757E+00 | loss scale: 1.0 | grad norm: 1.920 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      964/    1000 | consumed samples:         1928 | elapsed time per iteration (ms): 1321.4 | rate (tokens/sec): 6199.57 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.246606E+00 | loss scale: 1.0 | grad norm: 2.684 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      965/    1000 | consumed samples:         1930 | elapsed time per iteration (ms): 1322.9 | rate (tokens/sec): 6192.42 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.652456E+00 | loss scale: 1.0 | grad norm: 2.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      966/    1000 | consumed samples:         1932 | elapsed time per iteration (ms): 1323.0 | rate (tokens/sec): 6192.08 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.787206E+00 | loss scale: 1.0 | grad norm: 1.580 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      967/    1000 | consumed samples:         1934 | elapsed time per iteration (ms): 1322.6 | rate (tokens/sec): 6193.97 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.606873E+00 | loss scale: 1.0 | grad norm: 1.811 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      968/    1000 | consumed samples:         1936 | elapsed time per iteration (ms): 1320.6 | rate (tokens/sec): 6203.11 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.704737E+00 | loss scale: 1.0 | grad norm: 2.082 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      969/    1000 | consumed samples:         1938 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.62 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.651320E+00 | loss scale: 1.0 | grad norm: 1.437 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      970/    1000 | consumed samples:         1940 | elapsed time per iteration (ms): 1321.7 | rate (tokens/sec): 6198.18 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.099497E+00 | loss scale: 1.0 | grad norm: 1.685 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      971/    1000 | consumed samples:         1942 | elapsed time per iteration (ms): 1320.1 | rate (tokens/sec): 6205.62 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.608453E+00 | loss scale: 1.0 | grad norm: 1.941 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      972/    1000 | consumed samples:         1944 | elapsed time per iteration (ms): 1321.8 | rate (tokens/sec): 6197.56 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.722211E+00 | loss scale: 1.0 | grad norm: 1.618 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      973/    1000 | consumed samples:         1946 | elapsed time per iteration (ms): 1322.1 | rate (tokens/sec): 6196.31 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.811756E+00 | loss scale: 1.0 | grad norm: 2.143 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      974/    1000 | consumed samples:         1948 | elapsed time per iteration (ms): 1323.5 | rate (tokens/sec): 6189.50 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.577770E+00 | loss scale: 1.0 | grad norm: 1.776 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      975/    1000 | consumed samples:         1950 | elapsed time per iteration (ms): 1320.3 | rate (tokens/sec): 6204.46 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.220293E+00 | loss scale: 1.0 | grad norm: 2.219 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      976/    1000 | consumed samples:         1952 | elapsed time per iteration (ms): 1321.9 | rate (tokens/sec): 6197.16 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.327058E+00 | loss scale: 1.0 | grad norm: 2.132 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      977/    1000 | consumed samples:         1954 | elapsed time per iteration (ms): 1321.0 | rate (tokens/sec): 6201.45 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.298240E+00 | loss scale: 1.0 | grad norm: 2.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      978/    1000 | consumed samples:         1956 | elapsed time per iteration (ms): 1321.8 | rate (tokens/sec): 6197.83 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.909710E+00 | loss scale: 1.0 | grad norm: 2.180 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      979/    1000 | consumed samples:         1958 | elapsed time per iteration (ms): 1320.2 | rate (tokens/sec): 6205.22 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.339827E+00 | loss scale: 1.0 | grad norm: 1.810 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      980/    1000 | consumed samples:         1960 | elapsed time per iteration (ms): 1322.3 | rate (tokens/sec): 6195.47 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.379913E+00 | loss scale: 1.0 | grad norm: 2.191 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      981/    1000 | consumed samples:         1962 | elapsed time per iteration (ms): 1321.6 | rate (tokens/sec): 6198.42 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.537714E+00 | loss scale: 1.0 | grad norm: 1.900 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      982/    1000 | consumed samples:         1964 | elapsed time per iteration (ms): 1321.5 | rate (tokens/sec): 6198.92 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.646518E+00 | loss scale: 1.0 | grad norm: 1.648 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      983/    1000 | consumed samples:         1966 | elapsed time per iteration (ms): 1319.4 | rate (tokens/sec): 6208.75 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.531346E+00 | loss scale: 1.0 | grad norm: 1.774 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      984/    1000 | consumed samples:         1968 | elapsed time per iteration (ms): 1319.6 | rate (tokens/sec): 6208.09 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.712158E+00 | loss scale: 1.0 | grad norm: 1.864 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      985/    1000 | consumed samples:         1970 | elapsed time per iteration (ms): 1321.3 | rate (tokens/sec): 6200.04 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.643654E+00 | loss scale: 1.0 | grad norm: 1.687 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      986/    1000 | consumed samples:         1972 | elapsed time per iteration (ms): 1321.8 | rate (tokens/sec): 6197.84 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.832088E+00 | loss scale: 1.0 | grad norm: 2.302 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      987/    1000 | consumed samples:         1974 | elapsed time per iteration (ms): 1320.2 | rate (tokens/sec): 6205.27 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.522235E+00 | loss scale: 1.0 | grad norm: 1.717 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      988/    1000 | consumed samples:         1976 | elapsed time per iteration (ms): 1319.9 | rate (tokens/sec): 6206.35 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.435149E+00 | loss scale: 1.0 | grad norm: 1.775 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      989/    1000 | consumed samples:         1978 | elapsed time per iteration (ms): 1320.1 | rate (tokens/sec): 6205.47 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.032890E+00 | loss scale: 1.0 | grad norm: 1.913 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      990/    1000 | consumed samples:         1980 | elapsed time per iteration (ms): 1321.3 | rate (tokens/sec): 6199.84 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.062073E+00 | loss scale: 1.0 | grad norm: 1.787 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      991/    1000 | consumed samples:         1982 | elapsed time per iteration (ms): 1323.0 | rate (tokens/sec): 6191.94 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.313253E+00 | loss scale: 1.0 | grad norm: 2.293 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      992/    1000 | consumed samples:         1984 | elapsed time per iteration (ms): 1325.0 | rate (tokens/sec): 6182.75 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.656791E+00 | loss scale: 1.0 | grad norm: 1.716 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      993/    1000 | consumed samples:         1986 | elapsed time per iteration (ms): 1321.1 | rate (tokens/sec): 6200.92 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.429184E+00 | loss scale: 1.0 | grad norm: 1.921 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      994/    1000 | consumed samples:         1988 | elapsed time per iteration (ms): 1321.4 | rate (tokens/sec): 6199.55 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 3.109812E+00 | loss scale: 1.0 | grad norm: 2.125 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      995/    1000 | consumed samples:         1990 | elapsed time per iteration (ms): 1321.2 | rate (tokens/sec): 6200.22 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.163999E+00 | loss scale: 1.0 | grad norm: 1.863 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      996/    1000 | consumed samples:         1992 | elapsed time per iteration (ms): 1322.1 | rate (tokens/sec): 6196.37 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.538778E+00 | loss scale: 1.0 | grad norm: 1.818 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      997/    1000 | consumed samples:         1994 | elapsed time per iteration (ms): 1319.7 | rate (tokens/sec): 6207.56 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.596015E+00 | loss scale: 1.0 | grad norm: 1.753 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      998/    1000 | consumed samples:         1996 | elapsed time per iteration (ms): 1330.6 | rate (tokens/sec): 6156.75 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.221539E+00 | loss scale: 1.0 | grad norm: 1.993 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      999/    1000 | consumed samples:         1998 | elapsed time per iteration (ms): 1322.4 | rate (tokens/sec): 6194.71 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.506119E+00 | loss scale: 1.0 | grad norm: 1.832 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1000/    1000 | consumed samples:         2000 | elapsed time per iteration (ms): 1320.5 | rate (tokens/sec): 6203.66 | learning rate: 3.000E-04 | global batch size:     2 | lm loss: 2.563416E+00 | loss scale: 1.0 | grad norm: 1.693 | number of skipped iterations:   0 | number of nan iterations:   0 |
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 validation loss at iteration 1000 | lm loss value: 2.806648E+00 | lm loss PPL: 1.655434E+01 | ppl value: 1.676863E+01 | lm accuracy value: 4.240356E-01 | count loss mask value: 4.096000E+03 | 
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[after training is done] datetime: 2023-10-25 07:31:53 
saving checkpoint at iteration    1000 to /root/models/llama-2-7b-chat-hf-megatron_shard_tp_2_pp_1-pretrained
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 validation loss at the end of training for val data | lm loss value: 2.782947E+00 | lm loss PPL: 1.616659E+01 | ppl value: 1.639659E+01 | lm accuracy value: 4.295654E-01 | count loss mask value: 4.096000E+03 | 
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  successfully saved checkpoint at iteration    1000 to /root/models/llama-2-7b-chat-hf-megatron_shard_tp_2_pp_1-pretrained
Evaluating iter 1/10
Evaluating iter 2/10
Evaluating iter 3/10
Evaluating iter 4/10
Evaluating iter 5/10
Evaluating iter 6/10
Evaluating iter 7/10
Evaluating iter 8/10
Evaluating iter 9/10
Evaluating iter 10/10
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 validation loss at the end of training for test data | lm loss value: 3.490447E+00 | lm loss PPL: 3.280059E+01 | ppl value: 3.956309E+01 | lm accuracy value: 3.504517E-01 | count loss mask value: 4.096000E+03 | 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Done 2023-10-25 07:33:54.505011+00:00
Done 2023-10-25 07:33:54.679964+00:00
