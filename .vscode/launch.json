{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "finetune.py for pretrain",
            "type": "python",
            "request": "launch",
            "program": "/usr/local/bin/torchrun",
            "env": {
                "CUDA_VISIBLE_DEVICES": "2,3",
                "OMP_NUM_THREADS": "16",
                "CUDA_DEVICE_MAX_CONNECTIONS": "1"
            },
            "args": [
                "--nproc_per_node=2",
                "--master_port=6100",
                "finetune.py",
                "--tensor_model_parallel_size=2",
                "--pipeline_model_parallel_size=1",
                "--load=/root/models/original_epfLLM_megatron/llama-2-7b-chat-hf-megatron/shard-tp2-pp1",
                "--save=/root/models/original_epfLLM_megatron/llama-2-7b-chat-hf-megatron/shard-tp2-pp1-pretrained",
                "--tensorboard_dir=/root/models/original_epfLLM_megatron/llama-2-7b-chat-hf-megatron/shard-tp2-pp1-pretrained/logging",
                "--data_path=/root/datasets/booksum_megatron/booksum_megatron_text_document",
                "--model_name=llama2",
                "--tokenizer_type=PretrainedFromHF",
                "--tokenizer_name_or_path=/root/models/llama-2-7b-chat-hf",
                "--bf16",
                "--no_new_tokens",
                "--global_batch_size=2",
                "--micro_batch_size=1",
                "--num_workers=2",
                "--use_rms_norm",
                "--glu_activation=swiglu",
                "--no_tie_embed_logits",
                "--layernorm_epsilon=1e-5",
                "--use_flash_attn",
                "--no_bias_gelu_fusion",
                "--seq_length=4096",
                "--max_position_embeddings=4096",
                "--log_interval=1",
                "--save_interval=800",
                "--eval_interval=200",
                "--eval_iters=10",
                "--hidden_dropout=0.0",
                "--position_embedding_type=rotary",
                "--no_bias_dropout_fusion",
                "--use_checkpoint_args",
                "--attention_dropout=0.0",
                "--adam_beta1=0.9",
                "--adam_beta2=0.95",
                "--adam_eps=1e-5",
                "--lr_decay_style=cosine",
                "--lr_warmup_fraction=0.1",
                "--lr=3e-4",
                "--min_lr=3e-4",
                "--weight_decay=0.1",
                "--sequence_parallel",
                "--recompute_granularity=selective",
                "--log_timers_to_tensorboard",
                "--tensorboard_log_interval=1",
                "--scalar_loss_mask=0.0",
                "--rope_scaling_factor=1.0",
                "--metrics",
                "perplexity",
                "accuracy",
                "count_loss_mask",
                "--train_iters=10"
            ],
            "console": "integratedTerminal",
            "justMyCode": true
        },
        {
            "name": "finetune_fastchat_data.py",
            "type": "python",
            "request": "launch",
            "program": "/usr/local/bin/torchrun",
            "env": {
                "CUDA_VISIBLE_DEVICES": "2,3",
                "OMP_NUM_THREADS": "16",
                "CUDA_DEVICE_MAX_CONNECTIONS": "1"
            },
            "args": [
                "--nproc_per_node=2",
                "--master_port=6100",
                "finetune_fastchat_data.py",
                "--tensor_model_parallel_size=2",
                "--pipeline_model_parallel_size=1",
                "--load=/root/models/original_epfLLM_megatron/llama-2-7b-chat-hf-megatron/shard-tp2-pp1",
                "--save=/root/models/original_epfLLM_megatron/llama-2-7b-chat-hf-megatron/shard-tp2-pp1-finetuned",
                "--tensorboard_dir=/root/models/original_epfLLM_megatron/llama-2-7b-chat-hf-megatron/shard-tp2-pp1-pretrained/logging",
                "--data_path=/root/test/test_hf",
                "--model_name=llama2",
                "--finetune",
                "--variable_seq_lengths",
                "--data_type=instruction",
                "--tokenizer_type=PretrainedFromHF",
                "--tokenizer_name_or_path=/root/models/llama-2-7b-chat-hf",
                "--bf16",
                "--global_batch_size=2",
                "--micro_batch_size=1",
                "--num_workers=2",
                "--use_rms_norm",
                "--glu_activation=swiglu",
                "--no_tie_embed_logits",
                "--layernorm_epsilon=1e-5",
                "--use_flash_attn",
                "--no_bias_gelu_fusion",
                "--seq_length=4096",
                "--max_position_embeddings=4096",
                "--log_interval=1",
                "--save_interval=800",
                "--eval_interval=200",
                "--eval_iters=10",
                "--hidden_dropout=0.0",
                "--position_embedding_type=rotary",
                "--no_bias_dropout_fusion",
                "--use_checkpoint_args",
                "--attention_dropout=0.0",
                "--adam_beta1=0.9",
                "--adam_beta2=0.95",
                "--adam_eps=1e-5",
                "--lr_decay_style=cosine",
                "--lr_warmup_fraction=0.1",
                "--lr=2e-5",
                "--min_lr=2e-6",
                "--weight_decay=0.1",
                "--sequence_parallel",
                "--recompute_granularity=selective",
                "--log_timers_to_tensorboard",
                "--tensorboard_log_interval=1",
                "--scalar_loss_mask=0.0",
                "--rope_scaling_factor=1.0",
                "--metrics",
                "perplexity",
                "accuracy",
                "count_loss_mask",
                "--train_iters=100"
            ],
            "console": "integratedTerminal",
            "justMyCode": true
        },
        {
            "name": "preprocess_data.py",
            "type": "python",
            "request": "launch",
            "program": "tools/preprocess_data.py",
            "args": [
                "--input=/root/datasets/booksum.jsonl",
                "--output_prefix=/root/datasets/booksum_megatron",
                "--tokenizer_type=SentencePieceTokenizer",
                "--vocab_file=/root/models/llama-2-7b-chat-hf/tokenizer.model",
                "--chunk_size=32",
                "--workers=1",
                "--no_new_tokens",
            ],
            "console": "integratedTerminal",
            "justMyCode": true
        },
        {
            "name": "preprocess_instruct_data.py",
            "type": "python",
            "request": "launch",
            "program": "tools/preprocess_instruct_data.py",
            "args": [
                "--input=/root/datasets/OpenOrca/data.jsonl",
                "--output_prefix=/root/datasets/OpenOrca/OpenOrca_megatron",
                "--tokenizer_type=SentencePieceTokenizer",
                "--vocab_file=/root/models/llama-2-7b-chat-hf/tokenizer.model",
                "--chunk_size=32",
                "--workers=32",
                "--vocab_extra_ids_list=<|im_start|>,<|im_end|>",
                "--question_key=question",
                "--answer_key=response",
                "--system_key=system_prompt"
            ],
            "console": "integratedTerminal",
            "justMyCode": true
        }
    ]
}